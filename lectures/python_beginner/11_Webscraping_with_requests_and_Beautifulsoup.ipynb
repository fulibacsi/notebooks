{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python 101\n",
    "## XI. Webscraping - requests + Beautifulsoup\n",
    "\n",
    "---\n",
    "\n",
    "### 0. Easy file sharing\n",
    "Start your own web-server:\n",
    "- in command prompt change your directory to `notebooks/lectures/python_beginner/data`\n",
    "- start the server with the `python -m http.server` command\n",
    "\n",
    "### 1. Obtain a webpage\n",
    "\n",
    "The easiest way is to use a third party library called __`requests`__. Let's import it right away!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we simply ask a server to give us an html document by requesting it through an url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_url = 'http://localhost:8000/test.html'\n",
    "response = requests.get(existing_url)\n",
    "print(response.status_code) # hopefully 200 -> successful download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_existing_url = 'http://fulibacsi.github.io/data/test1.html'\n",
    "response = requests.get(not_existing_url)\n",
    "print(response.status_code) # unfortunately 404 -> not exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Common status codes:__\n",
    "- 200: success\n",
    "- 301: permanent redirect\n",
    "- 303: redirect\n",
    "- 400: bad request\n",
    "- 401: unauthorized\n",
    "- 404: not exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(existing_url)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter can render the page if it was successfully downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "if response.status_code == 200:\n",
    "    result = HTML(response.content)\n",
    "else:\n",
    "    result = 'Nah, let\\'s have a beer instead!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Let's parse it!\n",
    "\n",
    "We have a third party module for this purpose as well, the __`BeautifulSoup`__.  \n",
    "Let's import it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create a soup from the downloaded document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = response.content\n",
    "soup = BeautifulSoup(document, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the created soup (which is a parsed document) we can easily access any part of the document.  \n",
    "Let's try to:\n",
    "- get the title of the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get the title text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.title.getText())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get the text-only version of the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.getText())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get all the links from the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.findAll('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get the actual urls from the tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in soup.findAll('a'):\n",
    "    print(url.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During scraping, there are a lot of different tasks that must be solve in order to get the data we need. \n",
    "In this case this demo document has important and unimportant parts. We only need the important parts.   \n",
    "#### a) Let's find the important links!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_urls = []\n",
    "for url in soup.findAll('a'):\n",
    "    if 'important_part' in url.get('href'):\n",
    "        important_urls.append(url.get('href'))\n",
    "print(important_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Let's find the important text in the document\n",
    "- select every paragraph which has \"important\" class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.findAll('p', {'class': 'important'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Whooops, something's going on! Let's investigate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_paragraphs = soup.findAll('p', {'class': 'important'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- print the text in the tags, and tags' parent's id attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in important_paragraphs:\n",
    "    print(p.getText(), p.parent.get('id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see, that the \"fake\" result is from somewhere else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.find(id='not_main_section'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have a hidden fake section! Let's modify our search!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(id='main_content').findAll('p', {'class': 'important'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Let's find the pictures of our interest\n",
    "- Let's have the \"nice\" pictures from the div with random_images_1 class!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    soup\n",
    "    .find(id='main_content')\n",
    "    .find('div', {'class': 'random_images_1'})\n",
    "    .findAll('img', {'class': 'nice'})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Whoops again. Filter out the result we don't like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = (\n",
    "    soup\n",
    "    .find(id='main_content')\n",
    "    .find('div', {'class': 'random_images_1'})\n",
    "    .findAll('img', {'class': 'nice'})\n",
    ")\n",
    "nice_imgs = []\n",
    "for img in imgs:\n",
    "    if 'not' not in img['class']:\n",
    "        nice_imgs.append(img['src'])\n",
    "print(nice_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "#### 1. Save every important link to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URI = './data/'\n",
    "filename = 'important_urls.txt'\n",
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Let's get a random img/gif url from 9gag!\n",
    "- get the page from http://9gag.com/random\n",
    "- find the images, and get the `src` attribute's value\n",
    "- depending on the content you can find:\n",
    "    - not animated images are stored in __`img`__ tags with __`badge-item-img`__ class, in the __`src`__ attribute \n",
    "    - animated imgages are stored in __`div`__ tags with __`badge-animated-container-animated`__ class, in the __`data-mp4`__ attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URI = \"http://9gag.com/random\"\n",
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3. Querying webpages \n",
    "\n",
    "Collect the articles about migrants from index.hu\n",
    "\n",
    "This will require to search in the site.\n",
    "On the upper-left corner, there is a search icon. Use it, and observe the resulting url:\n",
    "\n",
    "`http://index.hu/24ora/?tol=1999-01-01&ig=2017-11-07&s=migráns`\n",
    "\n",
    "It has multiple parts:\n",
    "- `http://` - protocol\n",
    "- `index.hu` - base url\n",
    "- `/24ora/` - sub url\n",
    "- `?tol=1999-01-01&ig=2017-11-07&s=migráns` - query\n",
    "\n",
    "Let's investigate the query part a little more!  \n",
    "Every query starts with a __`?`__ charater followed by one or more key-value pairs. The key-value pairs are separated with the __`&`__ character. Based on this information, we can extract the query parameters:\n",
    "- `tol`\n",
    "- `ig`\n",
    "- `s`\n",
    "\n",
    "Use these values to construct our own request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'http://index.hu'\n",
    "sub_url = '/24ora'\n",
    "query = {\n",
    "    'tol': '1999-01-01',\n",
    "    'ig': '2017-11-07',\n",
    "    's': u'migráns'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the requests library to send the query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.get(url=base_url+sub_url, data=query) # some pages requires `params` instead of `data`\n",
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the response, extract the urls inside the `<article>` tags!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that only 30 results showed up. We can customize our query to cover shorter amount of timed by replacing __`tol`__ and __`ig`__ parameters with a formattable string: __`'{year}-{month:0>2}-{day:0>2}'`__. This string can be formatted by providing the required parameters:\n",
    "- year\n",
    "- month\n",
    "- day\n",
    "\n",
    "like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'{year}-{month:0>2}-{day:0>2}'.format(year=2016, month=1, day=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a useful library called __`datetime`__. You can use it to generate dates automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "date = datetime.date(1999, 1, 1)\n",
    "day_after_date = date + datetime.timedelta(days=1)\n",
    "day_before_date = date - datetime.timedelta(days=1)\n",
    "today = datetime.date.today()\n",
    "\n",
    "print(day_before_date)\n",
    "print(date)\n",
    "print(day_after_date)\n",
    "print(today)\n",
    "\n",
    "print(today.year, today.month, today.day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a loop which iterate through every day from 1999-01-01 till today and execute the same procedure you created previously. (Pro tip: create a function!) Observe the number of results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4. User agents\n",
    "\n",
    "Let's pretend to be a browser instead of a script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_AGENTS = [\n",
    "    # Chrome\n",
    "    'Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1667.0 Safari/537.36',\n",
    "    # Firefox\n",
    "    'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:25.0) Gecko/20100101 Firefox/25.0',\n",
    "    # Opera\n",
    "    'Opera/9.80 (Windows NT 6.0) Presto/2.12.388 Version/12.14',\n",
    "    # Safari\n",
    "    'Mozilla/5.0 (iPad; CPU OS 6_0 like Mac OS X) AppleWebKit/536.26 (KHTML, like Gecko) Version/6.0 Mobile/10A5355d Safari/8536.25',\n",
    "    # Internet Explorer, probably a good idea to leave this one out...\n",
    "    'Mozilla/5.0 (compatible; MSIE 10.6; Windows NT 6.1; Trident/5.0; InfoPath.2; SLCC1; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729; .NET CLR 2.0.50727) 3gpp-gba UNTRUSTED/1.0',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a wrapper function to handle the user-agent string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def get_header(agents):\n",
    "    return {'User-agent': random.choice(agents)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "#### 3. Get the main articles from index.hu\n",
    "Write a function that prints that extracts the current main articles! It should contain:\n",
    "- the title\n",
    "- the article text\n",
    "- the url\n",
    "- every picture from the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://index.hu'\n",
    "index_response = requests.get(url, headers=get_header(USER_AGENTS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Get the articles about migration from 444.hu\n",
    "\n",
    "Write a function that prints the titles of the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://444.hu/kereses'\n",
    "query = '?q=migrans'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
