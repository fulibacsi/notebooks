{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python 101 \n",
    "## Part IX.\n",
    "\n",
    "---\n",
    "\n",
    "## Web Scraping - Part III.\n",
    "\n",
    "### I. [SelectorGadget](https://chrome.google.com/webstore/detail/selectorgadget/mhjhnkcfbdhnjickkkdbjoemdmbfginb)\n",
    "\n",
    "__Making life easier to select the proper content from a website. The ones and only the ones you need.__\n",
    "\n",
    "1. Click on the SelectorGadget icon to activate it. It is located in the upper right corner.\n",
    "2. Right after clicking this, a bar will appear in the bottom right corner of the chrome window. Also you will realise that as you start moving the cursor, things will get frames. Do not panick, this is normal!\n",
    "![frame](pics/bar.png)\n",
    "3. You will probably want to get multiple instances of the same type of content (e.g. pictures from the main page of telex.hu). \n",
    "4. Rules for selection:\n",
    " - First click to mark an instace of the type of content you like\n",
    "  ![example](pics/example_selector.png) <br></br>\n",
    " - The same type of content will also be framed. If there is something you want to exclude (e.g. the telex logo at the top or the tiny weather icon), click on one of them. Starting with the second click, you may exclude anything. The program is smart enough to figure out that if you did not want the telex logo, it is likely that you will want to exclude the weather icon as well. Therefore, it is going to be removed automatically.<br></br>\n",
    "   ![example](pics/good_state.png)\n",
    "- In the bottom right corner, you will see the magic command (`.article_title img`) you should use to select all the content you want. Run `soup.select()` to get a list of instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://telex.hu\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, this is business as usual. Let's get the pictures!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list = []\n",
    "for image in soup.select(\".article_title img\"): # select will always return a list\n",
    "    image_list.append(url + image.get(\"src\")) # prefix is needed\n",
    "image_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ooooor the way cool kids do it. List comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[url + image.get(\"src\") for image in soup.select(\".article_title img\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![coolkids](https://a.wattpad.com/cover/163492905-352-k572763.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise I:\n",
    "Search for a specific brand of car in hasznaltauto.hu and list the car urls from the __first page__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise II:\n",
    "Get some pieces of information on the real estate market of Budapest. Check out all the houses on [ingatlan.com](https://ingatlan.com/lista/elado+lakas+budapest) and get the following content for the __first page__.\n",
    "- Price\n",
    "- Unit price (displayed in _Ft/m2_)\n",
    "- Number of rooms\n",
    "- Area (displayed in _m2_)\n",
    "\n",
    "Make sure you select the proper format of storing these variable! Printing them is not enough, save them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Dynamically generated pages\n",
    "\n",
    "Dynamically generated pages could not be parsed by simply downloading them since the generated content won't be present. For this case there is an another library called selenium. This library also requires a browser to operate. A browser will be started and every operation will be executed inside that browser. Its path must be set in order to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install selenium -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from helpers import get_download_dir, chromedriver_download\n",
    "\n",
    "chromedriver_download()\n",
    "os.environ['PATH'] += ';' + get_download_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Simple lookup\n",
    "- initialize the browser which will be used by the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- request a page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('http://9gag.com/random')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- find items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    media = (\n",
    "        driver\n",
    "        .find_element_by_class_name('post-container')\n",
    "        .find_element_by_tag_name('img')\n",
    "        .get_attribute('src')\n",
    "    )\n",
    "except NoSuchElementException:\n",
    "    media = (\n",
    "        driver\n",
    "        .find_element_by_class_name('post-container')\n",
    "        .find_element_by_tag_name('video')\n",
    "        .find_element_by_tag_name('source')\n",
    "        .get_attribute('src')\n",
    "    )\n",
    "    \n",
    "print(media)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Available finder methods:\n",
    "- `find_element_by_tag_name(tag)`\n",
    "- `find_elements_by_tag_name(tag)`\n",
    "- `find_element_by_class_name(class)`\n",
    "- `find_elements_by_class_name(class)`\n",
    "- `find_element_by_id(id)`\n",
    "- `find_element_by_css_selector(css_selector)`\n",
    "- `find_elements_by_css_selector(css_selector)`\n",
    "\n",
    "#### CSS selectors\n",
    "- `tagname`\n",
    "- `.classname`\n",
    "- `#id`\n",
    "- `[attribute=value]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    media = (driver\n",
    "             .find_element_by_css_selector('#individual-post .post-container img')\n",
    "             .get_attribute('src'))\n",
    "except NoSuchElementException:\n",
    "    media = (driver\n",
    "             .find_element_by_css_selector('#individual-post .post-container video source')\n",
    "             .get_attribute('src'))\n",
    "    \n",
    "media"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Interaction with the site\n",
    "- request the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://444.hu/kereses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- find search field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_field = driver.find_element_by_css_selector('#content-main input[name=q]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- fill in search query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_field.send_keys('migr√°ns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- find submit button and click on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_button = driver.find_element_by_css_selector('#content-main input[type=submit]')\n",
    "submit_button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- find related content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "for article in driver.find_elements_by_class_name('card'):\n",
    "    urls.append(article.find_element_by_tag_name('a').get_attribute('href'))\n",
    "len(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- solution for infinite scrolldown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def scrolldown():\n",
    "    lastHeight = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(1)\n",
    "        newHeight = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if newHeight == lastHeight:\n",
    "            break\n",
    "        lastHeight = newHeight\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "button = True\n",
    "while button:\n",
    "    print('.', end='')\n",
    "    \n",
    "    scrolldown()\n",
    "    for article in driver.find_elements_by_class_name('card'):\n",
    "        urls.append(article.find_element_by_tag_name('a').get_attribute('href'))\n",
    "    try:\n",
    "        button = driver.find_element_by_css_selector('a.infinity-next.button')\n",
    "        button.click()\n",
    "    except NoSuchElementException:\n",
    "        button = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Querying microservices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Microservices - also known as the microservice architecture - is an architectural style that structures an application as a collection of services that are\n",
    "\n",
    "- Highly maintainable and testable\n",
    "- Loosely coupled\n",
    "- Independently deployable\n",
    "- Organized around business capabilities\n",
    "- Owned by a small team\n",
    "\n",
    "Source and more reading [here](https://microservices.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Architectural design\n",
    "\n",
    "![Architecture1](https://i.stack.imgur.com/b62O1.png)\n",
    "![Architecture2](https://www.redhat.com/cms/managed-files/monolithic-vs-microservices.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Genaral algorithm to uncover and exploit microservices\n",
    "__Warning #1:__ Sometimes, the direct usage of microservices is forbidden for commercial purposes. Before you start building a business on it, you might want to read the related terms and conditions of the website. Rare usage should not result in any actions.\n",
    "\n",
    "__Warning #2:__ Not every website uses microservices (or they are restricted in some ways). Therefore, this method will __not__ work in every single case. Sometimes, parsing an HTML is just not something you can avoid. However, it is surely worth checking as you may retrieve the whole dataset without having to parse and clean anything. \n",
    "\n",
    "__Task__: Say you want to scrape the departing flights fro a given day from [Budapest Liszt Ferenc Airport](https://www.bud.hu/indulo_jaratok). You need every detail that is accessible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Open the website, right click and go inspect. On the top bar, instead of browsing the `Elements` tab, change to `Network`. If nothing is displayed here, refresh the page. This will show you the list of network traffic that happens under the hoods. There are pictures here, JavaScript codes and a bunch of scary process that we will avoid, don't worry. You will want to order the requests by `Type`. In most of the cases, `xhr` and `document` types will be the ones we care about. If you click on one of the `xhr` types, this is what should pop up. <br></br>\n",
    " ![micro0](pics/micro0.png) <br></br>\n",
    "2. The `Headers` tab shows you the input details of the request that was sent out retrieve this specific content. If you change to the `Preview` or the `Response` tabs, the result of this request will be shown to you. While clicking the former will give you a nicer and rendered look, the latter returns a raw version.<br></br>\n",
    "3. Now, the task is to find the entry that returns the pieces of flights data we need. Let's check all the ones with `Type` = `xhr` first and check their `Preview` tabs to find the right one. I think we have a winner here, this looks great: <br></br>\n",
    " ![micro2](pics/micro2.png)<br></br>\n",
    "4. Click on the \"play button looking\" triangle to expand an entry. Okat, this is very cool, we have it.\n",
    "5. Next, we need to find a way replicate it so that we can get the data programmatically. If only there was a way to retrieve the input data for this very request. Oh wait! This is what the `Headers` tab is there for, isn't it? It is!\n",
    "6. Now, the `Headers` tab contains details in a non-Python format (this is not entirely true, but at this point you are not assumed to have the skills needed to transform it manually).\n",
    "7. We are going to transform it with a third party service: https://curl.trillworks.com/\n",
    "8. We need to first copy the [curl](https://en.wikipedia.org/wiki/CURL) equivalent of the request by right clicking -> Copy -> copy as curl. Now, the curl command is copied to the keyboard. Go to https://curl.trillworks.com/ and paste it to the curl command box. This will generate the Python code we can use.\n",
    "![micro3](pics/micro3.png)<br></br>\n",
    "9. You are all done :) From now in, the sucess only depends on your Python skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code snippet curl.trillworks.com generated to me:\n",
    "import requests\n",
    "\n",
    "cookies = {\n",
    "    'cookie_bar': 'enabled',\n",
    "    '_ga': 'GA1.2.270795426.1604223546',\n",
    "    '_gid': 'GA1.2.1464611313.1604223546',\n",
    "    'XSRF-TOKEN': 'eyJpdiI6ImFDdE11RUFSZWEwa0QrN3VJRVJhbFE9PSIsInZhbHVlIjoibFhGNENRK3RPeVhRUW5VS3ZGYkhyREJTU29kVEQzMVhIeVQzOWo1dTNscUd2RkQxN0xURUZJcDBRblVCdHRQMUNVbXFDQXBmbXk3ZVdSR1A0SlBkWGc9PSIsIm1hYyI6IjI5YTcyZjJlYzk4YmZmOGZmYTFlNTQxMWQ4ZGVmM2ZjMDVhYjMwOWU4MzhkNjI5MjNjYzAzMTBlNTFhYjA5ZjUifQ%3D%3D',\n",
    "    'budhu_session': 'eyJpdiI6IlhJTHEraE5jYmJ0Z2lLXC9zeVk1VmRBPT0iLCJ2YWx1ZSI6IjFBT0FyQmhDaGc0UlwvM0Z5NDBYd1pOQzIxNlpHcGRqbGFGQ3NPOXI1NlZlaCtKWHZ0c3Z5UENkb0RxK1N5WkVpcHhBV1JxYUsybFU5aXRjampVU3FJUT09IiwibWFjIjoiNTQ3OWJlZTQzNjY3MzAwZmFlYzJiN2FlNTI4MTA5YjAyOWYxZWQ2ZDdmNmQ5MTkwNWYxMTEwNmM2YTc1Mjc5YSJ9',\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    'Connection': 'keep-alive',\n",
    "    'Accept': '*/*',\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36',\n",
    "    'X-Requested-With': 'XMLHttpRequest',\n",
    "    'Sec-Fetch-Site': 'same-origin',\n",
    "    'Sec-Fetch-Mode': 'cors',\n",
    "    'Sec-Fetch-Dest': 'empty',\n",
    "    'Referer': 'https://www.bud.hu/indulo_jaratok',\n",
    "    'Accept-Language': 'en-GB,en-US;q=0.9,en;q=0.8',\n",
    "}\n",
    "\n",
    "params = (\n",
    "    ('mode', 'list'),\n",
    "    ('lang', 'hun'),\n",
    "    ('dir', '0'),\n",
    "    ('flightdate_custom_from_date', 'today'),\n",
    "    ('flightdate_custom_from_time', '10:30'),\n",
    ")\n",
    "\n",
    "response = requests.get('https://www.bud.hu/api/ajaxFlights/', headers=headers, params=params, cookies=cookies)\n",
    "\n",
    "#NB. Original query string below. It seems impossible to parse and\n",
    "#reproduce query strings 100% accurately so the one below is given\n",
    "#in case the reproduced version is not \"correct\".\n",
    "# response = requests.get('https://www.bud.hu/api/ajaxFlights/?mode=list&lang=hun&dir=0&flightdate_custom_from_date=today&flightdate_custom_from_time=10:30', headers=headers, cookies=cookies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always check the status code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, 200 is great, means success. It is usually the case, that you do not need to include cookies in the request. Just saying, but up to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://www.bud.hu/api/ajaxFlights/', headers=headers, params=params) # deleted cookies from here\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as the response is a [JSON](https://en.wikipedia.org/wiki/JSON) file, we don't need to parse it with `BeautifulSoup`, just simply convert it to a variable. If you are not familiar with the format JSON, just think of it as a Python dictionary  (that can include lists as well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = response.json() # interpreting it as JSON\n",
    "type(data) # result object is a list this time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there is no documentation in what format data are coming, we need to uncover the pattern. But relax, it is usually very handy. First, have a look at the first item of the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0] # First item of the list, a dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will probably be a list of dictionaries, each item containing pieces of information on one spicific departing flight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise III: Let's hack the system!\n",
    "![hackerman](https://wompampsupport.azureedge.net/fetchimage?siteId=7575&v=2&jpgQuality=100&width=700&url=https%3A%2F%2Fi.kym-cdn.com%2Fentries%2Ficons%2Ffacebook%2F000%2F021%2F807%2Fig9OoyenpxqdCQyABmOQBZDI0duHk2QZZmWg2Hxd4ro.jpg) <br></br>\n",
    " Change the parameters so that:\n",
    " \n",
    " - Instead of today, it will return flights from the day before (that is, yesterday). \n",
    " - Instead of departing flights, it will return the arrivals.\n",
    " - Instead of showing flights after 10.30 AM, it will return all the flights.\n",
    " \n",
    "__Warning #3:__ Note, that every single website has different microservices and hence parameters. What we are doing it specific to [bud.hu](https://www.bud.hu/). When scraping another website, you need to uncover the parameter space and find the possibilities you have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_params = \n",
    "\n",
    "response = requests.get('https://www.bud.hu/api/ajaxFlights/', headers=headers, params=custom_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your result here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise IV: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
