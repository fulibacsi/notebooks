{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II. Webscraping\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Obtaining a webpage\n",
    "\n",
    "The easiest way is to use a third party library called __`requests`__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We simply ask a server to give us an html document by requesting it through an url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_url = 'http://localhost:8000/test.html'\n",
    "response = requests.get(existing_url)\n",
    "print(response.status_code) # hopefully 200 -> successful download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_existing_url = 'http://localhost:8000/test1.html'\n",
    "response = requests.get(not_existing_url)\n",
    "print(response.status_code) # unfortunately 404 -> not exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Common status codes:__\n",
    "- 200: success\n",
    "- 301: permanent redirect\n",
    "- 303: redirect\n",
    "- 400: bad request\n",
    "- 401: unauthorized\n",
    "- 404: not exists\n",
    "- 500: internal server error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(existing_url)\n",
    "print(response.content.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parsing\n",
    "\n",
    "There is a third party module for this purpose called __`BeautifulSoup`__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create a soup from the downloaded document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = response.content\n",
    "soup = BeautifulSoup(document, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the created soup (which is a parsed document) we can easily access any part of the document.  \n",
    "It is able to:\n",
    "- get the title of the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.title)\n",
    "print(type(soup.title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get the title text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.title.get_text())\n",
    "print(type(soup.title.get_text()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get the text-only version of the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get all the links from the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get the actual urls from the tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in soup.find_all('a'):\n",
    "    print(url.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During scraping, there are a lot of different tasks that must be solve in order to get the data we need. \n",
    "In this case this demo document has important and unimportant parts. We only need the important parts.   \n",
    "#### a) Let's find the important links!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_urls = []\n",
    "for url in soup.findAll('a'):\n",
    "    if 'important_part' in url.get('href'):\n",
    "        important_urls.append(url.get('href'))\n",
    "print(important_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Find the important text in the document\n",
    "- select every paragraph which has \"important\" class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('p', class_='important')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Whooops, something's going on! Investigate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_paragraphs = soup.find_all('p', class_='important')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- print the text in the tags, and tags' parent's id attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in important_paragraphs:\n",
    "    print(p.get_text(), '>', p.parent.get('id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see, that the \"fake\" result is from somewhere else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(id='not_main_section')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have a hidden fake section! Let's modify our search!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(id='main_content').find_all('p', class_='important')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Find the pictures of our interest\n",
    "- Get the \"nice\" pictures from the **`div`** with **`random_images_1`** class!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    soup\n",
    "    .find(id='main_content')\n",
    "    .find('div', class_='random_images_1')\n",
    "    .find_all('img', class_='nice')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Whoops again. Filter out the result we don't like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = (\n",
    "    soup\n",
    "    .find(id='main_content')\n",
    "    .find('div', class_='random_images_1')\n",
    "    .find_all('img', class_='nice')\n",
    ")\n",
    "nice_imgs = []\n",
    "for img in imgs:\n",
    "    if 'not' not in img.get('class'):\n",
    "        nice_imgs.append(img.get('src'))\n",
    "print(nice_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most important methods:\n",
    "- `.find(tag, id, class_, attrs)`\n",
    "- `.find_all(tag, id, class_, attrs)`\n",
    "- `.get(attribute)`\n",
    "- `.get_text()`\n",
    "\n",
    "#### Exercise:\n",
    "- Find every **visible** headlines (`h1`...`h6`) texts and subtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Querying webpages \n",
    "\n",
    "Collect the articles about migrants from index.hu\n",
    "\n",
    "This will require to search in the site.\n",
    "On the upper-left corner, there is a search icon. Use it, and observe the resulting url:\n",
    "\n",
    "`https://index.hu/24ora/?tol=1999-01-01&ig=2018-04-11&word=1&s=migr치ns`\n",
    "\n",
    "It has multiple parts:\n",
    "- `http://` - protocol\n",
    "- `index.hu` - base url\n",
    "- `/24ora/` - sub url\n",
    "- `?tol=1999-01-01&ig=2018-04-11&word=1&pepe=1&s=migr치ns` - query\n",
    "\n",
    "Let's investigate the query part a little more!  \n",
    "Every query starts with a __`?`__ charater followed by one or more key-value pairs. The key-value pairs are separated with the __`&`__ character. Based on this information, we can extract the query parameters:\n",
    "- `tol`\n",
    "- `ig`\n",
    "- `word`\n",
    "- `s`\n",
    "\n",
    "Use these values to construct our own request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'http://index.hu'\n",
    "sub_url = '/24ora'\n",
    "query = {\n",
    "    'tol': '1999-01-01',\n",
    "    'ig': '2018-04-11',\n",
    "    'word': 1,\n",
    "    's': 'migr치ns'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the requests library to send the query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.get(url=base_url+sub_url, data=query) # some pages requires `params` instead of `data`\n",
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise:\n",
    "- Using the response, extract the urls inside the `<article>` tags!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that only 30 results showed up. We can customize our query to cover shorter amount of timed by replacing __`tol`__ and __`ig`__ parameters with a formattable string: __`'{year}-{month:0>2}-{day:0>2}'`__. This string can be formatted by providing the required parameters:\n",
    "- year\n",
    "- month\n",
    "- day\n",
    "\n",
    "like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'{year}-{month:0>2}-{day:0>2}'.format(year=2016, month=1, day=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a useful library called __`datetime`__. You can use it to generate dates automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "date = datetime.date(1999, 1, 1)\n",
    "day_after_date = date + datetime.timedelta(days=1)\n",
    "day_before_date = date - datetime.timedelta(days=1)\n",
    "today = datetime.date.today()\n",
    "\n",
    "print(day_before_date)\n",
    "print(date)\n",
    "print(day_after_date)\n",
    "print(today)\n",
    "\n",
    "print(today.year, today.month, today.day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a loop which iterate through every day from 1999-01-01 till today and execute the same procedure you created previously. (Pro tip: create a function!) Observe the number of results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. User agents\n",
    "\n",
    "Let's pretend to be a browser instead of a script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_AGENTS = [\n",
    "    # Chrome\n",
    "    'Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1667.0 Safari/537.36',\n",
    "    # Firefox\n",
    "    'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:25.0) Gecko/20100101 Firefox/25.0',\n",
    "    # Opera\n",
    "    'Opera/9.80 (Windows NT 6.0) Presto/2.12.388 Version/12.14',\n",
    "    # Safari\n",
    "    'Mozilla/5.0 (iPad; CPU OS 6_0 like Mac OS X) AppleWebKit/536.26 (KHTML, like Gecko) Version/6.0 Mobile/10A5355d Safari/8536.25',\n",
    "    # Internet Explorer, probably a good idea to leave this one out...\n",
    "    'Mozilla/5.0 (compatible; MSIE 10.6; Windows NT 6.1; Trident/5.0; InfoPath.2; SLCC1; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729; .NET CLR 2.0.50727) 3gpp-gba UNTRUSTED/1.0',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a wrapper function to handle the user-agent string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def get_header(agents):\n",
    "    return {'User-agent': random.choice(agents)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise:\n",
    "Get the main articles from index.hu. Write a function that prints that extracts the current main articles! It should contain:\n",
    "- the title\n",
    "- the article text\n",
    "- the url\n",
    "- every picture from the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://index.hu'\n",
    "index_response = requests.get(url, headers=get_header(USER_AGENTS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Dynamically generated pages\n",
    "\n",
    "Dynamically generated pages could not be parsed by simply downloading them since the generated content won't be present. For this case there is an another library called `selenium`. This library also requires a browser to operate. A browser will be started and every operation will be executed inside that browser. Its path must be set in order to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "download_dir = os.path.expanduser('~')\n",
    "download_dir = os.path.join(download_dir, 'Downloads')\n",
    "\n",
    "os.environ['PATH'] += ';' + download_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Simple lookup\n",
    "- initialize the browser which will be used by the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- request a page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('http://9gag.com/random')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- find items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    media = (\n",
    "        driver\n",
    "        .find_element_by_class_name('post-container')\n",
    "        .find_element_by_tag_name('img')\n",
    "        .get_attribute('src')\n",
    "    )\n",
    "except NoSuchElementException:\n",
    "    media = (\n",
    "        driver\n",
    "        .find_element_by_class_name('post-container')\n",
    "        .find_element_by_tag_name('video')\n",
    "        .find_element_by_tag_name('source')\n",
    "        .get_attribute('src')\n",
    "    )\n",
    "    \n",
    "print(media)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Available finder methods:\n",
    "- `find_element_by_tag_name(tag)`\n",
    "- `find_elements_by_tag_name(tag)`\n",
    "- `find_element_by_class_name(class)`\n",
    "- `find_elements_by_class_name(class)`\n",
    "- `find_element_by_id(id)`\n",
    "- `find_element_by_css_selector(css_selector)`\n",
    "- `find_elements_by_css_selector(css_selector)`\n",
    "\n",
    "#### CSS selectors\n",
    "- `tagname`\n",
    "- `.classname`\n",
    "- `#id`\n",
    "- `[attribute=value]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element_by_css_selector('#individual-post .post-container video source').get_attribute('src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Interaction with the site\n",
    "- request the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://444.hu/kereses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- find search field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_field = driver.find_element_by_css_selector('#content-main input[name=q]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- fill in search query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_field.send_keys('migr치ns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- find submit button and click on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_button = driver.find_element_by_css_selector('#content-main input[type=submit]')\n",
    "submit_button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- find related content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "for article in driver.find_elements_by_class_name('card'):\n",
    "    urls.append(article.find_element_by_tag_name('a').get_attribute('href'))\n",
    "len(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- solution for infinite scrolldown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def scrolldown():\n",
    "    lastHeight = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(1)\n",
    "        newHeight = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if newHeight == lastHeight:\n",
    "            break\n",
    "        lastHeight = newHeight\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrolldown()\n",
    "urls = []\n",
    "for article in driver.find_elements_by_class_name('card'):\n",
    "    urls.append(article.find_element_by_tag_name('a').get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise:\n",
    "Search for a specific brand of car in hasznaltauto.hu and list the car urls from the first page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "\n",
    "- [web scraping tutorial](https://hackernoon.com/web-scraping-tutorial-with-python-tips-and-tricks-db070e70e071)\n",
    "- [selenium with python blogpost](https://realpython.com/blog/python/modern-web-automation-with-python-and-selenium/)\n",
    "- [another selenium blogpost](https://medium.com/@hoppy/how-to-test-or-scrape-javascript-rendered-websites-with-python-selenium-a-beginner-step-by-c137892216aa)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
