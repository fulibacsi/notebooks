{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pics/DL.png\" width=110 align=\"left\" style=\"margin-right: 10px\">\n",
    "\n",
    "# Introduction to Deep Learning\n",
    "\n",
    "## 09. Reinforcement Learning\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "\n",
    "### Purpose\n",
    "\n",
    "Reinforcement Learning(RL) is a type of machine learning technique that enables an agent to learn in an interactive environment by trial and error using feedback from its own actions and experiences.\n",
    "\n",
    "Though both supervised and reinforcement learning use mapping between input and output, unlike supervised learning where feedback provided to the agent is correct set of actions for performing a task, reinforcement learning uses rewards and punishment as signals for positive and negative behavior.\n",
    "\n",
    "As compared to unsupervised learning, reinforcement learning is different in terms of goals. While the goal in unsupervised learning is to find similarities and differences between data points, in reinforcement learning the goal is to find a suitable action model that would maximize the total cumulative reward of the agent. The figure below represents the basic idea and elements involved in a reinforcement learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does it work?\n",
    "\n",
    "<img src=\"./pics/external/rl/environment.jpg\" alt=\"Reinforcement learning Environment - Agent\">\n",
    "<br>Image from <a href=\"https://www.kdnuggets.com/2018/03/5-things-reinforcement-learning.html\">5 Things You Need to Know about Reinforcement Learning</a>, by <a href=\"https://www.kdnuggets.com/author/shweta-bhatt\">Shweta Bhatt</a>, Youplus.\n",
    "\n",
    "Main components of the Reinforcement Learning environment\n",
    "- **Agent**: The controlled entity in the environment which can interact with the environment, execute actions and observe the rewards\n",
    "- **Environment**: The abstract world in which the agent operates\n",
    "- **State**: $S_t$ is the current state of the agent in the environment\n",
    "- **Action**: $A_t$ is the interaction the agent selected to do in the current environment\n",
    "- **Reward**: $R_t$ is the reward the agent receives from the environment as a response for its action. The goal of the agent is to maximize the received reward.\n",
    "- **Policy**: $\\pi \\left(A_t = a | S_t = s, \\theta \\right)$ is the decision strategy of the agent. Based on the current state and the learned parameters the agent selects an action from its repertoire.\n",
    "- **Value**: $V_{\\pi}\\left(S\\right)$ The expected sum of rewards if the agent follows $\\pi$ policy.\n",
    "\n",
    "<img src=\"./pics/external/rl/mari-o.gif\" alt=\"MarI/O\">\n",
    "<br>Image from <a href=\"https://www.youtube.com/watch?v=qv6UVOQ0F44\">MarI/O - Machine Learning for Video Games</a>, by <a href=\"https://www.kdnuggets.com/author/shweta-bhatt\">Seth Bling</a>.\n",
    "\n",
    "The easiest way to get a grasp at Reinforcement Learning is through games: Using Super Mario World as an example, the agent's (aka. Mario's) goal is to get to the end of the level. The **world** around him **is the environment** where he must jump trough the enemies, (optionally) collect coins and kill enemies by jumping on them. Mario get's **rewards for progressing through the level** with a huge bonus when reaching the end but also got punishment (aka **negative reward**) **for walking into enemies**, or **falling into chasms** or letting the time run out. The **states are the position** of Mario **and the actual visible layout**; the **actions are the buttonpresses** on the controller.\n",
    "\n",
    "At each timestamp the agent evaluates the possible actions considering the current state and based on the policy, it chooses the most beneficial action. Leraning an optimal policy requires the agent to **explore** the action space while **exploit** the incoming rewards - maximize the overall reward. This dual problem is called **exploration vs exploitation** tradoff.\n",
    "\n",
    "The matematical frameworks to handle such problems are the **Markov Decision Process**es (MDP). With the help of MDPs, we can describe the environment in RL problems and formalize the process. MDPs describe the environment as a finite set of $S$ states, $A$ possible actions, $R$ reward values, and transitive possibilities $P(s', s | a)$. Real world environments does not contain prior knowledge about the environment - to handle this problem, RL has a set of processes called model-free approches.\n",
    "\n",
    "Since Reinforcement Learning is a whole separate branch of the machine learning algorithms, we won't go deeper into the field, instead let's see how can we realize a specific model free learning method called **Q-learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Two commonly used model-free algorithms are SARSA (State-Action-Reward-State-Action) and Q-learning. The main difference is how they explore their environment. SARSA is an on-policy method which learnd the overall value of an action by using the current policy to project its effectiveness. Q-learning is an off-policy algorithm, it uses an another (otherwise unused) policy to determine the value of an action. These methods are fairly simple but are not able to estimate values for unseen states.\n",
    "\n",
    "To overcome this problem, we'll use the Deep Q-Networks algorithm to estimate Q-values. This algorithm is able to handle low-dimensional discrete action spaces. To handle continuous problems, Deep Deterministic Policy Gradients are a good starting point, however it is out of the scope of this discussion. \n",
    "\n",
    "Deep Q networks revolves around updating Q-values to estimate an action's value in a given state. The value update rule is the core of the Q-learning algorithm and can be written as:\n",
    "\n",
    "$$\\begin{align}\n",
    "    Q^{\\textrm{new}}\\left( s_t, a_t \\right) & =\n",
    "    \\underbrace{Q \\left( s_t, a_t \\right)}_\\textrm{old value} + \n",
    "    \\underbrace{\\alpha}_\\textrm{learning rate} \\cdot\n",
    "    \\overbrace{\\Bigg( \n",
    "        \\underbrace{\n",
    "            \\underbrace{r_t}_\\textrm{reward} +\n",
    "            \\underbrace{\\gamma}_\\textrm{discount factor} \\cdot\n",
    "            \\underbrace{\\max_{a}{Q\\left( s_{t+1}, a \\right)}}_\\textrm{estimate of optimal future value}\n",
    "        }_\\textrm{new value (temporali difference target)} -\n",
    "        \\underbrace{Q\\left( s_t, a_t \\right)}_\\textrm{old value}\n",
    "    \\Bigg)}^\\textrm{temporal difference}\n",
    "\\end{align}$$\n",
    "\n",
    "DQNs will learn online, meaning that we donâ€™t simply create a bunch of trial/training data and feed it into the model, but we create training data through the trials we run and feed this information into it directly after running the trial instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Practice\n",
    "\n",
    "DQNs are not part of any widespread deep learning library, so we'll have to implement the algoritm ourselves. To make it easier to understand the process let's consider a small (but not at all straightforward!) problem:\n",
    "\n",
    "#### Building a hill-climber car\n",
    "\n",
    "Since Reinforcement Learning living its renaissance there are frameworks to easily simulate the RL environment. One of the most popular framework is called gym. It contains the problem [Mountain Car](https://gym.openai.com/envs/MountainCar-v0/) where the goal is to drive up a car to a hill. The car in the problem does not have enough power to go up the hill, so the agent has to rely on building up a momentum. \n",
    "\n",
    "The state of the agent is the elevation of the car and the velocity; the available actions are accelerate to the left (0), don't accelerate (1), and accelerate to the right (2). The simulation ends if the car's elevation is above 0.5, or more than 200 episodes has passed.\n",
    "\n",
    "First, let's see the problem using random actions in each episode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \n",
    "    def __init__(self, problem):\n",
    "        self.env = gym.make(problem)\n",
    "        \n",
    "    def __enter__(self):\n",
    "        self.env.reset()\n",
    "        return self.env\n",
    "    \n",
    "    def __exit__(self, *args, **kwargs):\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I. Random solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Environment('MountainCar-v0') as env:\n",
    "    for episode in range(1000):\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "        print(f\"Step {episode}:\")\n",
    "        print(f\"action: {action}\")\n",
    "        print(f\"observation: {observation}\")\n",
    "        print(f\"reward: {reward}\")\n",
    "        print(f\"done: {done}\")\n",
    "        print(f\"info: {info}\")\n",
    "\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II. Naive solution\n",
    "\n",
    "Build training data from random experiments, and train a NN on successful experiment samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_data_preparation(env, intial_games, episodes):\n",
    "    env.reset()\n",
    "    \n",
    "    training_data = []\n",
    "    accepted_scores = []\n",
    "    for episode in range(intial_games):\n",
    "        score = 0\n",
    "        memory = []\n",
    "        previous_state = []\n",
    "        for episode in range(episodes):\n",
    "            action = random.randrange(0, 3)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            if len(previous_state) > 0:\n",
    "                memory.append([previous_state, action])\n",
    "                \n",
    "            previous_state = state\n",
    "            if state[0] > -0.2:\n",
    "                reward = 1\n",
    "            \n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "        if score >= score_requirement:\n",
    "            accepted_scores.append(score)\n",
    "            for data in memory:\n",
    "                state_memory, action_memory = data\n",
    "                encoded_action = [0, 0, 0]\n",
    "                encoded_action[action_memory] = 1\n",
    "                training_data.append([state_memory, encoded_action])\n",
    "        \n",
    "        env.reset()\n",
    "    \n",
    "    print(accepted_scores)\n",
    "    return training_data\n",
    "\n",
    "\n",
    "def build_model(input_size, output_size):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(128, input_dim=input_size, activation='relu'))\n",
    "    model.add(Dense(52, activation='relu'))\n",
    "    model.add(Dense(output_size, activation='linear'))\n",
    "    \n",
    "    model.compile(loss='mse', optimizer=Adam())\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(training_data):\n",
    "    X = np.array([i[0] for i in training_data]).reshape(-1, len(training_data[0][0]))\n",
    "    y = np.array([i[1] for i in training_data]).reshape(-1, len(training_data[0][1]))\n",
    "    \n",
    "    model = build_model(input_size=len(X[0]), output_size=len(y[0]))\n",
    "    model.fit(X, y, epochs=10)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate train data and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "episodes = 200\n",
    "score_requirement = -198\n",
    "intial_games = 10000\n",
    "\n",
    "training_data = model_data_preparation(env, intial_games, episodes)\n",
    "trained_model = train_model(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test our trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "choices = []\n",
    "for each_game in range(10):\n",
    "    score = 0\n",
    "    memory = []\n",
    "    prev_state = []\n",
    "    for episode in range(episodes):\n",
    "        env.render()\n",
    "        if len(prev_state) == 0:\n",
    "            action = random.randrange(0,2)\n",
    "        else:\n",
    "            action = np.argmax(trained_model.predict(prev_state.reshape(-1, len(prev_obs)))[0])\n",
    "        \n",
    "        choices.append(action)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        memory.append([new_state, action])\n",
    "        \n",
    "        prev_state = new_state\n",
    "        score += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    env.reset()\n",
    "    scores.append(score)\n",
    "\n",
    "env.close()\n",
    "print(scores)\n",
    "print('Average Score:', sum(scores)/len(scores))\n",
    "print('choice 1:{} choice 0:{} choice 2:{}'.format(choices.count(1) / len(choices),\n",
    "                                                   choices.count(0) / len(choices),\n",
    "                                                   choices.count(2) / len(choices)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III. DQN solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A success counter helper class to track results of successful episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Success:\n",
    "\n",
    "    def __init__(self, threshold=10):\n",
    "        self.sum = 0\n",
    "        self.last10 = []\n",
    "        self.last10sum = sum(self.last10)\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def __iadd__(self, value):\n",
    "        self.sum += value\n",
    "        self.last10.append(value)\n",
    "        self.last10 = self.last10[-10:]\n",
    "        self.last10sum = sum(self.last10)\n",
    "        return self\n",
    "\n",
    "    def __add__(self, value):\n",
    "        new = Success()\n",
    "        new.sum = self.sum\n",
    "        new.last10 = self.last10\n",
    "        new.last10sum = self.last10sum\n",
    "        new.threshold = self.threshold\n",
    "        return new.__iadd__(value)\n",
    "\n",
    "    def __bool__(self):\n",
    "        return sum(self.last10) >= self.threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.gamma = 0.99  # discount factor\n",
    "\n",
    "        # control exploration vs explitation\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_decay = 0.95\n",
    "        self.epsilon_min = 0.01\n",
    "\n",
    "        self.memory = deque(maxlen=20000)\n",
    "        self.num_episodes = 400\n",
    "        self.max_steps = 201  # max is 200\n",
    "        \n",
    "        # NeuralNet parameters\n",
    "        self.learing_rate = 0.001\n",
    "        self.batch_size = 32\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "\n",
    "        self.sync_models()\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"Generate DNN to approximate Q-value.\n",
    "        \n",
    "        Create a network with:\n",
    "        - 1 dense layer with 24 neurons using relu activation\n",
    "        - 1 dense layer with 48 neurons using relu activation\n",
    "        - 1 dense layer with action space neurons using linear activation\n",
    "        \"\"\"\n",
    "        model = Sequential()\n",
    "        state_shape = self.env.observation_space.shape\n",
    "        action_shape = self.env.action_space.n\n",
    "\n",
    "        model.add(Dense(24, activation='relu', input_shape=state_shape))\n",
    "        model.add(Dense(48, activation='relu'))\n",
    "        model.add(Dense(action_shape, activation='linear'))\n",
    "        \n",
    "        optimizer = Adam(learning_rate=self.learing_rate)\n",
    "        model.compile(loss='mse', optimizer=optimizer)\n",
    "        return model\n",
    "\n",
    "    def sync_models(self):\n",
    "        \"\"\"Syncronize the learned and the off-policy model.\"\"\"\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Select next action given the actual state.\"\"\"\n",
    "        # Exploration\n",
    "        if np.random.rand(1) < self.epsilon:\n",
    "            return np.random.randint(0, self.env.action_space.n)\n",
    "        # Exploitation\n",
    "        return np.argmax(self.model.predict(state)[0])\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Update epsilon value by decaying it until reaching minimum value.\"\"\"\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon = max(self.epsilon_min, \n",
    "                               self.epsilon * self.epsilon_decay)\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        \"\"\"Save episode results for later use.\"\"\"\n",
    "        self.memory.append([state, action, reward, new_state, done])\n",
    "\n",
    "    def generate_batch(self):\n",
    "        \"\"\"Generate training batch from the saved memory by random sampling.\"\"\"\n",
    "        samples = np.array(random.sample(self.memory, self.batch_size))\n",
    "        \n",
    "        states, actions, rewards, new_states, dones = np.hsplit(samples, 5)\n",
    "\n",
    "        states = np.concatenate(np.squeeze(states[:]), axis=0)    # [batch_size x 2]\n",
    "        new_states = np.concatenate(np.concatenate(new_states))   # [batch_size x 2]\n",
    "        rewards = rewards.reshape(self.batch_size,).astype(float) # [batch_size]\n",
    "        actions = actions.reshape(self.batch_size,).astype(int)   # [batch_size]\n",
    "        dones = dones.reshape(self.batch_size,).astype(bool)      # [batch_size]\n",
    "        notdones = (~dones).astype(float)\n",
    "    \n",
    "        return states, actions, rewards, new_states, notdones\n",
    "\n",
    "    def replay(self):\n",
    "        \"\"\"Updating Q values using the actual model and an off-policy model.\n",
    "        \n",
    "        The update rule is:\n",
    "        Q(s_t, a_t) = Q(s_t, a_t) + alpha * (r_t + gamma * Q^*(s_t+1, a_t) - Q(s_t, a_t))\n",
    "        \n",
    "        Where the model is responsible of generating the Q(s_t, a_t) values, \n",
    "        and the off-policy model is responsible of generating the Q^*(s_t+1, a_t) values.\n",
    "        \n",
    "        The update is performed by the neural network backpropagation by setting the target\n",
    "        value to r_t + gamma * Q^*(s_t+1, a_t).\n",
    "        \"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # I. Generating examples from memory\n",
    "        states, actions, rewards, new_states, notdones = self.generate_batch()\n",
    "        # II. Generating Q(s_t, a_t)\n",
    "        targets = self.model.predict(states)\n",
    "        indices = np.arange(self.batch_size)\n",
    "        \n",
    "        # III. Using off-policy model to predict future Q values: \n",
    "        # Q^*(s_t+1, a_t)\n",
    "        Q_futures = self.target_model.predict(new_states).max(axis = 1)\n",
    "        \n",
    "        # IV. Generating temporal difference target\n",
    "        # td = r_t + gamma * Q^*(s_t+1, a_t)\n",
    "        targets[(indices, actions)] = rewards + self.gamma * Q_futures * notdones\n",
    "        \n",
    "        # V. Updating Q(s_t, a_t) by executing 1 training step\n",
    "        self.model.fit(states, targets, epochs=1, verbose=0)\n",
    "\n",
    "    def optimize_model(self, state, eps, render=True):\n",
    "        score = 0\n",
    "        max_position = -99\n",
    "\n",
    "        for step in range(self.max_steps):\n",
    "            # Show the animation every 50 eps\n",
    "            if render and eps % 50 == 0:\n",
    "                env.render()\n",
    "\n",
    "            # select action\n",
    "            action = self.act(state)\n",
    "            # observe environment after taking action\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            new_state = new_state.reshape(1, 2)\n",
    "\n",
    "            # Keep track of max position\n",
    "            position = new_state[0][0]\n",
    "            if position > max_position:\n",
    "                max_position = position\n",
    "\n",
    "            # Adjust reward for task completion\n",
    "            if position >= 0.5:\n",
    "                reward += 10\n",
    "            \n",
    "            # Save episode results\n",
    "            self.remember(state, action, reward, new_state, done)\n",
    "            # Train network on observed events\n",
    "            self.replay()\n",
    "            \n",
    "            # Update state and value reward\n",
    "            state = new_state\n",
    "            score += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        self.sync_models()\n",
    "        self.decay_epsilon()\n",
    "        \n",
    "        return step < 199\n",
    "\n",
    "    def fit(self, render=True):\n",
    "        success = Success()\n",
    "        episodes = tqdm(range(self.num_episodes))\n",
    "        for episode in episodes:\n",
    "            state = env.reset().reshape(1, 2)\n",
    "            success += self.optimize_model(state, episode, render)\n",
    "\n",
    "            episodes.set_postfix_str(f'overall: {success.sum}, '\n",
    "                                     f'last10: {success.last10sum}')\n",
    "            if success:\n",
    "                print(f'10 success in a row, stopping early at episode {episode}.')\n",
    "                episodes.close()\n",
    "                break\n",
    "        \n",
    "        return self\n",
    "\n",
    "                \n",
    "def play(env, model, n=1):\n",
    "    for _ in range(n):\n",
    "        done = False\n",
    "        state = env.reset().reshape(1, 2)\n",
    "        while not done:\n",
    "            env.render()\n",
    "            action = model.act(state)\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            state = new_state.reshape(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Environment('MountainCar-v0') as env:\n",
    "    env.seed(42)\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    dqn = DQN(env=env).fit(render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Environment('MountainCar-v0') as env:\n",
    "    play(env, dqn, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "### Solve the [CartPole](https://gym.openai.com/envs/CartPole-v1/) problem\n",
    "\n",
    "Use the DQN algorithm to learn how to balance a pole on top of a moving cart. This is actually an easier task, since our action space is reduced greatYou have to modify the previously created class slightly. Can you make the class more general?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good job!\n",
    "\n",
    "Thank you for your attention! Consider solving the exercises from the [DL 10 Home Assignment II.](./DL_10_Home_Assignment_II.ipynb) notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
