{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pics/DL.png\" width=110 align=\"left\" style=\"margin-right: 10px\">\n",
    "\n",
    "# Introduction to Deep Learning\n",
    "\n",
    "## 07. Embeddings\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Word](https://keras.io/layers/embeddings/) [Embeddings](https://radimrehurek.com/gensim/models/word2vec.html)\n",
    "\n",
    "<div style=\"display: inline-block;\">\n",
    "    <img src=\"./pics/external/w2v/w2v-king-queen-vectors.png\" width=300 align='left'>\n",
    "    <img src=\"./pics/external/w2v/w2v-king-queen-composition.png\" width=300 align='left'>\n",
    "</div>\n",
    "\n",
    "<div style='align: clear'/>\n",
    "<br>Images from <a href=\"https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/\">the morning paper</a>\n",
    "\n",
    "### Purpose \n",
    "\n",
    "Embedding is a method to generate vectorial representation of any otherwise hard to quantify objects. One of the most common embedding is word embedding, however embedding is not exclusive to natural language processing.\n",
    "\n",
    "Classical NLP quantified words by using different calculated syntactic descriptors, eg. bag of word representation. Embedding is a process where a 1 dimensional word is transformed into a multidimensional embedding space. These vectors represents the semantic meaning of the word. The intuition of the embedding model is that words with similar contexts have similar meaning.\n",
    "There are several way to generate such embeddings, one of the is using neural networks.\n",
    "\n",
    "Embedding words has an interesting \"side effect\" which you can see on the images above: the semantic vectors saved the relations of the words, eg. if you substract the vector of `man` from the vector of `king` and add the vector of `woman` you'll get the `queen`'s vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do they work?\n",
    "\n",
    "<img src=\"./pics/external/w2v/focus_word.png\" width=500>\n",
    "<br>Image from <a href=\"http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\">Word2Vec Tutorial - The Skip-Gram Model.</a>, by <a href=\"http://mccormickml.com/\">Chris McCormick</a>\n",
    "\n",
    "Using neural networks there are multiple ways to generate (word) embeddings. One of the embedding model is called Word2Vec, and it has two main approach to learn word-embeddings: \n",
    "- the continous bag-of-words (CBOW): the model predicts the selected word from the context words in the surrounding window (word order invariant)\n",
    "- the skip-gram architecture:  the model predicts the context words from the selected word (context words are weighted by their distance to the selected word)\n",
    "\n",
    "<img src=\"./pics/external/w2v/CBOW_multiword.png\" width=300>\n",
    "<br>Image from <a href=\"http://www.claudiobellei.com/2018/01/06/backprop-word2vec/\">The backpropagation algorithm for Word2Vec</a>, by <a href=\"http://www.claudiobellei.com/\">Claudio Bellei</a>\n",
    "\n",
    "**CBOW** architecture contains:\n",
    "- 1 **input layer** where the inputs are one-hot encoded context words of $V$ dimension (vocabulary size)\n",
    "- 1 **hidden layer** with $N$ neurons (embedding size) with linear activation function \n",
    "- 1 **output layer** with $V$ neuron and softmax\n",
    "\n",
    "The $W$ weights for the hidden layer has $V \\times N$ dimensionality and the $W'$ on the output layer has $N \\times V$ size. Since the activation function in the hidden layer is linear, the output can be expressed as:\n",
    "\n",
    "$$\\begin{align}\n",
    "    \\textbf{h} & = \\frac{1}{C} W^T \\sum_{c=1}^C\\textbf{x}^{(c)} = W^T\\overline{\\textbf{x}} \\\\\n",
    "    \\textbf{u} & = W'^T\\textbf{h}= \\frac{1}{C}\\sum_{c=1}^CW'^T W^T\\textbf{x}^{(c)}=W'^T W^T\\overline{\\textbf{x}} \\\\\n",
    "    \\textbf{y} & = \\mathbb{S}\\textrm{oftmax}(\\textbf{u})= \\mathbb{S}\\textrm{oftmax}\\left( W'^T W^T\\overline{\\textbf{x}}\\right)\n",
    "\\end{align}$$\n",
    "\n",
    "where $\\overline{\\textbf{x}}$ is the average of the input words, $\\overline{\\textbf{x}}=\\sum_{c=1}^C\\textbf{x}^{(c)}/C$. \n",
    "\n",
    "**Skipgram** architecture contains:\n",
    "- 1 **input layer** where the input is a one-hot encoded context words of $V$ dimension (vocabulary size)\n",
    "- 1 **hidden layer** with $N$ neurons (embedding size) with linear activation function \n",
    "- 1 **output layer** with $C \\times V$ neuron and softmax outputs\n",
    "\n",
    "<img src=\"./pics/external/w2v/Skipgram.png\" width=300>\n",
    "<br>Image from <a href=\"http://www.claudiobellei.com/2018/01/06/backprop-word2vec/\">The backpropagation algorithm for Word2Vec</a>, by <a href=\"http://www.claudiobellei.com/\">Claudio Bellei</a>\n",
    "\n",
    "$$\\begin{align}\n",
    "    \\textbf{h}   & = W^T\\textbf{x} \\\\\n",
    "    \\textbf{u}_c & = W'^T\\textbf{h} = W'^TW^T\\textbf{x} \\hspace{4.8cm} c=1, \\dots, C \\\\\n",
    "    \\textbf{y}_c & = \\mathbb{S}\\textrm{oftmax}(\\textbf{u}) = \\mathbb{S}\\textrm{oftmax}(W'^TW^T\\textbf{x}) \\hspace{2cm} c=1, \\dots, C\n",
    "\\end{align}$$\n",
    "\n",
    "where $\\mathbf{y}_1 = \\mathbf{y}_2 \\dots = \\mathbf{y}_C$.\n",
    "\n",
    "After training, we use the rows of the $W$ matrix as word embedding vectors.\n",
    "\n",
    "<img src=\"./pics/external/w2v/w2v_weight_matrix_lookup_table.png\" width=300>\n",
    "<br>Image from <a href=\"http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\">Word2Vec Tutorial - The Skip-Gram Model.</a>, by <a href=\"http://mccormickml.com/\">Chris McCormick</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Word2Vec models use backpropagation method to find the optimal weights. We won't discuss the gradient computation in details, one can find the update rules by finding the gradients for the logloss cost function. \n",
    "\n",
    "**CBOW**\n",
    "Cost function:  \n",
    "$$\\mathcal{L} = -u_{j^*} + \\log \\sum_i \\exp{(u_i)}$$\n",
    "\n",
    "Gradients:\n",
    "$$\\begin{align}\n",
    "    \\frac{\\partial\\mathcal{L}}{\\partial W'} & = (W^T\\overline{\\textbf{x}}) \\otimes \\textbf{e} \\\\\n",
    "    \\frac{\\partial \\mathcal{L}}{\\partial W} & = \\overline{\\textbf{x}}\\otimes(W'\\textbf{e}) \n",
    "\\end{align}$$\n",
    "\n",
    "**Skipgram**\n",
    "\n",
    "Cost function:  \n",
    "$$\\mathcal{L} = -\\sum_{c=1}^C u_{c,j^*} + \\sum_{c=1}^C \\log \\sum_{j=1}^V \\exp(u_{c,j})$$\n",
    "\n",
    "Gradients:\n",
    "$$\\begin{align}\n",
    "    \\frac{\\partial\\mathcal{L}}{\\partial W'} & = (W^T\\textbf{x}) \\otimes \\sum_{c=1}^C\\textbf{e}_c \\\\\n",
    "    \\frac{\\partial \\mathcal{L}}{\\partial W} & = \\textbf{x}\\otimes\\left(W'\\sum_{c=1}^C\\textbf{e}_c\\right)\n",
    "\\end{align}$$\n",
    "\n",
    "However the more interesting part about the training is the way we speed up the process, considering a 300 dimension embedding with a voaculary with 10.000 words results 3.000.000 weights to update. That is a huge amount, so there are some techniques applied during training to handle the size of the problem.\n",
    "\n",
    "- **Subsampling frequent words**: Randomly drop a word from the training data. We select this random word proportional to its frequency in the corpus, so frequent words are more likely to dropped randomly. This selection is also controlled by a `sample` parameter which modifies the selection probability: the smaller this value the more likely a word is dropped.\n",
    "- **Negative sampling**: Instead of updating every words' weight, we select only a few negative words randomly (where the associated one-hot encoding is 0) which is updated at each step (plus the positive word's as well). The number is generally between 5-20 for smaller datasets, 2-5 for larger ones. The selection of the words are inversily proportional to the word's frequency. \n",
    "- **Word Pairs and “Phrases”**: Frequent word pairs in the corpus are selected instead of individual words to reduce vocabulary size. Selection is based on cooccurence ratio, but it is less likely to include frequent words, to prevent creating pairs containing common words, like \"the\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other models\n",
    "\n",
    "- **Glove**: Similar to Word2Vec models, but it does not use neural networks. In GloVe, the loss function is the difference between the product of word embeddings and the log of the probability of co-occurrence. We try to reduce that and use SGD but solve it as we would solve a linear regression. It creates a global co-occurrence matrix by estimating the probability a given word will co-occur with other words. Ideally it should perform better than Word2Vec but in practice their performance is on par.\n",
    "- **BERT** and **ELMO**: Very recent advances of the field. They try to address the problem with \"general\" word embeddings - every word has a context-independent semantic representation without instance specific embedding. For example the \"apple\" will mean the fruit if trained on general text vs the company when trained on tech documents. They create word-sense embeddings for every context. So there are infinite number of embeddings for each word.\n",
    "- **FastText**: FastText is basically an extension of word2vec model which treats each word as a sequence of character n-grams. So the vector for a word is made of the sum of these character n-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Practice\n",
    "\n",
    "#### Learning simple word-embeddings\n",
    "\n",
    "Let's create a toy dataset for sentiment analysis. We will use the documents to learn word embeddings in the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Activation\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ['Well done!',\n",
    "        'Good work',\n",
    "        'Great effort',\n",
    "        'nice work',\n",
    "        'Excellent!',\n",
    "        'Weak',\n",
    "        'Poor effort!',\n",
    "        'not good',\n",
    "        'poor work',\n",
    "        'Could have done better.']\n",
    "\n",
    "labels = np.array([1, 1, 1, 1, 1,\n",
    "                   0, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode the documents\n",
    "vocab_size = 50\n",
    "encoded_docs = [one_hot(doc, vocab_size) for doc in docs]\n",
    "encoded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad documents to a max length of 4 words\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "padded_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding has three important parameters to set:\n",
    "- input_dim: the size of the vocabulary\n",
    "- output_dim: the size of the generated embedding\n",
    "- input_length: the length of the input sequence (the maximum number of words in a document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 8\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, embedding_size, input_length=max_length),\n",
    "    Flatten(),\n",
    "    Dense(1),\n",
    "    Activation('sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(padded_docs, labels, epochs=50, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('loss: {}, accuracy: {}'.format(*score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "#### News classification\n",
    "\n",
    "Classify the 20newsgroups dataset while building an embedding. As a first step, try to separate the atheism documents (`alt.atheism`) from the christian documents (`soc.religion.christian`).\n",
    "\n",
    "##### 1. Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from livelossplot import PlotLossesKeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian'] \n",
    "newsgroups = fetch_20newsgroups(subset='train', shuffle=True, categories=categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Transform documents\n",
    "\n",
    "Transform the documents to only contain maximum 144 words (old twitter habits die slowly) and limit the vocabulary in 5000 words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = None \n",
    "max_length = None\n",
    "embedding_size = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: transform the documents and labels as necessary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Build the model\n",
    "\n",
    "Described here:\n",
    "\n",
    "|    Layer   | Param # | Notes                                                   |\n",
    "|------------|---------|---------------------------------------------------------|\n",
    "| Embedding  |  250000 | Set the embedding size to `50`                          |\n",
    "| Conv1D     |   12550 | 1D convolution with `50` filters with kernel size `5`   |\n",
    "| MaxPooling |       0 | 1D max pooling with `5` pool size                       |\n",
    "| Conv1D     |    1510 | 1D convolution with `10` filters with kernel size `3`   |\n",
    "| MaxPooling |       0 | 1D max pooling with `3` pool size                       |\n",
    "| Flatten    |       0 |                                                         |\n",
    "| Dense      |    1620 | `20` neuron, `relu` activation                          |\n",
    "| Dense      |      ?? | ? decide the output neuron size and activation function |\n",
    "\n",
    "Questions to answer:\n",
    "- What should be the output layer's neuron count?\n",
    "- What should be the output nonlinearity?\n",
    "- Which cost function are appropriate here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "news_model = Sequential()\n",
    "\n",
    "# FILL IN\n",
    "\n",
    "# Compile the model\n",
    "news_model.compile()  # FILL IN\n",
    "news_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Train the model\n",
    "\n",
    "Use 20% of the data as validation, set the batch size to 64, the epoch count to 10. Use the `PlotLossesKeras` callback and turn off the logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "\n",
    "news_model.fit(X_train, y_train, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Evaluate your model\n",
    "\n",
    "Generate the test data and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "#### Siamese network for document similarity and Keras Functional API\n",
    "\n",
    "##### Functional API\n",
    "\n",
    "So far we only had to build models with sequential layer structure. Large part of the problems are solvable using only these models but there are some advanced architecture which has non-sequential layout, such case is model is with multiple inputs, multiple outputs, or layers with shared weights. With functional API we have more control over the network.\n",
    "\n",
    "Let's reproduce the same model we used for the sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visible = Input(shape=(max_length,))\n",
    "embedding = Embedding(vocab_size, 8)(visible)\n",
    "flatten = Flatten()(embedding)\n",
    "output = Dense(1, activation='sigmoid')(flatten)\n",
    "\n",
    "model = Model(inputs=visible, outputs=output)\n",
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "model.evaluate(padded_docs, labels, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Siamese network\n",
    "\n",
    "Siamese network is a special structure which has two inputs that is processed separately by two identical subnetworks (these subnetworks are even sharing the same weights) and then merged together to produce a desired input. Such network is able to compare two documents.\n",
    "\n",
    "In our case, we want to produce a similarity score between the two documents so in our merging layer we produce the cosine distance between the vectors produced by the shared embedded networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Create the shared layers without inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = Embedding(vocab_size, 8)\n",
    "flatten = Flatten()\n",
    "hidden = Dense(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Generate the two shared subnetworks using separate inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first document\n",
    "first = Input(shape=(max_length,))\n",
    "first_embedding = embedding(first)\n",
    "first_flatten = flatten(first_embedding)\n",
    "first_hidden = hidden(first_flatten)\n",
    "\n",
    "# second document\n",
    "second = Input(shape=(max_length,))\n",
    "second_embedding = embedding(second)\n",
    "second_flatten = flatten(second_embedding)\n",
    "second_hidden = hidden(second_flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Merge the layers by computing the cosine distance between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = Dot(axes=1, normalize=True)([first_hidden, second_hidden])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese = Model(inputs=[first, second], outputs=output)\n",
    "siamese.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "\n",
    "plot_model(siamese, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Generate training data from reusing one-hot encoded padded sentiment documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_docs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have 10 documents, 5 with positive and 5 with negative sentiment value. We could easily generate every combination (2 x 25 cases for the matching cases and 2 x 25 cases for the opposite cases for both direction) but let's assume for a second that we have much more sample data. In this case we'd like to generate training samples randomly. For that purpose we select random cases for every possible scenario and generate the samples that way. Let's generate the same amount of cases as we would by generating every possible combination:\n",
    "\n",
    "| first  | second | outcome |\n",
    "|--------|--------|---------|\n",
    "| 25 pos | 25 pos |    1    |\n",
    "| 25 pos | 25 neg |   -1    |\n",
    "| 25 neg | 25 pos |   -1    |\n",
    "| 25 neg | 25 neg |    1    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_indices(sentiment, size=50):\n",
    "    start = 0 + 5 * (sentiment == 'negative')\n",
    "    end = 5 + 5 * (sentiment == 'negative')\n",
    "    return np.random.randint(start, end, size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first input: 50 positive, 50 negative\n",
    "first_docs = padded_docs[np.concatenate((generate_indices('positive'), \n",
    "                                         generate_indices('negative')))]\n",
    "\n",
    "# second input: 25 positive, 25 negative, 25 positive, 25 negative\n",
    "second_docs = padded_docs[np.concatenate((generate_indices('positive', 25), \n",
    "                                          generate_indices('negative', 25),\n",
    "                                          generate_indices('positive', 25), \n",
    "                                          generate_indices('negative', 25)))]\n",
    "\n",
    "# labels: 50 positive, 100 negative, 50 positive\n",
    "document_distances = np.vstack((np.ones((25, 1)), np.ones((50, 1)) * -1, np.ones((25, 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese.fit([first_docs, second_docs], document_distances, \n",
    "            batch_size=10, epochs=20, validation_split=0.1, \n",
    "            callbacks=[PlotLossesKeras()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's try out our prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in np.random.randint(1, len(first_docs), size=20):\n",
    "    doc_pair = [first_docs[index-1:index], \n",
    "                second_docs[index-1:index]]\n",
    "    expected = document_distances[index-1][0]\n",
    "    predicted = siamese.predict(doc_pair).flatten()[0]\n",
    "    print(f'expected distance: {expected:.2f} predicted: {predicted:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "#### Newsgroups document similarity\n",
    "\n",
    "Let's try to build a similar network for the 20 newsgroups. This time let's use all of the topics and generate the pairs using the following rules:\n",
    "If two documents are from\n",
    "- two opposite group: their similarity is -1.0\n",
    "- the same specific group: they are similar, their expected similarity is 1\n",
    "- the same subgroup: their similarity is 0.5\n",
    "- the same general group: their similarity is 0.1\n",
    "- any other case: 0.0\n",
    "\n",
    "| main group |   subgroup  | group                    | opposite groups                                |\n",
    "|------------|-------------|--------------------------|------------------------------------------------|\n",
    "|    comp    | graphics    | comp.graphics            |                                                |\n",
    "|    comp    | os          | comp.os.ms-windows.misc  |                                                |\n",
    "|    comp    | sys         | comp.sys.ibm.pc.hardware |                                                |\n",
    "|    comp    | sys         | comp.sys.mac.hardware    |                                                |\n",
    "|    comp    | windows     | comp.windows.x\t          |                                                |\n",
    "|    rec     |             | rec.autos\t              |                                                |\n",
    "|    rec     |             | rec.motorcycles\t      |                                                |\n",
    "|    rec     | sport       | rec.sport.baseball\t      |                                                |\n",
    "|    rec     | sport       | rec.sport.hockey\t      |                                                |\n",
    "|    sci     |             | sci.crypt\t              |                                                |\n",
    "|    sci     |             | sci.electronics\t      |                                                |\n",
    "|    sci     |             | sci.med\t              |                                                |\n",
    "|    sci     |             | sci.space\t              |                                                |\n",
    "|    misc    |             | misc.forsale             |                                                |\n",
    "|    talk    | politics    | talk.politics.misc       |                                                |\n",
    "|    talk    | politics    | talk.politics.guns\t      |                                                |\n",
    "|    talk    | politics    | talk.politics.mideast    |                                                |\n",
    "|    talk    | religion    | talk.religion.misc       | alt.atheism                                    |\n",
    "|    alt     |             | alt.atheism\t          | soc.religion.christian, <br>talk.religion.misc |\n",
    "|    soc     | religion    | soc.religion.christian   | alt.atheism                                    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5000\n",
    "max_length = 144\n",
    "embedding_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian',              # opposites\n",
    "              'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware',  # same subgroup\n",
    "              'rec.autos', 'rec.motorcycles']                       # same group\n",
    "             \n",
    "newsgroups = fetch_20newsgroups(subset='train', shuffle=True, categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opposites = {'alt.atheism': ['soc.religion.christian', 'talk.religion.misc'],\n",
    "             'soc.religion.christian': ['alt.atheism'],\n",
    "             'talk.religion.misc': ['alt.atheism']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxonomy(group_id, target_names):\n",
    "    group = target_names[group_id]\n",
    "    if group.count('.') == 1:\n",
    "        main, _ = group.split('.')\n",
    "        return main, None, group\n",
    "    \n",
    "    main, sub, *_ = group.split('.')\n",
    "    return main, sub, group\n",
    "\n",
    "\n",
    "def get_score(index_1, index_2, target_names, opposites):\n",
    "    # handle same group\n",
    "    if index_1 == index_2:\n",
    "        return True\n",
    "    \n",
    "    main_1, sub_1, group_1 = get_taxonomy(index_1, target_names)\n",
    "    main_2, sub_2, group_2 = get_taxonomy(index_2, target_names)\n",
    "    \n",
    "    # handle opposites\n",
    "    opposite = opposites.get(group_1, [])\n",
    "    if group_2 in opposite:\n",
    "        return -1.0\n",
    "    \n",
    "    # handle same main / subgroups\n",
    "    if main_1 == main_2:\n",
    "        # same subgroup\n",
    "        if sub_1 is not None and sub_2 is not None and sub_1 == sub_2:\n",
    "            return 0.5\n",
    "        # same main group\n",
    "        return 0.1\n",
    "    \n",
    "    # unrelated\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_taxonomy('soc.religion.christian', newsgroups.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('same score: ', get_score('comp.sys.ibm.pc.hardware', 'comp.sys.ibm.pc.hardware', newsgroups.target_names, opposites))\n",
    "print('same sub score: ', get_score('comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', newsgroups.target_names, opposites))\n",
    "print('same main score: ', get_score('rec.autos', 'rec.motorcycles', newsgroups.target_names, opposites))\n",
    "print('orthogonal score: ', get_score('rec.autos', 'comp.sys.mac.hardware', newsgroups.target_names, opposites))\n",
    "print('opposites score: ', get_score('alt.atheism', 'soc.religion.christian', newsgroups.target_names, opposites))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate 3000 training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(newsgroups.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 3000\n",
    "vocab_size = len(tokenizer.word_index) + 1  # all recognized words + 1 for missing word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_docs = tokenizer.texts_to_sequences(newsgroups.data)\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "labels = newsgroups.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(docs, labels, target_names, opposites, sample_size=5000):\n",
    "    f = np.random.randint(0, len(labels), size=sample_size)\n",
    "    s = np.random.randint(0, len(labels), size=sample_size)\n",
    "    similarity = np.array([get_score(labels[fi], labels[si], target_names, opposites) \n",
    "                           for fi, si in zip(f, s)])\n",
    "\n",
    "    return docs[f], docs[s], similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fX, sX, y = generate_sample(padded_docs, labels, \n",
    "                            newsgroups.target_names, opposites, \n",
    "                            sample_size)\n",
    "\n",
    "Counter(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(input_layer, network):\n",
    "    model = input_layer\n",
    "    for layer in network:\n",
    "        model = layer(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-trained models\n",
    "\n",
    "Let's use a pre-trained embedding model: Google pre-trained google news embedding. It is available to download from [here](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit). \n",
    "Using pre-trained networks have the advantage to add external knowledge to the network, potentially speeding up training.  \n",
    "The imported layer can be used with \"frozen\" weights, so the training wont effect the layer or you can adjust the initial weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "filename = './data/googlenews/GoogleNews-vectors-negative300.bin.gz'\n",
    "w2v = gensim.models.KeyedVectors.load_word2vec_format(filename, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = w2v.get_keras_embedding(train_embeddings=False)\n",
    "conv_1 = Conv1D(50, 5, activation='relu')\n",
    "pool_1 = MaxPooling1D(5)\n",
    "conv_2 = Conv1D(10, 3, activation='relu')\n",
    "pool_2 = MaxPooling1D(3)\n",
    "flatten = Flatten()\n",
    "hidden = Dense(20, activation='tanh')\n",
    "\n",
    "common = [embedding, conv_1, pool_1, conv_2, pool_2, flatten, hidden]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_input = Input(shape=(max_length,))\n",
    "first_hidden = generate_model(first_input, common)\n",
    "\n",
    "second_input = Input(shape=(max_length,))\n",
    "second_hidden = generate_model(second_input, common)\n",
    "\n",
    "output = Dot(axes=1, normalize=True)([first_hidden, second_hidden])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_siamese = Model(inputs=[first_input, second_input], outputs=output)\n",
    "news_siamese.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "\n",
    "plot_model(news_siamese, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_siamese.fit([fX, sX], y, \n",
    "                 batch_size=64, epochs=10, shuffle=True,\n",
    "                 validation_split=0.1, verbose=1,\n",
    "                 callbacks=[PlotLossesKeras()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate by trying out some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(news_siamese.predict([fX[:10], sX[:10]]), 1), y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good job!\n",
    "\n",
    "In the next chapter we'll examine how can we deal with data where the sequence of the data also carries important information by using recurrent neural networks in [DL 08 Recurrent Neural Networks](./DL_08_Recurrent_Networks.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
