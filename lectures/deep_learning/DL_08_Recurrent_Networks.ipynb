{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pics/DL.png\" width=110 align=\"left\" style=\"margin-right: 10px\">\n",
    "\n",
    "# Introduction to Deep Learning\n",
    "\n",
    "## 08. Recurrent Networks\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Recurrent Neural Networks (RNN)](https://keras.io/api/layers/recurrent_layers/)\n",
    "\n",
    "<img src=\"./pics/external/lstm/recurrent_neural_network_unfold.svg\" alt=\"Recurrent neural network unfold.svg\" height=\"213\" width=\"640\"><br>By <a href=\"//commons.wikimedia.org/wiki/User:Ixnay\" title=\"User:Ixnay\">François Deloche</a> - <span class=\"int-own-work\" lang=\"en\">Own work</span>, <a href=\"https://creativecommons.org/licenses/by-sa/4.0\" title=\"Creative Commons Attribution-Share Alike 4.0\">CC BY-SA 4.0</a>, <a href=\"https://commons.wikimedia.org/w/index.php?curid=60109157\">Link</a></p>\n",
    "\n",
    "### Purpose\n",
    "\n",
    "The networks we talked about so far had quite simple architectures, we could express them as a series of matrix multiplications. Lots of the cases they are capable of perfectly express wide range of problems. However some of the real life problems requires information from the previous events - solving a problem requires some sort of \"memory\". Recurrent Neural Networks solve this problem by storing the previous inputs in their inter state. This memory enables to recognize text, speech, handwriting, etc.\n",
    "\n",
    "So basically the neuron has memory and remembers it's previous informations by using the results of the previous inputs. This behaviour can be interpreted as the neuron with one extra input: it's previous output. For easier implementation (both matematical, bot programmatical viewpoint) we unfold this neuron into a chain of multiple neurons as you can see on the image above.\n",
    "\n",
    "#### Use cases\n",
    "\n",
    "- Sequence prediction: $[1, 2, 3, 4, 5] \\rightarrow \\fbox{RNN} \\rightarrow [6]$\n",
    "    - Weather Forecasting\n",
    "    - Stock Market Prediction\n",
    "- Sequence classification: $[1, 2, 3, 4, 5] \\rightarrow \\fbox{RNN} \\rightarrow [\\text{\"good\"}]$\n",
    "    - Anomaly Detection\n",
    "    - Sentiment Analysis\n",
    "- Sequence to sequence prediction: $[1, 2, 3, 4, 5] \\rightarrow \\fbox{RNN} \\rightarrow [6, 7, 8, 9, 10]$\n",
    "    - Language Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do they work?\n",
    "\n",
    "Unrolling (or unfolding) means that we spread a neuron into a network of neurons for a complete sequence. Sequence length is one important parameter for the model. Each step in a sequence has its own dedicated neuron.\n",
    "\n",
    "The formal description of the prediction is as follows:\n",
    "\n",
    "$t$ is the step we are observing at a time, $x_t$ is the input at a given step $t$. $x_t$ can be the opening price of a stock at a given date. $h_t$ is the hidden state, or the memory of the neuron. It is computed using the previous memory value and the actual input using the $h_t=f(Ux_t + Vh_{t-1})$ equation, where $f$ is the activation function, and $o_t = f(Wh_t)$. The initial memory (denoted as $h_{-1}$) is usually set to zero. There are 3 set of weights in each neuron: $U$ is the input weight, $V$ is the weight for the hidden state, and finally $W$ is the output weight.\n",
    "\n",
    "$h_t$ acts as a memory storing the information from the previous time steps, and the output $o_t$ relies only on the actual memory value. $U$, $V$ and $W$ is shared among the unrolled neurons, reducing the amount of parameters required for training. The output of a recurrent neuron can be either a single value, or the whole sequence as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "A modified version of the Backpropagation algorithm called **B**ack**p**ropagation **T**rough **T**ime (BPTT) is used to calculate the update rules for the weights. Do not let the fancy name confuse you, it's the same algorithm we used, the trick is that we have to backpropagate $t$ steps in the network. Luckily the chain rule is applicable to compute the gradients and the update values are computable recursivably.\n",
    "Formally:\n",
    "\n",
    "Using the forward equations:\n",
    "$$\\begin{align}\n",
    "    a_t & = Ux_t + Vh_{t-1} \\\\\n",
    "    h_t & = f(a_t) \\\\\n",
    "    o_t & = f(Wh_t)\n",
    "\\end{align}$$\n",
    "\n",
    "The delta at the \"output layer\" of the unrolled neuron:\n",
    "$$\\begin{align}\n",
    "    \\delta^t_o = (\\hat{o_t} - o_t) \\otimes h_t \n",
    "\\end{align}$$\n",
    "\n",
    "\n",
    "The delta at the \"hidden layer\" of the unrolled neuron:\n",
    "$$\\begin{align}\n",
    "    \\delta^t_h = f'(a_t)(U\\delta^t_o + W\\delta^{t+1}_h)\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of RNNs\n",
    "- One to one: Vanilla recurrent neural network, one input value, one output value. \n",
    "- One to many: one input with multiple outputs (a sequence of outputs)\n",
    "- Many to one: multiple input (a sequence) and one output value\n",
    "- Many to many: multiple inputs and multiple outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Practice\n",
    "\n",
    "#### Many to One predictor: Build a simple decay predictor\n",
    "\n",
    "The contents of the computer RAM have to be refreshed constantly to keep its stored values. The current in the memory curcuits decays constantly. We have a series of observations about the refreshes and an observed voltage at the end of the observation window. Our goal is to predict the current at the end of our observation. \n",
    "\n",
    "The length of our observation is 10 μs and its resolution is 1 μs. Each refreshment set the voltage to 1.5V and its value is decreasing by 0.2V each μs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import SimpleRNN, LSTM\n",
    "from keras.layers import Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.metrics import RootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data generation\n",
    "\n",
    "Let's generate 256 samples by randomly selecting the refresh signals timestep. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_size = 256\n",
    "length = 10\n",
    "features = 1\n",
    "\n",
    "row_index = np.arange(sample_size)\n",
    "col_index = np.random.randint(length, size=sample_size)\n",
    "\n",
    "X = np.zeros((sample_size, length, features))\n",
    "X[(row_index, col_index)] = 1\n",
    "\n",
    "y = np.maximum(1.5 - (length - col_index - 1) * 0.2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 256 sample, 10 time steps, and 1 feature\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X[0, :, 0], y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build the model:\n",
    "\n",
    "Let's have a really simple network with:\n",
    "- 25 RNN unit with linear activation\n",
    "- 1 fully connected output layer with ReLU activation\n",
    "- adam optimizer with default parameters\n",
    "- since we want to predict a continous value, we use mse loss\n",
    "- RMSE evaluation metric\n",
    "\n",
    "\n",
    "The hardest part is the setup of the RNN layer:\n",
    "- We have to set the correct input shape: \n",
    "    - we have 256 samples\n",
    "    - the sequences are 10 μs long\n",
    "    - have one feature\n",
    "- We ignore the sample size, and use the last two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(SimpleRNN(25, input_shape=(length, features)))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=[RootMeanSquaredError()])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit and test our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(X, y, epochs=50, batch_size=64, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test = np.identity(length).reshape((length, length, 1))\n",
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Many to Many: Build a simple decay predictor v2\n",
    "\n",
    "Instead of predicting the voltage at the end of the observation window, let's predict the voltage every μs.\n",
    "\n",
    "##### Data generation\n",
    "\n",
    "We'll reuse the same input from the previous example and only change the expected output. Instead of generating one expected output value, we generate a sequence of volts. For simplicity we use a 10 μs output window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prototype = np.array([1.5, 1.3, 1.1, 0.9, 0.7, 0.5, 0.3, 0.1, 0.0, 0.0])\n",
    "y_series = np.vstack([\n",
    "    np.concatenate((np.zeros(col_index[i]), \n",
    "                    prototype[:(length - col_index[i])]))\n",
    "    for i in range(sample_size)\n",
    "]).reshape(sample_size, length, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same network layout, however we have to modify the RNN layer by setting the `return_sequences` parameter to `True` which will make the neurons to output the values at each step instead of outputting only the final value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(SimpleRNN(25, \n",
    "                    input_shape=(length, features), \n",
    "                    return_sequences=True))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=[RootMeanSquaredError()])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already dealt with the output shape during the data generation by transforming the vectors into 3d tensors with `sample size` x `sequence length` x `features` dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(X, y_series, epochs=100, batch_size=64, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Forecast monthly closing stock values.\n",
    "We generate a timeseries consisting one stock closing price over 220 months, starting from 2001.  \n",
    "Our task is to create a model which is able to predict the closing value of the next month. The input is 20 months of stock prices and we want our model to predict a series of 20 prices starting from the second month instead of predicting just one.  \n",
    "\n",
    "Eg.:\n",
    "\n",
    "input months:  `[2001.01, 2001.02, ..., 2002.07 , 2002.08]`  \n",
    "output months: `[2001.02, 2001.03, ..., 2002.08 , 2002.09]`  \n",
    "\n",
    "So we basically ask our model to repeat the values seen in the beginning of the input, and guess one value after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_stock_prices(start='2001', length=201, freq='M'):\n",
    "    index = pd.date_range(start=start, periods=length, freq=freq)\n",
    "    values = np.random.uniform(-18, 18, size=length).cumsum()\n",
    "    values += np.abs(values.min()) + 1e-1 # stock prices never go below 0\n",
    "    return pd.Series(values, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ts = generate_stock_prices(start='2001', length=222)\n",
    "stock_prices = ts.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\n",
    "\n",
    "ax1.plot(ts.index, ts)\n",
    "ax1.plot(ts.index[90:100], ts[90:100], \"b-\", linewidth=3, label=\"A training instance\")\n",
    "ax1.legend(loc=\"upper left\")\n",
    "ax1.set_xlabel(\"Time\")\n",
    "ax1.set_title(\"A time series (generated)\", fontsize=14)\n",
    "\n",
    "ax2.plot(ts.index[90:100], ts[90:100], \"b-\", markersize=8, label=\"instance\")\n",
    "ax2.plot(ts.index[91:101], ts[91:101], \"bo\", markersize=10, label=\"target\", markerfacecolor='red')\n",
    "ax2.set_title(\"A training instance\", fontsize=14)\n",
    "ax2.set_xlabel(\"Time\")\n",
    "ax2.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate samples\n",
    "\n",
    "20 month long samples, the expected output is shifted by one month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_stock_samples(series, length, input_features, output_features):\n",
    "    # useful trick: reshape is able to deduct the last shape if it is set to -1\n",
    "    X = series[:-output_size].reshape(-1, length, input_features)\n",
    "    y = series[output_size:].reshape(-1, length, output_features)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "length = 20\n",
    "input_features =  1\n",
    "output_features = 1\n",
    "train_size = 201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stock_train = stock_prices[:train_size]\n",
    "stock_test = stock_prices[train_size:]\n",
    "\n",
    "stock_train.shape, stock_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train = create_stock_samples(stock_train, length, input_features, output_features)\n",
    "X_test, y_test = create_stock_samples(stock_test, length, input_features, output_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the model\n",
    "\n",
    "Expected architecture:\n",
    "- RNN layer consisting 120 neurons with ReLU activation\n",
    "- [optional]: Dense layer with 60 neurons and ReLU activation\n",
    "- Dense output with linear activation\n",
    "\n",
    "Use `Adam` optimzer.\n",
    "\n",
    "Questions to answer:\n",
    "- What is the input shape?\n",
    "- What is the output shape?\n",
    "- How should you setup your recurrent layer for this problem?\n",
    "- What is the ML task?\n",
    "- Which cost function is appropriate here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. Build the model\n",
    "stock_model = Sequential()\n",
    "\n",
    "# FILL IN\n",
    "\n",
    "# 2. Compile the model\n",
    "stock_model.compile()  # FILL IN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the model\n",
    "\n",
    "It is a pretty complex problem so use 5000 epochs, but use early stopping with 25 patience value. Use the test data as validation.\n",
    "\n",
    "Extra: \n",
    "tqdm has a nice progress bar you can use:\n",
    "- import `TqdmCallback` from `tqdm.keras` and add it to the callbacks\n",
    "- set verbosity to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3. Fit the model\n",
    "\n",
    "stock_model.fit(X_train, y_train, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation\n",
    "\n",
    "Let's observe how our model perform on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = stock_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.title(\"Forecast vs Actual\", fontsize=14)\n",
    "plt.plot(pd.Series(np.ravel(y_test)), \"bo\", markersize=8, label=\"Actual\", color='green')\n",
    "plt.plot(pd.Series(np.ravel(y_pred)), \"r.\", markersize=8, label=\"Forecast\", color='red')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.xlabel(\"Time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## [Long-Short Term Memory (LSTM) Networks](https://keras.io/api/layers/recurrent_layers/lstm/)\n",
    "\n",
    "### Purpose\n",
    "\n",
    "The biggest problem with the simple RNN setup is easily understood, if we consider a model which has to deal with the following sentence: \"I grew up in _France_... that's why I speak fluent _French_.\" The related information are far away from each other and it's easy for the network to miss these nuances and however in theory they can learn them, but in practice they often unable to do so.  \n",
    "But a more complex version of them is able to overcome this difficulty, they're called LSTMs.\n",
    "\n",
    "They are build to have long term memory, and have the same kind of chained structure, but the modules themselves are different. What they are excel at is learning complex series with the ability to find sequence boundaries.\n",
    "\n",
    "\n",
    "\n",
    "### How do they work?\n",
    "\n",
    "<img src=\"./pics/external/lstm/LSTM3-chain.png\" width=\"600\"><br>By <a href=\"https://colah.github.io/about.html\">Christopher Olah</a>, <a href=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/\">Link</a>\n",
    "\n",
    "LSTMs also have the same chain like structure as the Vanilla RNNS, but their hidden state is a complex one. Instead of having a simple hidden state, it contains multiple gates to control the remembering and forgetting process.\n",
    "\n",
    "<img src=\"./pics/external/lstm/LSTM3-C-line.png\" width=\"600\"><br>By <a href=\"https://colah.github.io/about.html\">Christopher Olah</a>, <a href=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/\">Link</a>\n",
    "\n",
    "The hidden state (also referred as cell state) of the LSTM neuron is the pipe on the top which is controlled by the different gates inside the neuron. These gates uses sigmoid functions to control the amount of signal going through the cell. If you recall that sigmoid function outputs values between 0 and 1, you can see that the gates are working as taps, controlling the amount of signal going through the pipe. There are 3 gates in an LSTM neuron.\n",
    "\n",
    "##### Forget gate\n",
    "\n",
    "<img src=\"./pics/external/lstm/LSTM3-focus-f.png\" width=\"600\"><br>By <a href=\"https://colah.github.io/about.html\">Christopher Olah</a>, <a href=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/\">Link</a>\n",
    "\n",
    "The first interaction with the cell state \"pipe\" is to determine how much of the previous signal we want to filter out, or in other words: forget, hence the name *forget gate*. This gate uses a sigmoid activation function to control the amount of signal to keep. Using the previous output $h_{t-1}$ and the actual input $x_t$ to determine the $f_t$ multiplier for the previous state $C_{t-1}$.  \n",
    "\n",
    "##### Input gate\n",
    "\n",
    "<img src=\"./pics/external/lstm/LSTM3-focus-i.png\" width=\"600\"><br>By <a href=\"https://colah.github.io/about.html\">Christopher Olah</a>, <a href=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/\">Link</a>\n",
    "\n",
    "After forgetting the unwanted values the cell will determine what it wants to store based on the actual input and the previous output. That is the job of the *input gate*, consisting two parts: a tanh layer to generate $\\tilde{C}_t$, the values to store  and a sigmoid layer which determines $i_t$ the amount of the information the cell should store. \n",
    "\n",
    "<img src=\"./pics/external/lstm/LSTM3-focus-C.png\" width=\"600\"><br>By <a href=\"https://colah.github.io/about.html\">Christopher Olah</a>, <a href=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/\">Link</a>\n",
    "\n",
    "Using the forget and input gates we modify the $C_{t-1}$ previous state by first multiplying by $f_t$ and then add the new information $i_t * \\tilde{C}_t$.\n",
    "\n",
    "##### Output gate\n",
    "\n",
    "<img src=\"./pics/external/lstm/LSTM3-focus-o.png\" width=\"600\"><br>By <a href=\"https://colah.github.io/about.html\">Christopher Olah</a>, <a href=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/\">Link</a>\n",
    "\n",
    "The final step is to merge the updated cell state $C_t$ with the actual and previous inputs $h_{t-1}$ and $x_t$. We handle the two input sources (state, and input) separately: \n",
    "1. the *inputs* are multiplied and used as inputs in an ordinary neuron with sigmoind activation function, and \n",
    "2. the *cell state* is going through a tanh activation gate,\n",
    "3. finally the two sources are multiplied to generate $t_t$, the *output* of the neuron.\n",
    "This output is used as the output of the neuron, but also passed forward to the neuron in the next timestep.\n",
    "\n",
    "##### Example\n",
    "\n",
    "Using the NLP example from before: The LSTM starts reading the input sentence and using the input gate, store the subject of the sentence, but once it reaches the end of a sentence by reading a `\".\"` character, the LSTM wants to forget about the subject using the *forget gate*, and select the next one using the *input gate*, again. In this particular case, the neuron wants to to output information about the subject, specifically that the speaker is French, so it's *output gate* will both passes the information forward to later learn about the language skills of said subject and also outputs the nationality of the speaker.\n",
    "\n",
    "#### Modification: GRU\n",
    "\n",
    "<img src=\"./pics/external/lstm/LSTM3-var-GRU.png\" width=\"600\"><br>By <a href=\"https://colah.github.io/about.html\">Christopher Olah</a>, <a href=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/\">Link</a>\n",
    "\n",
    "There are many different version of the described model, but one of the most popular modification is the **G**ated **R**ecurrent **U**nit or *GRU* ([paper](http://arxiv.org/pdf/1406.1078v3.pdf) by Cho, et al., 2014) which merges the cell state with the hidden state and the input gate with the forget gate creating a single *update gate*. Since it encorporates fewer gates it uses less parameters so it is becoming simpler.\n",
    "\n",
    "#### Training\n",
    "\n",
    "Training an LSTM is really similar training the Vanilla RNNs using the same unfolding view of the neuron but LSTMs have much more parameters to set in each iteration, so it is more affected by the vanishing and exploding gradient problem. We can use the chain rules and the recursive calculation of the deltas as well, however we omit the actual equations this time. The curious reader can find the calculation details in the original [paper](http://www.bioinf.jku.at/publications/older/2604.pdf) by Hochreiter & Schmidhuber (1997)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Practice\n",
    "\n",
    "#### 1. Learning the alphabet\n",
    "\n",
    "Let's build a network to learn to continue the alphabet, but the length of the given sequence is variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from livelossplot import PlotLossesKeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "# create mapping of characters to indexes\n",
    "char_to_int = {c: i for i, c in enumerate(alphabet)}\n",
    "int_to_char = {i: c for i, c in enumerate(alphabet)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_size = 1000\n",
    "length = 5\n",
    "features = 1\n",
    "output_size = len(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_alphabet_samples(max_length, output):\n",
    "    length = np.random.randint(2, max_length + 1)\n",
    "    start = np.random.randint(0, len(alphabet) - length - 1)\n",
    "    sequence = list(range(start, start + length + 1))\n",
    "    return sequence[:-1], sequence[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_raw, y_raw = zip(*[generate_alphabet_samples(length, output) \n",
    "                     for _ in range(sample_size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = pad_sequences(X_raw, maxlen=length, dtype='float32')\n",
    "# reshape X to [samples x time steps x features] shape\n",
    "X = np.reshape(X, (-1, length, features))\n",
    "# normalize\n",
    "X = X / output_size\n",
    "\n",
    "# convert y to array\n",
    "y = np.array(y_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(length, features)))\n",
    "model.add(Dense(output_size))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "model.fit(X, y, \n",
    "          epochs=25, \n",
    "          batch_size=batch_size, \n",
    "          verbose=0, \n",
    "          callbacks=[PlotLossesKeras()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores = model.evaluate(X, y, verbose=0)\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    pattern_index = np.random.randint(sample_size)\n",
    "    x = X[pattern_index].reshape(-1, length, features)\n",
    "    prediction = np.argmax(model.predict(x, verbose=0))\n",
    "    \n",
    "    pattern = [int_to_char[value] for value in X_raw[pattern_index]]\n",
    "    result = int_to_char[prediction]\n",
    "    \n",
    "    print(pattern, \"->\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Build a sentiment predictor on movie reviews\n",
    "\n",
    "Build a sentiment prediction on the built-in movie reviews dataset. Sentiment value is a numerical representation of a written text. 5 means positive, 3 is neutral and 1 is negative opinion. In this particular problem these values are transformed into a binary problem: 1 means positive review, and 0 means negative.   \n",
    "Let's make use of an embedding layer to learn a textual representation of the reviews, and use them as a sequence of inputs. Our target value is the sentiment value for any given document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_review_length = 500\n",
    "# pad sequences will fill every doc in the corpus to a given length\n",
    "X_train = pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = pad_sequences(X_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding_vector_length = 32\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=top_words,                 # number of words in the vocab\n",
    "              output_dim=embedding_vector_length,  # size of the embedding vector\n",
    "              input_length=max_review_length),     # size of the documents\n",
    "    LSTM(units=100),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, \n",
    "          validation_split=0.1, \n",
    "          epochs=3, \n",
    "          batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test, batch_size=16)\n",
    "print('test loss: {:.2%}, test accuracy: {.2%}'.format(*score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Build a calculator\n",
    "\n",
    "Let's build a model which is able to calculate the sum of two numbers.  \n",
    "This is actually a more interesting problem than it seems for the sight: The operation is given as text, just as the expected output. To solve this problem we are going to introduce an LSTM architecture that is usually used in machine translation, **encoder - decoder** pattern.\n",
    "\n",
    "The encoder-decoder pattern is used to solve sequence to sequence (seq2seq) problems and consists of two LSTM layers, one for encoding the input sequence, and one for to decode the information coming from the encoder into the output. The encoder layer will be responsible to create an inner model about its input while the output interprets this representation and generates its translation.\n",
    "\n",
    "Let's see it in action, but first we have to generate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chars = \"0123456789+ \"\n",
    "\n",
    "def encode(expression, width, mapping):\n",
    "    string = '+'.join([str(exp) for exp in expression])\n",
    "    padding = ' ' * (width - len(string))\n",
    "    string_expression = string + padding\n",
    "    encoded = np.array([mapping.index(char) for char in string_expression])\n",
    "    one_hot = np.zeros((width, len(mapping)))\n",
    "    one_hot[(np.arange(width), encoded)] = 1\n",
    "    return one_hot\n",
    "\n",
    "def decode(encoded, mapping):\n",
    "    _, indices = np.nonzero(encoded)\n",
    "    string = ''.join(mapping[index] for index in indices)\n",
    "    numbers = string.strip().split('+')\n",
    "    decoded = np.array([int(number) for number in numbers])\n",
    "    return decoded\n",
    "\n",
    "def generate_calculation_sample(max_value, n_numbers, mapping):\n",
    "    x_max_width = len(str(max_value)) * n_numbers + n_numbers - 1\n",
    "    y_max_width = len(str(max_value * n_numbers))\n",
    "    \n",
    "    X = np.random.randint(1, max_value + 1, size=n_numbers)\n",
    "    y = np.sum(X, keepdims=True)\n",
    "    \n",
    "    X_encoded = encode(X, width=x_max_width, mapping=mapping)\n",
    "    y_encoded = encode(y, width=y_max_width, mapping=mapping)\n",
    "    \n",
    "    return X_encoded, y_encoded\n",
    "    \n",
    "def generate_calculation_samples(samples, max_value, n_numbers, mapping):\n",
    "    x_max_width = len(str(max_value)) * n_numbers + n_numbers - 1\n",
    "    y_max_width = len(str(max_value * n_numbers))\n",
    "    \n",
    "    X, y = zip(*[generate_calculation_sample(max_value, n_numbers, mapping)\n",
    "                 for _ in range(samples)])\n",
    "    \n",
    "    X = np.array(X).reshape(samples, x_max_width, len(mapping))\n",
    "    y = np.array(y).reshape(samples, y_max_width, len(mapping))\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_value = 99\n",
    "n_numbers = 2\n",
    "input_length = len(str(max_value)) * n_numbers + n_numbers - 1\n",
    "output_length = len(str(max_value * n_numbers))\n",
    "samples = 5000\n",
    "features = len(chars)\n",
    "\n",
    "X, y = generate_calculation_samples(samples, max_value, n_numbers, chars)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the model!\n",
    "\n",
    "Our network will have:\n",
    "\n",
    "**encoder layer**:\n",
    "\n",
    "LSTM\n",
    "- 128 neurons\n",
    "    - input shape: (`input string length` x `character mapping size`)\n",
    "    - without return_sequence\n",
    "\n",
    "**decoder layer**:\n",
    "\n",
    "Repeater\n",
    "- 1 special repeat layer which repeats the same vector n times\n",
    "    - n: `output string length`\n",
    "\n",
    "LSTM\n",
    "- 128 neurons\n",
    "    - input shape is automatically determined\n",
    "    - with return_sequence\n",
    "    \n",
    "**output layer**:\n",
    "\n",
    "Dense network\n",
    "- `character mapping size` neurons\n",
    "- `softmax` activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import RepeatVector\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "calculator = Sequential()\n",
    "\n",
    "# Encoder\n",
    "calculator.add(LSTM(units=128, input_shape=(input_length, features)))\n",
    "\n",
    "# Decoder\n",
    "calculator.add(RepeatVector(n=output_length))\n",
    "calculator.add(LSTM(units=128, return_sequences=True))\n",
    "\n",
    "# Output\n",
    "calculator.add(Dense(features))\n",
    "calculator.add(Activation(activation='softmax'))\n",
    "\n",
    "# compile\n",
    "calculator.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_model(calculator, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs_num = 50\n",
    "batch_size = 128\n",
    "\n",
    "calculator.fit(X, y, \n",
    "          epochs=epochs_num, \n",
    "          batch_size=batch_size, \n",
    "          verbose=0, \n",
    "          callbacks=[PlotLossesKeras()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_samples = 10\n",
    "\n",
    "examples, expected = generate_calculation_samples(test_samples, max_value, n_numbers, chars)\n",
    "prediction = calculator.predict(examples.reshape(-1, input_length, features))\n",
    "\n",
    "for i in range(test_samples):\n",
    "    input_numbers = decode(examples[i], chars)\n",
    "    expected_value = decode(expected[i], chars)[0]\n",
    "    predicted = int(''.join([chars[pred] for pred in np.argmax(prediction[i], axis=1)]))\n",
    "    print(f\"[{'x' * (expected_value == predicted):1}] \"\n",
    "          f\"{' + '.join(str(n) for n in input_numbers): <7} \"\n",
    "          f\"= {predicted: >3} \"\n",
    "          f\"[{expected_value: >3}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "How would you modify the MNIST example to work with 2 low resolution mnist picture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good job!\n",
    "\n",
    "In the next chapter we'll discover how can we deal with situations where we don't have prior knowledge about the problem and we have to explore and then exploit the environment using (deep) reinforcement learning in [DL 09 Reinforcement Learning](./DL_09_Reinforcement_Learning.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
