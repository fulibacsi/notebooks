{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pics/DL.png\" width=110 align=\"left\" style=\"margin-right: 10px\">\n",
    "\n",
    "# Introduction to Deep Learning\n",
    "\n",
    "## 03. Neural Networks\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a neural network?\n",
    "\n",
    "<img src=\"./pics/ann/mlp.png\" width=300 align=\"left\"/>\n",
    "\n",
    "##### Nomenclature\n",
    "\n",
    "As we saw in the previous notebook, a single neuron is not able to solve non-linear problems. But they are not called **networks** for nothing! The power of artificial neural networks lies in their topology.  \n",
    "If we connect more of the **neurons** we get a (real) neural network (*ANN* or *NN*). The neurons are ordered into **layer**s.  \n",
    "The first layer is the **input layer** then there are zero, or more **hidden layer**(s), finally there is the **output layer**. Each layer can cosist any number of neurons. Based on the different topologies there are different ANN subtypes.  \n",
    "The most simple version of ANNs is the multi layer feed forward perceptron network (MLP). \n",
    "\n",
    "##### Prediction\n",
    "\n",
    "The prediction is called a forward propagation as the information travels  through the network from the input layer to the output. Each neuron in the network processes its inputs individually by summing the weighted inputs and transforming it by an activation function. The outputs of neurons in a layer will be the inputs of the neurons on the next layer.\n",
    "In practice the bias term is included in the weights and a constant input is intorduced each layer to support this inclusion.\n",
    "\n",
    "##### Training\n",
    "\n",
    "<img src=\"./pics/ann/mlp_backprop.jpg\" width=300 align=\"right\"/>\n",
    "\n",
    "The weight updating algorithm is called [**Backpropagation**](https://en.wikipedia.org/wiki/Backpropagation). It basically propagates the errors back to their \"root\" neuron. So every neuron which was (even slightly) responsible for an error will get their weights updated accordingly. The weight updating equations can be find by calculating the gradient similarly to the Perceptron rules. Backpropagation is similar to [gradient descent rules](https://en.wikipedia.org/wiki/Backpropagation#Derivation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does backpropagation work?\n",
    "\n",
    "<img src=\"./pics/ann/mlp_notation.png\">\n",
    "\n",
    "##### Notation\n",
    "\n",
    "- dataset: $X$\n",
    "- training sample: $\\mathbf{x}^n \\in X$, where $\\mathbf{x}^n = (X^n_0, \\ldots, X^n_N)$\n",
    "- inputs: $\\{x_{0}, \\ldots, x_{N}\\}$\n",
    "- weights: $\\{w_{i0}, \\ldots, w_{iN}\\}$\n",
    "- bias: $b_i$\n",
    "- activity: $a^n_j = \\sum_i w_{ji} X^n_i$\n",
    "- activation function: $f$\n",
    "- output: $y^n_j = x^n_{kj} = f(a^n_j) = f \\left( \\sum_i w_{ji} X^n_i \\right)$\n",
    "- target: $t^n_j$\n",
    "- error: $e^n_j = t^n_j - y^n_j$\n",
    "- cost function: $C^n = \\frac{1}{2} \\sum_j {\\left(e^n_j\\right)}^2$\n",
    "- output layer index: $k$\n",
    "\n",
    "##### Weight update rule\n",
    "\n",
    "There are two types of neuron weights in the network: the neurons in the output layer, and the neurons on the hidden layers. We will search for an expression which tells us how \"guilty\" is a weight for an output, we'll call this expression *sensitivity*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Output layer\n",
    "\n",
    "As a first step, let's find the update rule for the output layer. It is straightforward, since the weights of the output neuron are directly responsible for the error. The error itself is a great measure for sensitivity, let's use it to compute the gradient for the weight changes - we consider the quadratic sum  of errors on the output layer (let's use the index $k$ since we are talking about the output layer) this will be our **cost function**:\n",
    "\n",
    "$$\n",
    "    C^n = \\frac{1}{2}{\\left(\\sum_k e^n_k\\right)}^2\n",
    "$$\n",
    "\n",
    "To find the gradient, we'll differentiate the cost function with respect to $w_{ji}$. We're going to use the chain rule to find it:\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial C^n}{\\partial w_{kj}} \n",
    "    = \\frac{\\partial C^n}{\\partial e^n_k} \n",
    "      \\frac{\\partial e^n_k}{\\partial y^n_k} \n",
    "      \\frac{\\partial y^n_k}{\\partial a^n_k} \n",
    "      \\frac{\\partial a^n_k}{\\partial w_{kj}}\n",
    "$$\n",
    "\n",
    "Let's go through the parts one by one:\n",
    "\n",
    "$$\\begin{align}\n",
    "    \\frac{\\partial C^n}{\\partial e^n_k}\n",
    "    & = \\frac{\\partial \\frac{1}{2}{\\left(\\sum_k  e^n_k \\right)}^2}{\\partial e^n_k}\n",
    "    = e^n_k\n",
    "\\\\\n",
    "    \\frac{\\partial e^n_k}{\\partial y^n_k}\n",
    "    & = \\frac{\\partial t^n_k - y^n_k}{\\partial y^n_k}\n",
    "    = -1\n",
    "\\\\\n",
    "    \\frac{\\partial y^n_k}{\\partial a^n_k}\n",
    "    & = \\frac{\\partial f \\left( a^n_k \\right)}{\\partial a^n_k}\n",
    "    = f'\\left( a^n_k \\right)\n",
    "\\\\\n",
    "    \\frac{\\partial a^n_k}{\\partial w_{kj}} \n",
    "    & = \\frac{\\partial}{\\partial w_{kj}} \\left( \\sum_i w_{kj} X_{j} \\right)\n",
    "    = X_j\n",
    "\\end{align}$$\n",
    "\n",
    "Combining the results will yield:\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial C^n}{\\partial w_{kj}}\n",
    "    = -e^n_k f'\\left( a^n_k \\right) x^n_{kj}\n",
    "$$\n",
    "\n",
    "The error gradient is also referred as *delta* and can be expressed as:\n",
    "\n",
    "$$\\begin{align}\n",
    "    \\delta^n_k \n",
    "    & = \\frac{\\partial C^n}{\\partial a^n_k} \\\\\n",
    "    & = \\frac{\\partial C^n}{\\partial e^n_k}\n",
    "        \\frac{\\partial e^n_k}{\\partial y^n_k}\n",
    "        \\frac{\\partial y^n_k}{\\partial a^n_k} \\\\\n",
    "    & = -e^n_k  f'\\left( a^n_k \\right)\n",
    "\\end{align}$$\n",
    "\n",
    "Using it we can rewrite our results:\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial C^n}{\\partial w_{kj}} = \\delta^n_k x^n_{kj}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hidden layers\n",
    "\n",
    "We'll use the results above to compute the hidden layer's deltas. The errors inside the hidden layers are indirectly computed by back propagating the errors. Consider the previous equation, since: ${\\partial C^n}\\big/{\\partial w_{kj}} = \\delta^n_k y_{kj}$. The inputs ($x_{kj}$) of the output layer is the output ($y_j$) of the last hidden layer, so we can say that ${\\partial C^n}\\big/{\\partial w_{kj}} = \\delta^n_k y_j$.  \n",
    "Based on this notation, let's rewrite the error equation as a reminder:\n",
    "\n",
    "$$\\begin{align}\n",
    "    C^n\n",
    "    & = \\frac{1}{2} \\sum_k {\\left(e^n_k\\right)}^2  \\\\\n",
    "    & = \\frac{1}{2} \\sum_k {\\left(t^n_k - y^n_k\\right)}^2  \\\\\n",
    "    & = \\frac{1}{2} \\sum_k {\\left(t^n_k - f(a^n_k)\\right)}^2  \\\\\n",
    "    & = \\frac{1}{2} \\sum_k {\\left(t^n_k - f\\left(\\sum_i w_{kj} x^n_{kj}\\right)\\right)}^2 \\\\\n",
    "    & = \\frac{1}{2} \\sum_k {\\left(t^n_k - f\\left(\\sum_i w_{kj} y^n_j\\right)\\right)}^2 \\\\\n",
    "    & = \\frac{1}{2} \\sum_k {\\left(t^n_k - f\\left(\\sum_i w_{kj} f\\left(a^n_j\\right)\\right)\\right)}^2 \\\\\n",
    "    & = \\frac{1}{2} \\sum_k {\\left(t^n_k - f\\left(\\sum_i w_{kj} f\\left(\\sum_i w_{ji} x^n_i\\right)\\right)\\right)}^2 \\\\\n",
    "\\end{align}$$\n",
    " \n",
    "Using this error, we can calculate the hidden layer's derivate using the chain rule again:\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial C^n}{\\partial w_{ji}}\n",
    "    = \\underbrace{\n",
    "        \\left( \\sum_k \\frac{\\partial C^n}{\\partial e^n_{k}} \n",
    "                      \\frac{\\partial e^n_{k}}{\\partial y^n_{k}} \n",
    "                      \\frac{\\partial y^n_{k}}{\\partial a^n_k} \n",
    "                      \\frac{\\partial a^n_k}{\\partial y^n_{j}}\n",
    "        \\right)\n",
    "      }_\\text{output neurons} \n",
    "      \\underbrace{\n",
    "          \\frac{\\partial y^n_{j}}{\\partial a^n_{j}} \n",
    "          \\frac{\\partial a^n_{j}}{\\partial w_{ji}}\n",
    "       }_\\text{hidden neuron}\n",
    "$$\n",
    "\n",
    "We have the results from the output layer, let's substitute it:\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial C^n}{\\partial w_{ji}} \n",
    "    = \\left( \\sum_k \\delta^n_k \\frac{\\partial a^n_k}{\\partial y^n_{j}} \\right) \n",
    "      \\frac{\\partial y^n_{j}}{\\partial a^n_{j}} \n",
    "      \\frac{\\partial a^n_{j}}{\\partial w_{ji}}\n",
    "$$\n",
    "\n",
    "Let's deal with the activation value:\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial a^n_k}{\\partial y^n_{j}} \n",
    "    = \\frac{\\partial}{\\partial y^n_{j}} \\left(\\sum_j w_{kj} y^n_{j} \\right) \n",
    "    = w_{kj}\n",
    "$$\n",
    "\n",
    "and with the output of the hidden layer:\n",
    "\n",
    "$$\\begin{align}\n",
    "    \\frac{\\partial y^n_{j}}{\\partial a^n_{j}}\n",
    "    & = \\frac{\\partial f\\left(a^n_j\\right)}{\\partial a^n_{j}}\n",
    "    = f'\\left( a^n_j \\right)\n",
    "\\\\\n",
    "    \\frac{\\partial a^n_{j}}{\\partial w_{ji}}\n",
    "    & = \\frac{\\partial \\sum_i w_{ji} x^n_i}{\\partial w_{ji}}\n",
    "    = x_i\n",
    "\\end{align}$$\n",
    "\n",
    "Putting the results together we get:\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial C^n}{\\partial w_{ji}}\n",
    "    = \\left( \\sum_k \\delta^n_k w_{kj}\\right) f'\\left( a^n_j \\right) x_i\n",
    "$$\n",
    "\n",
    "It is really similar to the results in the output layer, let's use the same tehcnique: move the differentiation of the activity to a separate definition called *delta*:\n",
    "\n",
    "$$\\begin{align}\n",
    "    \\delta^n_j \n",
    "    & = \\frac{\\partial C^n}{\\partial a^n_k} \\\\\n",
    "    & = \\left( \\sum_k \\frac{\\partial E^n}{\\partial e^n_{k}} \n",
    "                      \\frac{\\partial e^n_{k}}{\\partial y^n_{k}} \\right) \n",
    "        \\frac{\\partial y^n_{k}}{\\partial a^n_k} \\\\\n",
    "    & = \\left(\\sum_k \\delta^n_k w_{kj} \\right) f'\\left( a^n_j \\right)\n",
    "\\end{align}$$\n",
    "\n",
    "So hidden layer's expression changes to:\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial C^n}{\\partial w_{ji}} = \\delta^n_j x_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generalization\n",
    "\n",
    "The differentation above was about a two layer network, so we have to extend and generalize these results. Luckily, observing the final equations, one can notice the similarities, so we create the following rule for computing the *delta*, and therefore the gradient:\n",
    "\n",
    "$$\n",
    "    \\delta^n_i = \\begin{cases}\n",
    "        e^n_j f'\\left( a^n_j \\right) \\, \n",
    "        & \\textrm{when neuron $j$ is output} \\\\\n",
    "        \\left( \\sum_j w_{ji} \\delta^n_j \\right) f'\\left( a^n_j \\right) \n",
    "        & \\textrm{when neuron $j$ is hidden},\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "for any layer $i$ followed by $j$. The computed gradient is:\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial C^n}{\\partial w_{ji}} = \\delta^n_j y_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation algorithm\n",
    "\n",
    "#####  1. Forward step:\n",
    "\n",
    "We use our weights and activation rules to compute the output layer by layer going forward starting from the first, input layer. The output of a neuron in layer $j$ for case $n$ will be calculated as $y^n_j = f \\left( w_j y^n_{j - 1} \\right)$.\n",
    "\n",
    "\n",
    "##### 2. Backward step: \n",
    "\n",
    "We calculate the error on the output layer, and using backpropagate the error through the network:\n",
    "- calculate the **output error** $\\delta^n$ using $\\delta^n_j = e^n_{j + 1} f'\\left( a^n_{j + 1} \\right)$\n",
    "- backpropagate the error for every preceding layer $l$: $\\delta^n_j = w^n_{j + 1} \\delta^n_{j + 1}f'\\left( a^n_{j + 1} \\right)$\n",
    "- update weigths using the gradient descent rules: $w_{j} = w_{j} + \\alpha \\delta^n_j y_{j - 1}$ where $\\alpha$ is our learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In practice\n",
    "\n",
    "To implement the algorithm, we have to choose an activation function and find its first derivate. \n",
    "The tipical activation) functions are the **sigmoid** and the **hyperbolic tangent** function.\n",
    "\n",
    "##### Sigmoid\n",
    "\n",
    "<img src=\"./pics/functions/sigmoid.png\" width=400 align=\"left\">\n",
    "\n",
    "$$\\begin{align}\n",
    "    \\sigma(x) & = \\frac{1}{1+e^{-x}} \\\\\n",
    "    \\frac{\\partial \\sigma(x)}{\\partial x} & = \\sigma (x)\\cdot (1-\\sigma(x))\n",
    "\\end{align}$$\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "##### Hyperbolic tangent\n",
    "\n",
    "<img src=\"./pics/functions/tanh.png\" width=400 align=\"left\">\n",
    "\n",
    "$$\\begin{align}\n",
    "    \\tanh(x) & = \\frac{1 - e^{-2x}}{1 + e^{-2x}} \\\\\n",
    "    \\frac{\\partial \\tanh(x)}{\\partial x} & = 1 - \\frac{(e^x-e^{-x})^2}{(e^x+e^{-x})^2} = 1 - \\tanh^2(x)\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try it out!\n",
    "\n",
    "<img src=\"./pics/ann/simple_mlp_network.png\" width=400>\n",
    "\n",
    "Let's build a neural network using the rules above. The network topology should include a hidden layer with two neurons, and an output layer with a single neuron, like the one above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from helpers import plot_results_with_hyperplane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1. / (1. + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1. - np.tanh(x) ** 2\n",
    "\n",
    "# create a mapping to simplify later code snippets\n",
    "activation_function = {\n",
    "    'sigmoid': {'f': sigmoid, \"f'\": sigmoid_prime},\n",
    "    'tanh': {'f': tanh, \"f'\": tanh_prime},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implement Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_weight(layers, index):\n",
    "    # number of neurons in prev layer + 1 bias\n",
    "    input_size = layers[index - 1] + 1\n",
    "    \n",
    "    # number of neurons in the actual layer\n",
    "    extra_bias = int(not index == len(layers) - 1) # + 1 bias if not output layer \n",
    "    neuron_count = layers[index] + extra_bias\n",
    "    \n",
    "    shape = (input_size, neuron_count)\n",
    "    \n",
    "    # 2 * (0 ... 1) - 1 = (-1 ... 1)\n",
    "    return 2 * np.random.random(shape) - 1\n",
    "\n",
    "\n",
    "def add_bias(X):\n",
    "    \"\"\"Add a bias column to x with 1. values.\"\"\"\n",
    "    if X.ndim == 1:\n",
    "        return np.concatenate(([1], X))\n",
    "    \n",
    "    nrows, _ = X.shape\n",
    "    ones = np.ones((nrows, 1))\n",
    "    return np.concatenate((ones, X), axis=1)\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, layers=[2, 2, 1], activation='sigmoid', alpha=0.1):\n",
    "        self.activation = activation_function[activation][\"f\"]\n",
    "        self.activation_prime = activation_function[activation][\"f'\"]\n",
    "        \n",
    "        self.alpha = alpha\n",
    "    \n",
    "        # init weights\n",
    "        self.layers = layers\n",
    "        self.weights = [random_weight(layers, i) \n",
    "                        for i in range(1, len(layers))]\n",
    "    \n",
    "    def __str__(self):\n",
    "        layers = \" x \".join(str(l) for l in self.layers)\n",
    "        return f'NeuralNet[{layers}]'\n",
    "        \n",
    "    def forward(self, x):\n",
    "        nlayers = len(self.weights)\n",
    "        \n",
    "        # 0. initialize input\n",
    "        # a_0 = x\n",
    "        a = [add_bias(x)]\n",
    "\n",
    "        # 1. forward step\n",
    "        # -> a_{j+1} = f(Wa_j)\n",
    "        for layer in range(nlayers):\n",
    "            dot_value = np.dot(a[layer], self.weights[layer])\n",
    "            activation = self.activation(dot_value)\n",
    "            a.append(activation)\n",
    "        \n",
    "        return a\n",
    "\n",
    "    def delta(self, a, y):\n",
    "        nlayers = len(self.weights)\n",
    "        \n",
    "        # 2. delta calculation\n",
    "        # 2.a output layer\n",
    "        # -> e = t - a \n",
    "        # -> delta = e * f'(a)\n",
    "        error = y - a[-1]  # move the -1 multiplyer to the weight update step\n",
    "        deltas = [error * self.activation_prime(a[-1])]\n",
    "\n",
    "        # 2.b hidden layers\n",
    "        # -> delta_j = delta_{j+1} * w_{j+1} * f'(a_{j+1})\n",
    "        for layer in range(nlayers - 1, 0, -1):\n",
    "            dot_value = np.dot(deltas[-1], self.weights[layer].T)\n",
    "            delta = dot_value * self.activation_prime(a[layer])\n",
    "            deltas.append(delta)\n",
    "\n",
    "        # 2.c reverse deltas\n",
    "        # [level3(output) -> level2(hidden)] => [level2(hidden) -> level3(output)]\n",
    "        deltas.reverse()\n",
    "        return deltas\n",
    "        \n",
    "    def backward(self, a, deltas):\n",
    "        nlayers = len(self.weights)\n",
    "    \n",
    "        # 3. backpropagation\n",
    "        # -> w_j = w_j + alpha * delta_j * a_{j-1}\n",
    "        for layer in range(nlayers):\n",
    "            inputs = np.atleast_2d(a[layer]).T\n",
    "            delta = np.atleast_2d(deltas[layer])\n",
    "            self.weights[layer] += self.alpha * np.dot(inputs, delta) # -1 multiplier added here\n",
    "\n",
    "        \n",
    "    def fit(self, X, y, epochs=100):\n",
    "        nrows, nfeats = X.shape\n",
    "        nlayers = len(self.weights)\n",
    "            \n",
    "        for _ in range(epochs):\n",
    "            for i in range(nrows):\n",
    "                a = self.forward(X[i])\n",
    "                deltas = self.delta(a, y[i])\n",
    "                self.backward(a, deltas)\n",
    "        \n",
    "        return self\n",
    "            \n",
    "    def predict(self, X): \n",
    "        a = add_bias(X)\n",
    "        for layer in self.weights:\n",
    "            a = self.activation(np.dot(a, layer))\n",
    "        return a > 0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Dataset\n",
    "\n",
    "Let's use the Rosenblatt compatible non-linear `XOR` truth table.\n",
    "\n",
    "A | B | output |\n",
    "--|---|--------|\n",
    "0 | 0 | -1     |\n",
    "0 | 1 |  1     |\n",
    "1 | 0 |  1     |\n",
    "1 | 1 | -1     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])\n",
    "labels = np.array([1, -1, -1, 1])\n",
    "\n",
    "plt.scatter(x=inputs[:, 0], y=inputs[:, 1], c=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet = NeuralNetwork(layers=[2, 2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet.fit(inputs, labels, epochs=100)\n",
    "plot_results_with_hyperplane(inputs, labels, nnet, str(nnet));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "The previous model failed to learn the presented problem. Let's try to figure out what was the problem!\n",
    "To rule out the problem, let's use scikit-learn's implementation of the neural network, called `MLPClassifier`.\n",
    "\n",
    "#### a) Validating the implementation\n",
    "\n",
    "1. Find and import the model.\n",
    "2. Initialize with the same parameters we used with our own implementation.\n",
    "3. Fit the model on the same dataset.\n",
    "4. Use `plot_results_with_hyperplane` function to show the scikit-learn model's behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 1. import MLPClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 2. initialize with the correct parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 3. Fit the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Answer the following questions:\n",
    "\n",
    "1. How does the scikit-learn model behave?\n",
    "2. How would you start to investigate the problem? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Try a different dataset\n",
    "\n",
    "Let's take a step back and use the linearly separable problem from before: the truth table of the `AND` logical operator:\n",
    "\n",
    "A | B | output |\n",
    "--|---|--------|\n",
    "0 | 0 | -1     |\n",
    "0 | 1 | -1     |\n",
    "1 | 0 | -1     |\n",
    "1 | 1 |  1     |\n",
    "\n",
    "1. Reinitialize our model\n",
    "2. Fit on the dataset \n",
    "3. Plot the fitted model's behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 1. define dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 2. re-initialize the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 3. Fit the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) What was the outcome?\n",
    "\n",
    "- What was the reason of the failure? \n",
    "- Do you know a quick way to fix it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e) Resolve the problem\n",
    "\n",
    "Hint 1: \n",
    "<font color='white'>The problem can be fixed from two direction: either modify the dataset or the network.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint 2: <font color='white'>Try to compare the activation function with the expected output.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Resolve the problem :) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good job!\n",
    "\n",
    "In the next chapter we'll discover how can we implement and expand the same network using `keras`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
