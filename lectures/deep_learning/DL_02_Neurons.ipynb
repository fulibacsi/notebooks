{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pics/DL.png\" width=110 align=\"left\" style=\"margin-right: 10px\">\n",
    "\n",
    "# Introduction to Deep Learning\n",
    "\n",
    "## 02. Neurons\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a neuron?\n",
    "\n",
    "<img src =\"pics/neuron.png\" width=\"300\" align=\"left\"/>\n",
    "\n",
    "###### Purpose\n",
    "\n",
    "The basic building block of our neural system is the **neuron**, or nerve cell. These cells are specified to process and transmit information through the nerve system. It is activated by electrical signals and it is able to communicate with other cells through **synapses**. For this purpose neurons have different extensions.  \n",
    "\n",
    "##### Main parts\n",
    "\n",
    "- **dendrite**s: To receive information, neurons has branching extensions called dendrites. A neuron can be connected to many other cells through its dendrites.\n",
    "- **axon**: A longer, cable like extension used to transmit electrical surges - information to other cells. It's length could be 100x of the neuron's body. Many neurons can connect to a sigle axon.\n",
    "- **soma**: The body of the neuron. It contains the **nucleon**.\n",
    "\n",
    "##### Working mechanism\n",
    "\n",
    "A simplified high level view of the inner workings of a neuron is the following:  \n",
    "\n",
    "The **neuron is stimulated** through its dendrites by the connected neurons through the synapses between the stimulating neuron's axon and the receiving neuron's dendrite.  \n",
    "The cell absorbes the stimulation **until a threshold**. When it reaches said threshold, **it'll** start to **stimulate the connected cells** through it's axon. A neuron actively stimulating is called a \"firing neuron\".  \n",
    "These stimulations are small electrical signals. As a **connection used** more and more it **is getting stronger** and the information / **knowledge is stored in** these **connection strengths**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is an artificial neuron?\n",
    "\n",
    "<img src =\"pics/artificial_neuron.png\" width=\"300\" align=\"left\"/>\n",
    "\n",
    "##### Taxonomy\n",
    "\n",
    "Artificial Neural Networks are a supervised machine learning methods for classification and regression purpose. They are based on the inner workings of the (human) brain. Their basic building blocks are simple execution units and the connections between them.  \n",
    "\n",
    "##### Prediction\n",
    "\n",
    "These execution units are called **neurons**, and their job is to compute the weighted summation of their inputs, then applying an output function. Based on their simple nature a [neuron](https://en.wikipedia.org/wiki/Perceptron) is only capable of solving linear problems. Their mechanism is easily expressed by the following equation:\n",
    "$$y = f(w \\cdot x + b)$$\n",
    "where $f$ is an applied activation function (also referenced as output function, or output nonlinearity), representing the nonlinear physical properties of a biological nerve cell; $w$ is the weights associated with the inputs, representing the strength of the connections between the cells; $x$ is the incoming data, representing the electrical signal values; and $b$ is the bias, representing a neuron's threshold.\n",
    "\n",
    "##### Training\n",
    "\n",
    "The learning process is simply adjusting the input weights based on the observed data. First a **forward step** is executed which **is** a **prediction** based on the incoming inputs, the actual weight and bias values using the equation above. The output is compared to the associated label and the error is computed. The goal is to minimize this *error* by modifying the weights. The **weight update is** often referenced as the **backward step**.  \n",
    "Depending on the activation function, there are many different weight update rule available. There are many ways to **create** such **update rules**, but the most common is by **using stochastic gradient descent methods**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try it out!\n",
    "\n",
    "Implement a simple neuron capable of predicting based on its initial weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from helpers import plot_results_with_hyperplane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Dataset\n",
    "\n",
    "Let's use a simple dataset, the truth table of the `AND` logical operation.\n",
    "\n",
    "A | B | output |\n",
    "--|---|--------|\n",
    "0 | 0 | 0      |\n",
    "0 | 1 | 0      |\n",
    "1 | 0 | 0      |\n",
    "1 | 1 | 1      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "labels = np.array([0, 0, 0, 1])\n",
    "\n",
    "plt.scatter(x=inputs[:, 0], y=inputs[:, 1], c=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example activation function\n",
    "\n",
    "Let's use a really simple binary absolute function:\n",
    "\n",
    "$$\n",
    "\\operatorname{abs_{bin}}(x) := {\\begin{cases}\n",
    "                                    1 & {\\text{if }} x>=0, \\\\\n",
    "                                    0 & {\\text{otherwise}}.\n",
    "                                \\end{cases}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_abs(x):\n",
    "    \"\"\"Returns 1 if x >= 0; 0 otherwise.\"\"\"\n",
    "    return (x >= 0).astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick check if it really does what it's suppose to do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in np.array([-0.1, 0, 12]):\n",
    "    print(f'binary_abs({value}) = {binary_abs(value)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    \n",
    "    def __init__(self, n):\n",
    "        self.bias = -np.random.random()\n",
    "        self.weights = np.random.rand(1, n)\n",
    "        \n",
    "    def predict(self, inputs):\n",
    "        weighted_inputs = inputs * self.weights + self.bias\n",
    "        sums_of_inputs = np.sum(weighted_inputs, axis=1)\n",
    "        result = binary_abs(sums_of_inputs)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron = Neuron(2)\n",
    "preds = neuron.predict(inputs)\n",
    "\n",
    "plot_results_with_hyperplane(inputs, labels, neuron);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rosenblatt Perceptron model\n",
    "\n",
    "<img src =\"pics/neuron_explanation.png\" width=\"500\"/>\n",
    "\n",
    "One of the first neuron model was the Rosenblatt Perceptron model named after its creator, the psychologist Frank Rosenblatt, who experimented with artificial neural networks and proposed the model in 1958. It is a linear binary classifier. \n",
    "\n",
    "#### Prediction\n",
    "\n",
    "The selected activation function was the threshold function (also referenced as $\\operatorname{sign}$):\n",
    "\n",
    "$$\n",
    "\\operatorname{sign}(x):={\\begin{cases}\n",
    "                             1 & {\\text{if }} x>=0, \\\\\n",
    "                             -1 & {\\text{otherwise}}.\n",
    "                         \\end{cases}}\n",
    "$$\n",
    "\n",
    "So the prediction can be written as:\n",
    "\n",
    "$$\\operatorname{f}(x) = \\operatorname{sign}(w \\cdot x + b)$$\n",
    "\n",
    "#### Training\n",
    "\n",
    "To compute the weight update function, we are going to use stochastic gradient descent. The gradient theorem states that the gradient points towards the maximum increase from any given point (given some condition is met). Gradient descent methods use this theorem: pick an initial point, and follow the negative gradient to find a local optimum.\n",
    "\n",
    "Our objective is to minimize the error which can be expressed as follows:\n",
    "$$e = t - y$$\n",
    "where $d$ is the expected label, $y$ is the outcome of the model. Based on the value range of $t$ and $y$, there can be 3 possible values: \n",
    "- **0**, if $t$ and $y$ matches, \n",
    "- **+2** if the prediction was -1 instead of 1,\n",
    "- and **-2** if we it predicted 1 when the expected value was -1.\n",
    "\n",
    "To simplify the computation of the [derivative](https://en.wikipedia.org/wiki/Sign_function), we'll introduce the following inequalities:\n",
    "$$t \\cdot (w \\cdot x + b) > 0$$\n",
    "if there is an error, \n",
    "$$t \\cdot (w \\cdot x + b) < 0$$\n",
    "otherwise.\n",
    "\n",
    "Using these we'll write the function to minimize as:\n",
    "\n",
    "$$\\min \\operatorname{L}(w, b) = - \\sum_{i | x_i \\in E} t_i \\cdot (w \\cdot x_i + b)$$\n",
    "\n",
    "Where $E$ is the set of incorrectly classified cases. From here we can get the two update rule by taking the partial derivatives:\n",
    "\n",
    "$$\\begin{align}\n",
    "    \\frac{\\partial \\operatorname{L}(w, b)}{\\partial w} & = - \\sum_{i | x_i \\in E} t_i \\cdot x_i \\\\\n",
    "    \\frac{\\partial \\operatorname{L}(w, b)}{\\partial b} & = - \\sum_{i | x_i \\in E} t_i\n",
    "\\end{align}$$\n",
    "\n",
    "Using this results, our update functions are:\n",
    "\n",
    "$$\\begin{align}\n",
    "    w_{t+1} & = w_{t} + \\alpha \\cdot t_{i} \\cdot x_{i} \\\\\n",
    "    b_{t+1} & = w_{t} + \\alpha \\cdot t_{i}\n",
    "\\end{align}$$\n",
    "\n",
    "Where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try it out!\n",
    "\n",
    "Implement the Rosenblatt Perceptron model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Dataset\n",
    "\n",
    "We are going to use the same dataset with a slight change: the outputs should be -1 instead of 0 in order to work with the Perceptron model.\n",
    "\n",
    "A | B | output |\n",
    "--|---|--------|\n",
    "0 | 0 | -1     |\n",
    "0 | 1 | -1     |\n",
    "1 | 0 | -1     |\n",
    "1 | 1 |  1     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "labels = np.array([-1, -1, -1, 1])\n",
    "\n",
    "plt.scatter(x=inputs[:, 0], y=inputs[:, 1], c=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sign(x):\n",
    "    return (x >= 0).astype('int') - (x < 0).astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in np.array([-0.1, 0, 12]):\n",
    "    print(f'sign({value}) = {sign(value)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rosenblatt:\n",
    "    \n",
    "    def __init__(self, n, alpha=0.1, epochs=10):\n",
    "        \"\"\"Implement the Rosenblatt Perceptron model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        - n : int\n",
    "          Number of inputs\n",
    "        - alpha : float\n",
    "          Learning rate\n",
    "        - epochs : int\n",
    "          Number of training rounds\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.epochs = epochs\n",
    "        self.weights = np.random.rand(n)\n",
    "        self.bias = np.random.random()\n",
    "        \n",
    "    def predict(self, X):\n",
    "        X_w = np.dot(X, self.weights) + self.bias\n",
    "        return sign(X_w)\n",
    "    \n",
    "    def train_one_step(self, x, y):\n",
    "        y_hat = self.predict(x)\n",
    "        direction = y * (y != y_hat)  # equals 0 if the prediction is correct \n",
    "        self.bias += self.alpha * direction\n",
    "        self.weights += self.alpha * direction * x    \n",
    "        \n",
    "    def train(self, X, y):\n",
    "        nrows, nfeats = X.shape\n",
    "        for _ in range(self.epochs):\n",
    "            for i in range(nrows):\n",
    "                self.train_one_step(X[i], y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron = Rosenblatt(n=2)\n",
    "preds = perceptron.predict(inputs)\n",
    "\n",
    "plot_results_with_hyperplane(inputs, labels, perceptron, 'perceptron');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the model is improved after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron.train(inputs, labels)\n",
    "preds = perceptron.predict(inputs)\n",
    "\n",
    "plot_results_with_hyperplane(inputs, labels, perceptron, 'perceptron');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron.weights, perceptron.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scikit-learn model\n",
    "\n",
    "Scikit-learn implemented the perceptron model, let's try that implementation on the same problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron = Perceptron(verbose=2, random_state=42).fit(inputs, labels)\n",
    "\n",
    "plot_results_with_hyperplane(inputs, labels, perceptron, 'perceptron');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations\n",
    "\n",
    "The perceptron model (and a single neuron really) is a linear model with known limitations. Consider the following dataset - the truth table of the `XOR` logical operation:\n",
    "\n",
    "A | B | output |\n",
    "--|---|--------|\n",
    "0 | 0 | -1     |\n",
    "0 | 1 |  1     |\n",
    "1 | 0 |  1     |\n",
    "1 | 1 | -1     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "labels = np.array([-1, 1, 1, -1])\n",
    "\n",
    "plt.scatter(x=inputs[:, 0], y=inputs[:, 1], c=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a **linearly unseparable case**: we cannot draw a single straight line which separates the classes without errors. Let's see what happens if we try to fit a model on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron = Perceptron(random_state=42).fit(inputs, labels)\n",
    "plot_results_with_hyperplane(inputs, labels, perceptron, 'perceptron');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron.coef_, perceptron.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "conf_mat = confusion_matrix(labels, perceptron.predict(inputs))\n",
    "sns.heatmap(conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that it failed at this problem, just as we expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Consider the following rosenblatt perceptron:\n",
    "\n",
    "<img src =\"pics/neuron_exercise.png\" width=\"400\" align=\"left\"/>\n",
    "\n",
    "**1. Compute** what will be the prediction, the value of the weights, and the bias after one pass on the data, given the following inputs?\n",
    "\n",
    "|$x_1$ | $x_2$ | $x_3$ | $d$ |\n",
    "|------|-------|-------|-----|\n",
    "| 1    | 0     | 1     | 1   |\n",
    "| 0    | 1     | 0     | -1  |\n",
    "\n",
    "And the initial weights are:\n",
    "\n",
    "|$w_1$ | $w_2$ | $w_3$ | $b$  |\n",
    "|------|-------|-------|------|\n",
    "| 0.3  | 0.5   | -0.2  | -1.0 |\n",
    "\n",
    "**2. Validate** your answer using the previous Rosenblatt class!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good job!\n",
    "\n",
    "In the next chapter we'll look into the neural networks and examine what happens if more than one neuron is present in the network."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
