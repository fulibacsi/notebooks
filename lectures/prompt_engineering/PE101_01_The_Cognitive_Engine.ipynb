{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb57e418",
   "metadata": {},
   "source": [
    "# Prompt Engineering 101 - Part I.\n",
    "## The Cognitive Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7793712c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a44421",
   "metadata": {},
   "source": [
    "### *The Mechanics of Alien Intelligence*\n",
    "\n",
    "## 1. It Reads Numbers, Not Words (Tokenization)\n",
    "AI does not see \"Apple\". It sees `[2034]`.\n",
    "* **The Math Glitch:** Because `9.11` tokenizes into `[9, ., 11]`, the AI often thinks it is larger than `9.9` `[9, ., 9]`. It is predicting text patterns, not calculating values.\n",
    "\n",
    "## 2. Meaning is Geometry (Embeddings)\n",
    "Words are coordinates in a high-dimensional map.\n",
    "* **Semantic Proximity:** \"King\" is close to \"Queen\". \"Paris\" is close to \"France\".\n",
    "* **The Chameleon Effect:** A word's location shifts based on context. \"Bank\" (river) and \"Bank\" (money) are mathematically distinct concepts to a modern AI.\n",
    "\n",
    "## 3. The Stochastic Parrot (Probabilities)\n",
    "The AI predicts the next word based on statistical likelihood.\n",
    "* **Bias:** If the internet associates \"Doctor\" with \"He\", the AI will too, unless steered.\n",
    "* **Hallucination:** When the AI doesn't know the answer, it picks the most *statistically plausible* sounding word, even if it's a lie.\n",
    "\n",
    "## 4. You Are The Pilot (Parameters)\n",
    "* **Temperature:** Controls randomness.\n",
    "    * **Low (0.0 - 0.3):** Factual, robotic, consistent. (Use for: Data extraction).\n",
    "    * **High (0.8 - 1.5):** Creative, chaotic, diverse. (Use for: Brainstorming).\n",
    "* **Seed:** A number that freezes randomness. Using the same Seed + same Temperature = Identical result every time.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947523b4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3b23b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title üõ†Ô∏è Step 0: Laboratory Setup\n",
    "# We are installing the 'transformers' library to access the AI models\n",
    "# and visualization tools to see what's happening inside.\n",
    "\n",
    "!pip install transformers torch scipy matplotlib seaborn scikit-learn --quiet\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, BertModel, BertTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# We load TWO brains today to compare them:\n",
    "# 1. GPT-2 (The Writer) - Good at generating text.\n",
    "# 2. BERT (The Reader) - Good at understanding meaning.\n",
    "\n",
    "print(\"Loading the 'Writer' (GPT-2)...\")\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "gpt_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "gpt_model.eval()\n",
    "\n",
    "# GPT-2 doesn't have a default pad token, so we use the EOS token.\n",
    "# This silences the \"attention mask\" warnings.\n",
    "gpt_tokenizer.pad_token = gpt_tokenizer.eos_token\n",
    "gpt_model.config.pad_token_id = gpt_tokenizer.eos_token_id\n",
    "\n",
    "print(\"Loading the 'Reader' (BERT)...\")\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "bert_model.eval()\n",
    "\n",
    "print(\"‚úÖ Setup Complete! Ready to explore.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d363f633",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb14139",
   "metadata": {},
   "source": [
    "## **Topic 1: Tokenization (The Input)**\n",
    "\n",
    "**Concept:** The AI does not read English (or any language). It reads numbers. \"Tokenization\" is the process of translating text into a string of integers.\n",
    "\n",
    "**Analogy:** Think of this like Morse Code. The AI doesn't hear the \"beep,\" it processes the pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202cd3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title üî¢ Topic 1: Seeing the Matrix (Tokenization)\n",
    "input_text = \"The quick brown fox jumps over the lazy dog.\" # @param {type:\"string\"}\n",
    "\n",
    "# 1. Convert Text to Tokens (Numbers)\n",
    "input_ids = gpt_tokenizer.encode(input_text)\n",
    "\n",
    "# 2. Convert Tokens back to Text chunks (to see how it splits words)\n",
    "tokens = [gpt_tokenizer.decode([x]) for x in input_ids]\n",
    "\n",
    "# Visualization\n",
    "print(f\"Original Text:  {input_text}\")\n",
    "print(f\"-\"*40)\n",
    "print(f\"{'TOKEN (The Chunk)':<20} | {'ID (The Number)':<10}\")\n",
    "print(f\"-\"*40)\n",
    "\n",
    "for t, i in zip(tokens, input_ids):\n",
    "    # We use repr() to show spaces clearly\n",
    "    print(f\"{repr(t):<20} | {i:<10}\")\n",
    "\n",
    "print(f\"-\"*40)\n",
    "print(f\"Total Tokens: {len(input_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd770749",
   "metadata": {},
   "source": [
    "**Discussion Point:** Notice how common words like \"The\" have a specific ID. Notice if you type a complex word (like \"unimaginable\"), it might get split into multiple tokens.  \n",
    "**Constraint:** The AI charges by the *token*, not the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541865b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title üßÆ Topic 1.5: The Math Glitch (Why LLMs struggle with numbers)\n",
    "# We often assume AI is a computer, so it must be good at math.\n",
    "# Let's see how it actually \"reads\" numbers.\n",
    "\n",
    "# CASE A: A Simple Number\n",
    "num_simple = \"100\"\n",
    "# CASE B: A Decimal that confuses tokenizers\n",
    "num_confusing = \"9.11\"\n",
    "# CASE C: A slightly larger number\n",
    "num_comparison = \"9.9\"\n",
    "\n",
    "def show_tokens(text):\n",
    "    ids = gpt_tokenizer.encode(text)\n",
    "    tokens = [gpt_tokenizer.decode([x]) for x in ids]\n",
    "    print(f\"Input: '{text}' -> Tokens: {tokens}  (IDs: {ids})\")\n",
    "\n",
    "print(\"--- HOW AI READS NUMBERS ---\")\n",
    "show_tokens(num_simple)\n",
    "show_tokens(num_confusing)\n",
    "show_tokens(num_comparison)\n",
    "\n",
    "print(\"\\n--- THE 'GLITCH' EXPLAINED ---\")\n",
    "print(\"1. It sees '9.9' as two chunks: [9] and [.9]\")\n",
    "print(\"2. It sees '9.11' as three chunks: [9], [.], and [11]\")\n",
    "print(\"3. To an AI predicting text, the token [11] is 'bigger' (more characters) than [9].\")\n",
    "print(\"4. This is why LLMs often say 9.11 is larger than 9.9. They are doing text completion, not math.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc8301a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784a4706",
   "metadata": {},
   "source": [
    "## **Topic 2: Embeddings (The Meaning)**\n",
    "\n",
    "**Concept:** Once text is a number, the AI looks up that number in a massive multi-dimensional map. Words that mean similar things are \"close\" to each other on this map.\n",
    "\n",
    "**Analogy:** Imagine a library. \"King\" and \"Queen\" are on the same shelf. \"Apple\" and \"Orange\" are in the food section. \"Apple\" and \"iPhone\" might be in the tech section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f687b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title üó∫Ô∏è Topic 2: The Map of Meaning (Embeddings)\n",
    "\n",
    "# Let's pick 4 words to see how the AI groups them\n",
    "words = [\"King\", \"Queen\", \"Apple\", \"Orange\", \"Computer\", \"Phone\"]\n",
    "word_ids = [gpt_tokenizer.encode(w)[0] for w in words]\n",
    "\n",
    "# Extract the \"embeddings\" (The vector coordinates)\n",
    "# The model sees these words as 768-dimensional coordinates.\n",
    "# We will squash them down to 2 dimensions so we can plot them on a screen.\n",
    "embeddings = gpt_model.transformer.wte.weight[word_ids].detach().numpy()\n",
    "\n",
    "# Use PCA to reduce 768 dimensions to 2 (X and Y axis)\n",
    "pca = PCA(n_components=2)\n",
    "reduced_embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], s=100, c='blue')\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, (reduced_embeddings[i, 0]+0.02, reduced_embeddings[i, 1]+0.02), fontsize=14)\n",
    "\n",
    "plt.title(\"How the AI 'Sees' Concepts in Space\")\n",
    "plt.xlabel(\"Abstract Dimension 1\")\n",
    "plt.ylabel(\"Abstract Dimension 2\")\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f435103a",
   "metadata": {},
   "source": [
    "**Discussion Point:** Look at the chart. Even without being told, the AI knows \"King\" and \"Queen\" belong together, and \"Apple\" and \"Orange\" belong together. This is \"Semantic Understanding.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5961009c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb7ba2e",
   "metadata": {},
   "source": [
    "## **Topic 3: Attention (The Context)**\n",
    "\n",
    "**Concept:** Words change meaning based on neighbors. \"Bank\" means something different in \"River bank\" vs \"Bank deposit.\" The **Attention Mechanism** allows tokens to \"talk\" to each other to resolve this ambiguity.\n",
    "\n",
    "**Analogy:** When you read a sentence, your eyes jump back and forth to connect \"He\" to \"John.\" The AI does this mathematically.\n",
    "\n",
    "*Note: Visualizing raw attention weights from GPT-2 is complex. This visualization is a conceptual simulation to demonstrate the **effect** of attention.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3effac41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title üëÅÔ∏è Topic 3: Context Matters (Attention Visualization)\n",
    "\n",
    "sentence = \"The bank of the river.\"\n",
    "# We will simulate how the word \"bank\" pays attention to other words\n",
    "words = sentence.split()\n",
    "attention_scores = [0.1, 0.9, 0.1, 0.05, 0.8] # \"Bank\" focuses heavily on \"Bank\" and \"River\"\n",
    "\n",
    "# Plotting a heatmap\n",
    "plt.figure(figsize=(10, 2))\n",
    "sns.heatmap([attention_scores], annot=[words], fmt=\"\", cmap=\"Blues\", cbar=False,\n",
    "            xticklabels=False, yticklabels=False, square=True, linewidths=1)\n",
    "plt.title(\"Visualizing Attention: When processing 'bank', where does the AI look?\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nExplanation:\")\n",
    "print(\"Darker Blue = The AI is paying more attention to this word to understand the context.\")\n",
    "print(\"In this case, 'River' helps the AI understand that 'Bank' refers to nature, not money.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1c03c8",
   "metadata": {},
   "source": [
    "## **The Chameleon Word (Contextual Embeddings)**\n",
    "\n",
    "**Concept:** \n",
    "- **Static Embedding (The Dictionary)**: When the word \"Apple\" first enters the model, it is just a generic concept. It holds all meanings at once (Fruit, Tech Company, Record Label).\n",
    "- **Contextualized Embedding (The Understanding)**: As the token passes through the model's layers (interacting with other words via Attention), its vector physically moves in the high-dimensional space. By the final layer, \"Apple\" (in a pie context) has moved close to \"Food\", while \"Apple\" (in an iPhone context) has moved close to \"Technology\".\n",
    "\n",
    "**The Demo:** We will take the word **\"Apple\"**. We will place it in two different sentences. We will then measure the distance of that same word to the concept of **\"Fruit\"** and **\"Technology\"** to prove it has \"changed sides.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04da4ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ü¶é Topic 3.5: The Chameleon Word (Contextual Embeddings)\n",
    "# We will look at how the word \"Bank\" changes its \"location\" in meaning-space\n",
    "# depending on the sentence it lives in.\n",
    "# We also observe how does the model behaviour changes based on its purpose.\n",
    "\n",
    "# Define two sentences with the same word but different meanings\n",
    "# Does the word \"Bank\" change meaning based on the sentence?\n",
    "sent_nature = \"The boat floated by the river bank\"\n",
    "sent_finance = \"I went to the bank to deposit money\"\n",
    "\n",
    "\n",
    "# Define \"Anchor Concepts\" to measure against\n",
    "# We want to see if the word is closer to \"Water\" or \"Money\"\n",
    "# Anchors to measure against (The \"North Stars\" of meaning)\n",
    "anchor_word_nature = \"water\"\n",
    "anchor_word_finance = \"money\"\n",
    "\n",
    "# --- HELPER FUNCTIONS ---\n",
    "                                          \n",
    "def get_word_index(tokenizer, text, word):\n",
    "    \"\"\" Helper to find where the word is in the token list \"\"\"\n",
    "    # Encode with special tokens (if any)\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "    tokens = [tokenizer.decode([t]).strip().lower() for t in input_ids[0]]\n",
    "    \n",
    "    # Find the index\n",
    "    try:\n",
    "        # We search for the word. Note: Tokenizers are tricky (e.g., \" bank\" vs \"bank\")\n",
    "        # This is a simplified search for demonstration\n",
    "        idx = tokens.index(word.lower())\n",
    "    except ValueError:\n",
    "        # Fallback: if exact match fails, look for substring\n",
    "        # (e.g. GPT2 might tokenize \" bank\" with a space)\n",
    "        idx = -1\n",
    "        for i, t in enumerate(tokens):\n",
    "            if word.lower() in t:\n",
    "                idx = i\n",
    "                break\n",
    "    return idx, input_ids\n",
    "\n",
    "def get_gpt2_embedding(sentence, word):\n",
    "    # 1. Find the word index\n",
    "    idx, input_ids = get_word_index(gpt_tokenizer, sentence, word)\n",
    "    \n",
    "    if idx == -1: return None # Word not found\n",
    "    \n",
    "    # 2. Run the model\n",
    "    with torch.no_grad():\n",
    "        outputs = gpt_model(input_ids, output_hidden_states=True)\n",
    "        \n",
    "    # 3. Get the vector\n",
    "    # GPT-2: We use the last layer. \n",
    "    # Even though it's the \"Writer\", we want to see what it thinks \"bank\" is \n",
    "    # at that specific moment in the sequence.\n",
    "    hidden_states = outputs.hidden_states[-1]\n",
    "    return hidden_states[0, idx, :] \n",
    "\n",
    "def get_bert_embedding(sentence, word):\n",
    "    # 1. Find the word index\n",
    "    idx, input_ids = get_word_index(bert_tokenizer, sentence, word)\n",
    "    \n",
    "    if idx == -1: return None\n",
    "    \n",
    "    # 2. Run the model\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(input_ids)\n",
    "        \n",
    "    # 3. Get the vector\n",
    "    # BERT: Use 2nd to last layer for best semantic representation\n",
    "    return outputs.hidden_states[-2][0, idx, :]\n",
    "\n",
    "# --- RUN THE EXPERIMENT ---\n",
    "\n",
    "# 1. Get Anchor vectors (Baseline meanings)\n",
    "# We embed them in simple contexts to get a clean read\n",
    "vec_bert_water = get_bert_embedding(\"water is clear\", anchor_word_nature)\n",
    "vec_bert_money  = get_bert_embedding(\"money is green\", anchor_word_finance)\n",
    "\n",
    "# 2. Get Target Word vectors (The word \"Bank\")\n",
    "vec_bert_nature = get_bert_embedding(sent_nature, \"bank\")\n",
    "vec_bert_finance = get_bert_embedding(sent_finance, \"bank\")\n",
    "\n",
    "# 3. Calculate Similarities (Cosine Similarity)\n",
    "# 1.0 = Identical, 0.0 = Unrelated\n",
    "cos = torch.nn.CosineSimilarity(dim=0)\n",
    "\n",
    "# BERT Results\n",
    "bert_nat_w = cos(vec_bert_nature, vec_bert_water).item()\n",
    "bert_nat_c = cos(vec_bert_nature, vec_bert_money).item()\n",
    "bert_fin_w = cos(vec_bert_finance, vec_bert_water).item()\n",
    "bert_fin_c = cos(vec_bert_finance, vec_bert_money).item()\n",
    "\n",
    "# --- VISUALIZATION ---\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Plot 2: BERT (Reader)\n",
    "x = np.arange(2)\n",
    "width = 0.35\n",
    "\n",
    "rects1 = ax.bar(x - width/2, [bert_nat_w, bert_fin_w], width, label='Similarity to Water', color='blue')\n",
    "rects2 = ax.bar(x + width/2, [bert_nat_c, bert_fin_c], width, label='Similarity to Money', color='green')\n",
    "\n",
    "ax.set_ylabel('Similarity')\n",
    "ax.set_title('BERT (Reader)\\nBidirectional reading creates strong context')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(['Bank in RIVER sentence', 'Bank in MONEY sentence'])\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.0)\n",
    "\n",
    "# Add labels\n",
    "def autolabel(rects, ax):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.2f}',\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3), textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1, ax)\n",
    "autolabel(rects2, ax)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- ANALYSIS ---\")\n",
    "print(\"1. In the River sentence (Left Pair), the Blue Bar (Water) is much higher.\")\n",
    "print(\"2. In the Money sentence (Right Pair), the Gold Bar (Money) is much higher.\")\n",
    "print(\"This proves the AI 'Understands' context.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93623981",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c4f829",
   "metadata": {},
   "source": [
    "## **Topic 4: The Prediction Machine (Probabilities)**\n",
    "\n",
    "**Concept:** The AI is not creative; it is probabilistic. It calculates the % chance of *every single word in the dictionary* coming next, and picks the most likely one.\n",
    "\n",
    "**Analogy:** Autocomplete on steroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cd6f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title üé≤ Topic 4: Predicting the Next Token\n",
    "\n",
    "# Type a prompt here\n",
    "prompt_text = \"The most popular pet in the world is the\" # @param {type:\"string\"}\n",
    "\n",
    "# 1. Process the input\n",
    "inputs = gpt_tokenizer(prompt_text, return_tensors=\"pt\")\n",
    "\n",
    "# 2. Ask the model for predictions\n",
    "with torch.no_grad():\n",
    "    outputs = gpt_model(**inputs)\n",
    "    next_token_logits = outputs.logits[0, -1, :] # Get the scores for the last word\n",
    "    probs = F.softmax(next_token_logits, dim=-1) # Convert scores to percentages\n",
    "\n",
    "# 3. Get the Top 5 candidates\n",
    "top_k = 5\n",
    "top_probs, top_indices = torch.topk(probs, top_k)\n",
    "\n",
    "# Visualization\n",
    "candidates = [gpt_tokenizer.decode([idx]) for idx in top_indices]\n",
    "scores = top_probs.numpy() * 100\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=scores, y=candidates, hue=candidates, palette=\"viridis\", legend=False)\n",
    "plt.xlabel(\"Probability (%)\")\n",
    "plt.title(f\"What comes after: '{prompt_text}'?\")\n",
    "plt.show()\n",
    "\n",
    "print(\"The AI Ranking:\")\n",
    "for c, s in zip(candidates, scores):\n",
    "    print(f\"Token: {repr(c):<15} | Probability: {s:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc1a7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ‚öñÔ∏è Topic 4.5: The Mirror (Visualizing Bias)\n",
    "# LLMs reflect the bias of their training data (the internet).\n",
    "# Let's see what the model thinks is the most likely pronoun for different jobs.\n",
    "\n",
    "# Try changing \"nurse\" to \"doctor\" or \"engineer\" or \"teacher\".\n",
    "profession_prompt = \"The nurse called the patient because\" # @param {type:\"string\"}\n",
    "\n",
    "inputs = gpt_tokenizer(profession_prompt, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = gpt_model(**inputs)\n",
    "    # Get probabilities for the NEXT word\n",
    "    probs = F.softmax(outputs.logits[0, -1, :], dim=-1)\n",
    "\n",
    "# Let's specifically check the probability of \"he\" vs \"she\"\n",
    "id_he = gpt_tokenizer.encode(\" he\")[0]\n",
    "id_she = gpt_tokenizer.encode(\" she\")[0]\n",
    "\n",
    "prob_he = probs[id_he].item() * 100\n",
    "prob_she = probs[id_she].item() * 100\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.barplot(x=['he', 'she'], y=[prob_he, prob_she], palette=['lightblue', 'pink'])\n",
    "plt.title(f\"Probability of Pronouns after: '{profession_prompt}'\")\n",
    "plt.ylabel(\"Probability (%)\")\n",
    "plt.ylim(0, max(prob_he, prob_she) + 5)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Probability of 'he':  {prob_he:.2f}%\")\n",
    "print(f\"Probability of 'she': {prob_she:.2f}%\")\n",
    "print(f\"Bias Factor: The model is {max(prob_he, prob_she)/min(prob_he, prob_she):.1f}x more likely to choose one over the other.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd2a3cb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafb32f8",
   "metadata": {},
   "source": [
    "## **Topic 5: Autoregression (The Loop)**\n",
    "\n",
    "**Concept:** How does it write a whole essay? It predicts one word, adds it to the list, reads the *new* list, predicts the next word, and repeats. This is **Autoregression**.\n",
    "\n",
    "**Analogy:** Laying down train tracks while riding the train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68e988f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title üîÑ Topic 5: The Loop (Autoregression in Action)\n",
    "\n",
    "start_prompt = \"Once upon a time,\" # @param {type:\"string\"}\n",
    "steps_to_generate = 10\n",
    "\n",
    "current_text = start_prompt\n",
    "print(f\"Starting Text: {current_text}\\n\")\n",
    "\n",
    "input_ids = gpt_tokenizer.encode(current_text, return_tensors=\"pt\")\n",
    "\n",
    "for i in range(steps_to_generate):\n",
    "    # 1. Run the model\n",
    "    with torch.no_grad():\n",
    "        outputs = gpt_model(input_ids)\n",
    "        next_token_logits = outputs.logits[0, -1, :]\n",
    "\n",
    "    # 2. Pick the single best winner (Greedy decoding)\n",
    "    next_token_id = torch.argmax(next_token_logits).item()\n",
    "    next_token = gpt_tokenizer.decode([next_token_id])\n",
    "\n",
    "    # 3. Update the text\n",
    "    current_text += next_token\n",
    "    input_ids = torch.cat([input_ids, torch.tensor([[next_token_id]])], dim=1)\n",
    "\n",
    "    print(f\"Step {i+1}: AI chose '{repr(next_token)}' -> Current Story: {current_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3908504e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ü§ñ Topic 5.5: Base Model Behavior (Why it doesn't answer you)\n",
    "# Modern AI (ChatGPT) is \"Instruction Tuned\". Old AI (GPT-2) is a \"Base Model\".\n",
    "# Base models don't answer questions; they just add more text.\n",
    "# Let's see what happens if we treat GPT-2 like ChatGPT.\n",
    "\n",
    "question_prompt = \"Q: What is the capital of France?\\nA:\"\n",
    "\n",
    "#input_ids = gpt_tokenizer.encode(question_prompt, return_tensors=\"pt\")\n",
    "\n",
    "inputs = gpt_tokenizer(question_prompt, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "output = gpt_model.generate(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=30,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    pad_token_id=gpt_tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "generated_text = gpt_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"--- PROMPT ---\\n{question_prompt}\")\n",
    "print(f\"\\n--- GPT-2 RESPONSE ---\")\n",
    "print(generated_text)\n",
    "print(\"\\n--- ANALYSIS ---\")\n",
    "print(\"Did it answer 'Paris'? Or did it generate another Question?\")\n",
    "print(\"Base models often think they are writing a list of questions.\")\n",
    "print(\"To fix this, we need 'Instruction Tuning' (RLHF), which we will discuss in the lecture.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84d37c5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc67f813",
   "metadata": {},
   "source": [
    "## **Topic 6: Hallucination & Temperature (The Risks)**\n",
    "\n",
    "**Concept:** \"Hallucination\" happens when the probabilistic \"winner\" is factually wrong, but statistically likely. We can control the AI's \"creativity\" using a setting called **Temperature**.\n",
    "\n",
    "* **Low Temperature (0.1):** Safe, robotic, repetitive.\n",
    "* **High Temperature (1.5):** Creative, chaotic, prone to nonsense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a81494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title üå°Ô∏è Topic 6: Controlling Chaos (Temperature)\n",
    "\n",
    "prompt = \"The secret to happiness is\" # @param {type:\"string\"}\n",
    "temperature = 1.5 # @param {type:\"slider\", min:0.1, max:2.0, step:0.1}\n",
    "\n",
    "# Encode\n",
    "inputs = gpt_tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "# Generate with Temperature\n",
    "output = gpt_model.generate(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=50,\n",
    "    do_sample=True, # Allow it to pick non-top options\n",
    "    temperature=temperature,\n",
    "    top_k=50,\n",
    "    pad_token_id=gpt_tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "generated_text = gpt_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"--- GENERATION (Temp: {temperature}) ---\")\n",
    "print(generated_text)\n",
    "print(\"-\" * 30)\n",
    "\n",
    "if temperature < 0.5:\n",
    "    print(\"Analysis: Low temp. The output is likely very standard and safe.\")\n",
    "elif temperature > 1.2:\n",
    "    print(\"Analysis: High temp. Did you see any weird words or grammar mistakes? That is the AI taking 'risks'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e3aea4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e54bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title üé≤ Topic 7: The Seed (Controlling Randomness)\n",
    "# In business, we often don't want randomness; we want consistency.\n",
    "# By setting a \"Seed\", we force the random number generator to start at the same place.\n",
    "\n",
    "prompt = \"A quick recipe for a healthy breakfast is\"\n",
    "seed_number = 42 # @param {type:\"integer\"}\n",
    "\n",
    "def generate_with_seed(seed):\n",
    "    # Set the seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    inputs = gpt_tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    \n",
    "    output = gpt_model.generate(\n",
    "        input_ids, \n",
    "        attention_mask=attention_mask,\n",
    "        max_length=40, \n",
    "        do_sample=True, \n",
    "        temperature=1.0, # High temp usually means random, but the seed locks it!\n",
    "        pad_token_id=gpt_tokenizer.eos_token_id\n",
    "    )\n",
    "    return gpt_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"--- ATTEMPT 1 (Seed {seed_number}) ---\")\n",
    "print(generate_with_seed(seed_number))\n",
    "\n",
    "print(f\"\\n--- ATTEMPT 2 (Same Seed {seed_number}) ---\")\n",
    "print(generate_with_seed(seed_number))\n",
    "\n",
    "print(f\"\\n--- ATTEMPT 3 (Different Seed {seed_number + 1}) ---\")\n",
    "print(generate_with_seed(seed_number + 1))\n",
    "\n",
    "print(\"\\n--- LESSON ---\")\n",
    "print(\"If you control the Seed, you control the Chaos.\")\n",
    "print(\"Even with high temperature, the same seed produces the EXACT same text.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8ce248",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bf9825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title üö¶ Topic 8: The \"Lie Detector\" (Visualizing Confidence)\n",
    "# We can ask the model: \"How sure were you about that word?\"\n",
    "# Green = Confident. Red = Guessing.\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "def colorize_text(words, confidences):\n",
    "    html_str = \"\"\n",
    "    for word, conf in zip(words, confidences):\n",
    "        # Color: Green (High Conf) to Red (Low Conf)\n",
    "        # We use HSL for easy color scaling\n",
    "        # Hue 120 (Green) -> Hue 0 (Red)\n",
    "        hue = conf * 120 \n",
    "        html_str += f'<span style=\"background-color: hsl({hue}, 80%, 80%); padding: 2px; border-radius: 4px; margin: 1px;\">{word}</span>'\n",
    "    return html_str\n",
    "\n",
    "prompt = \"The fastest animal on earth is the\"\n",
    "inputs = gpt_tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "# Generate\n",
    "with torch.no_grad():\n",
    "    output = gpt_model.generate(\n",
    "        input_ids, \n",
    "        attention_mask=attention_mask,\n",
    "        max_length=20, \n",
    "        output_scores=True, \n",
    "        return_dict_in_generate=True,\n",
    "        pad_token_id=gpt_tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "# Extract tokens and scores\n",
    "generated_ids = output.sequences[0]\n",
    "scores = output.scores # Tuple of scores for each step\n",
    "\n",
    "# Calculate probabilities for the GENERATED tokens\n",
    "confidences = []\n",
    "generated_words = []\n",
    "\n",
    "# Skip the input prompt, look only at new words\n",
    "input_len = input_ids.shape[1]\n",
    "new_tokens = generated_ids[input_len:]\n",
    "\n",
    "for i, token_id in enumerate(new_tokens):\n",
    "    # Get the logits for this step\n",
    "    step_logits = scores[i]\n",
    "    # Convert to probability (0-1)\n",
    "    probs = F.softmax(step_logits, dim=-1)\n",
    "    # Get the probability of the token that was actually chosen\n",
    "    token_prob = probs[0, token_id].item()\n",
    "    \n",
    "    confidences.append(token_prob)\n",
    "    generated_words.append(gpt_tokenizer.decode([token_id]))\n",
    "\n",
    "# Display\n",
    "print(f\"Prompt: {prompt}...\")\n",
    "display(HTML(colorize_text(generated_words, confidences)))\n",
    "\n",
    "print(\"\\n--- ANALYSIS ---\")\n",
    "print(\"GREEN words: The AI is sure (Common phrases, facts).\")\n",
    "print(\"RED/ORANGE words: The AI is guessing (Names, specific numbers, creative choices).\")\n",
    "print(\"If a Fact is RED, verify it!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594be33d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9bed72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title üß† Topic 9: The Context Window (The Memory Budget)\n",
    "# AI has a limited \"Context Window\". If you exceed it, it forgets the start.\n",
    "# This is a conceptual visualization of how space fills up.\n",
    "\n",
    "# Standard GPT-4 Context: ~128,000 tokens\n",
    "# Standard Harry Potter Book: ~100,000 tokens\n",
    "# This Cell visualizes the \"Budget\"\n",
    "\n",
    "tokens_in_harry_potter = 100000\n",
    "tokens_in_contract = 5000\n",
    "tokens_in_email = 200\n",
    "\n",
    "model_context_limit = 128000 \n",
    "\n",
    "# Create simple bar chart\n",
    "labels = ['GPT-4 Memory Limit', 'Harry Potter Book', 'Business Contract']\n",
    "values = [model_context_limit, tokens_in_harry_potter, tokens_in_contract]\n",
    "colors = ['lightgray', 'red', 'blue']\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "bars = plt.barh(labels, values, color=colors)\n",
    "\n",
    "# Add line for limit\n",
    "plt.axvline(x=model_context_limit, color='black', linestyle='--', label='Context Limit')\n",
    "\n",
    "plt.title(\"Visualizing the 'Memory Budget' (Context Window)\")\n",
    "plt.xlabel(\"Tokens\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Can GPT-4 read one Harry Potter book? {'Yes' if tokens_in_harry_potter < model_context_limit else 'No'}\")\n",
    "print(f\"Can it read TWO Harry Potter books at once? {'Yes' if tokens_in_harry_potter * 2 < model_context_limit else 'No (It would forget the beginning)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b3ac67",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb7b008",
   "metadata": {},
   "source": [
    "# The \"Big Three\" Model Families (2024-2025)\n",
    "\n",
    "| Model Family | The Personality | Best Use Case | Weakness |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **GPT-4o (OpenAI)** | **The Jack of All Trades** | ‚Ä¢ Logic & Reasoning<br>‚Ä¢ Complex Instruction Following<br>‚Ä¢ Data Analysis | Can be \"lazy\" or overly verbose. Tone often feels robotic. |\n",
    "| **Claude 3.5 (Anthropic)** | **The Writer** | ‚Ä¢ **Humanities & Writing**<br>‚Ä¢ Nuanced summarization<br>‚Ä¢ Coding & Web Dev | Stricter safety refusals (\"I can't do that\"). |\n",
    "| **Gemini 1.5 (Google)** | **The Analyst** | ‚Ä¢ **Massive Context** (Reading 10 books at once)<br>‚Ä¢ Video/Audio processing<br>‚Ä¢ Google Workspace integration | Sometimes prone to \"sycophancy\" (agreeing with you even if you are wrong). |\n",
    "| **Llama 3 (Meta)** | **The Open Option** | ‚Ä¢ Privacy (can run locally)<br>‚Ä¢ Cost-efficiency<br>‚Ä¢ No subscription required (if local) | Requires hardware setup (unless used via cloud). |\n",
    "\n",
    "### Summary Recommendation\n",
    "* **Writing an Essay?** Use **Claude**.\n",
    "* **Analyzing 50 PDFs?** Use **Gemini**.\n",
    "* **Solving a Logic Puzzle?** Use **GPT-4o**."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
