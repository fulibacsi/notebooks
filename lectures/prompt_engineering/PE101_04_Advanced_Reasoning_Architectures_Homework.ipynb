{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "985bbd2b",
   "metadata": {},
   "source": [
    "# Prompt Engineering 101 - Part IV.\n",
    "## Advanced Reasoning Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dc07bd",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb13587e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title üõ†Ô∏è Step 1: Laboratory Setup (Gemini API with Recursive Wrapper)\n",
    "# We are connecting to Google's \"Gemini 2.5 Flash\" model.\n",
    "\n",
    "# 1. Install the Google AI SDK\n",
    "!pip install -q -U google-genai\n",
    "\n",
    "from google.colab import userdata\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# 2. Configure the API Key\n",
    "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
    "\n",
    "# 3. Create a Robust Wrapper Class\n",
    "class GeminiModel:\n",
    "    def __init__(self, api_key, preferred_model='gemini-2.5-flash'):\n",
    "        self.client = genai.Client(api_key=api_key)\n",
    "        \n",
    "        # Priority list of models and their availability status\n",
    "        # True = Available, False = Exhausted (429)\n",
    "        self.models = {\n",
    "            'gemini-2.5-flash': True,\n",
    "            'gemini-2.5-flash-lite': True,\n",
    "            'gemini-3-flash-preview': True,\n",
    "        }\n",
    "        \n",
    "        # Validation: Ensure the user's choice is valid\n",
    "        if preferred_model not in self.models:\n",
    "            raise ValueError(f\"Invalid model '{preferred_model}'. \"\n",
    "                             f\"Valid options: {list(self.models.keys())}\")\n",
    "            \n",
    "        self.current_model = preferred_model\n",
    "\n",
    "    def _get_next_available_model(self):\n",
    "        \"\"\"Sets the first model from the non-exhausted models as the currently selected model. \n",
    "        Raises error if no available model left.\"\"\"\n",
    "        for model_name, is_available in self.models.items():\n",
    "            if is_available:\n",
    "                print(f\"üîÑ Switching to fallback: {model_name}\")\n",
    "                self.current_model = model_name\n",
    "                return \n",
    "    \n",
    "        raise RuntimeError(\"‚ùå All available models are currently exhausted. \"\n",
    "                           \"Please wait for quotas to reset.\")\n",
    "\n",
    "    def generate_content(self, contents, config=None):\n",
    "        \"\"\"\n",
    "        Recursively attempts to generate content.\n",
    "        If a model fails with a quota error, it marks it as unavailable \n",
    "        and retries with the next available model.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Attempt generation\n",
    "            response = self.client.models.generate_content(\n",
    "                model=self.current_model,\n",
    "                contents=contents,\n",
    "                config=config\n",
    "            )\n",
    "            return response\n",
    "\n",
    "        except Exception as e:\n",
    "            # Check for Rate Limit / Quota errors\n",
    "            if \"429\" in str(e) or \"ResourceExhausted\" in str(e):\n",
    "                print(f\"‚ö†Ô∏è Quota exceeded for {self.current_model}. Marking as unavailable...\")\n",
    "                \n",
    "                # Update State: Mark current as failed\n",
    "                self.models[self.current_model] = False\n",
    "                \n",
    "                # Switch to next available\n",
    "                self._get_next_available_model()\n",
    "            \n",
    "                # Recursive Step: Try again with the new model\n",
    "                return self.generate_content(contents, config)\n",
    "            \n",
    "            # If it's a logic error (e.g. invalid prompt), raise immediately\n",
    "            raise e\n",
    "\n",
    "# 4. Initialize\n",
    "try:\n",
    "    model = GeminiModel(GEMINI_API_KEY, preferred_model='gemini-2.5-flash')\n",
    "    print(f\"‚úÖ Connection Established. Primary: {model.current_model}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135eba0e",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c792f690",
   "metadata": {},
   "source": [
    "# üè† Homework: The \"CEO's Brain\"\n",
    "\n",
    "### The Scenario\n",
    "You are advising the CEO of a failing bookstore chain.\n",
    "They have 3 options:\n",
    "1.  Close all stores and go Online Only.\n",
    "2.  Turn stores into \"Coffee & Community\" hubs.\n",
    "3.  Sell the company to a larger retailer.\n",
    "\n",
    "### The Task\n",
    "Write a **Single Python Prompt** that uses a \"System of Thought\" to solve this.\n",
    "Your prompt must include:\n",
    "1.  **Persona:** Strategic Consultant.\n",
    "2.  **Tree of Thoughts:** Analyze all 3 options (Pros/Cons/Risks).\n",
    "3.  **Recursive Criticism:** Critique the initial analysis for bias.\n",
    "4.  **Final Recommendation:** A synthesized conclusion.\n",
    "\n",
    "### Submission\n",
    "Submit the Python code and the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4309ba80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title (Hidden) Answer Key\n",
    "advisor_prompt = \"\"\"\n",
    "ACT AS: Senior Strategy Consultant.\n",
    "SCENARIO: Failing Bookstore Chain. Options: (1) Online Only, (2) Coffee Hubs, (3) Sell.\n",
    "\n",
    "PROCESS:\n",
    "1. TREE OF THOUGHTS: Analyze each of the 3 options. List the Financial Upside, Brand Risk, and Execution Difficulty for each.\n",
    "2. CRITIQUE: Look at the analysis in Step 1. Are we being too optimistic about \"Coffee Hubs\"? Are we underestimating the cost of \"Online Only\"?\n",
    "3. SYNTHESIS: Based on the critique, select the single best path forward and explain why.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "szisz_ds_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
