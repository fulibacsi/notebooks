{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Data Science \n",
    "## Part XI. - Complex LLM Applications\n",
    "\n",
    "### Table of Contents\n",
    "1. Lord of the Rings Fact Checker with RAG \n",
    "2. Guess Who? with a LangChain SQL Agent\n",
    "3. Exercise: Trolley Dilemma, D&D Edition: Morality by Alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧙‍♂️ Lord of the Rings Fact Checker with RAG\n",
    "\n",
    "We’ll walk through building a simple **fact-checking assistant** that can answer questions about the *Lord of the Rings* books using semantic retrieval and an LLM. This demonstrates a Retrieval-Augmented Generation (RAG) pipeline with LangChain.\n",
    "\n",
    "\n",
    "### ⚙️ Installation Requirements\n",
    "\n",
    "You'll need the following Python packages:\n",
    "\n",
    "```bash\n",
    "pip install langchain-experimental langchainhub faiss-cpu sentence-transformers datasets huggingface_hub hf_xet tf-keras sqlite-utils\n",
    "```\n",
    "> Note: If you're using a GPU-enabled environment, you can optionally install `faiss-gpu` instead of `faiss-cpu`.\n",
    "\n",
    "### 🔄 Overview of the Workflow\n",
    "\n",
    "1. **Chunk** the documents using Hugging Face’s `SemanticChunker`\n",
    "2. **Embed** the semantic chunks with a SentenceTransformer model\n",
    "3. **Store** the embeddings in a FAISS in-memory vector index\n",
    "4. **Build a LangChain** pipeline:\n",
    "   - Step 1: Convert the user question into a clearer full sentence for better embedding retrieval\n",
    "   - Step 2: Use the sentence to **query** the FAISS index\n",
    "   - Step 3: Feed the **retrieved chunks + original question** to Gemini for answering\n",
    "\n",
    "\n",
    "### 🧩 Why is this useful?\n",
    "\n",
    "- Embedding the full book allows **semantic search**: the model can find relevant parts even if the query is phrased differently.\n",
    "- Turning the question into a clearer sentence improves retrieval performance.\n",
    "- Using a vector database makes retrieval fast and scalable.\n",
    "- The LLM can reason with the context retrieved from your documents—not just its training data.\n",
    "\n",
    "This is a classic example of **grounding** the model’s answers in external knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Logging in, Setting up tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import mlflow\n",
    "\n",
    "dotenv.load_dotenv(\".env\")\n",
    "\n",
    "mlflow.langchain.autolog()\n",
    "mlflow.set_tracking_uri('http://127.0.0.1:5000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the experiment name to track in MLFlow. This is optional but recommended.   \n",
    "If you don't set an experiment, then all runs will be tracked under the root experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"LOTR_Fact_Checker_RAG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧩 Step 1: Chunking the Book\n",
    "\n",
    "We use a semantic-aware chunking strategy to split the document while preserving meaning. This is especially important when embedding large passages like full books or long-form documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the LOTR books\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "url = 'https://archive.org/stream/tolkien-j.-the-lord-of-the-rings-harper-collins-ebooks-2010/Tolkien-J.-The-lord-of-the-rings-HarperCollins-ebooks-2010_djvu.txt'\n",
    "file_path = \"./data/lotr.txt\"\n",
    "\n",
    "print('Processing book LOTR.', end='')\n",
    "LOTR_book_html = requests.get(url).content\n",
    "print('.', end='')\n",
    "LOTR_book = BeautifulSoup(LOTR_book_html, \"html.parser\").getText() \n",
    "print(\"done.\")\n",
    "\n",
    "print(f\"Saving as {file_path}...\", end='')\n",
    "with open(file_path, \"w\") as f:\n",
    "    f.write(LOTR_book)\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Loading with Textloader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.text.TextLoader.html#textloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.text import TextLoader\n",
    "\n",
    "\n",
    "# Load a sample from the Lord of the Rings book\n",
    "loader = TextLoader(file_path)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Embedding with Google Gemini embedding](https://python.langchain.com/docs/integrations/text_embedding/google_generative_ai/) & [Chunking with semantic chunker](https://python.langchain.com/docs/how_to/semantic-chunker/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# You can try Google Gemini Embeddings if it is available:\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "\n",
    "# Use a Hugging Face tokenizer for semantic chunking as a fallback\n",
    "#embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "splitter = SemanticChunker(embeddings)\n",
    "\n",
    "chunks = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧠 Step 2: Embedding and Storing in FAISS\n",
    "\n",
    "We use Google Gemini's [TextEmbedding004](https://ai.google.dev/gemini-api/docs/models#text-embedding) or Hugging Face’s [MiniLM](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) model to compute embeddings. The `FAISS` vectorstore allows for efficient similarity-based retrieval during the question-answering phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Create FAISS index\n",
    "vectorstore = FAISS.from_documents(chunks, embedding=embeddings)\n",
    "\n",
    "# Set up retriever\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🤖 Step 3: Building the Chain\n",
    "\n",
    "The final chain uses three stages:\n",
    "- **Rewrite Stage**: Rephrases the user’s original question into a complete sentence.\n",
    "- **Retrieval Stage**: Embeds the rewritten query and retrieves the most relevant chunks.\n",
    "- **Answer Generation**: Combines the user’s original question with the retrieved context and generates an answer using the Gemini LLM.\n",
    "\n",
    "This design allows better control and debuggability at each stage. It also makes each component replaceable or upgradable in future iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableMap, RunnableLambda\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "# Step 0: Define LLM endpoint\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "\n",
    "\n",
    "# Step 1: Expand the query to a full sentence\n",
    "rewrite_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You help convert user questions into full, clear sentences.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "rewrite_chain = (\n",
    "    rewrite_prompt\n",
    "    | llm\n",
    "    | RunnableLambda(lambda x: x.content)\n",
    ")\n",
    "\n",
    "\n",
    "# Step 2: Use expanded query to retrieve relevant chunks\n",
    "retrieval_chain = (\n",
    "    RunnableMap({\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"expanded\": rewrite_chain,\n",
    "    }) \n",
    "    | RunnableMap({\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"context\": lambda x: retriever.invoke(x[\"expanded\"]),\n",
    "    })\n",
    ")\n",
    "\n",
    "\n",
    "# Step 3: Answer with context\n",
    "answer_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Use the context from the Lord of the Rings books to answer the user's question.\"),\n",
    "    (\"human\", \"Context:\\n{context}\\n\\nQuestion:\\n{question}\")\n",
    "])\n",
    "\n",
    "rag_chain = (\n",
    "    retrieval_chain\n",
    "    | RunnableMap({\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"context\": lambda x: \"\\n\\n\".join(doc.page_content for doc in x[\"context\"]),\n",
    "    }) \n",
    "    | answer_prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Executing queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query example\n",
    "question = \"Was Aragorn related to Elrond?\"\n",
    "response = rag_chain.invoke({\"question\": question})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example illustrates key LangChain concepts such as:\n",
    "- Custom chaining with `RunnableLambda` and `RunnableMap`\n",
    "- Composability and modularity\n",
    "- Use of external vectorstores and semantic retrievers\n",
    "- Applying LLMs at multiple stages of the chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧠 Guess Who? with a LangChain SQL Agent\n",
    "\n",
    "In this section, we’ll build a **game-like agent** where an LLM tries to **guess the character** you're thinking of from a small SQLite database.\n",
    "\n",
    "### 🎯 Objective\n",
    "\n",
    "The agent’s goal is to identify a character based on answers to yes/no questions. It does this by:\n",
    "1. Extracting history of prior interactions\n",
    "2. Generating SQL queries to narrow down candidates\n",
    "3. Using retrieved data to make an educated guess\n",
    "\n",
    "### 🧰 Tools Used\n",
    "\n",
    "- **SQLite**: Small local database with structured data\n",
    "- **LangChain SQLAgent**: Lets the LLM reason over SQL queries via tools\n",
    "- **Output parsers**: To ensure the final answer is clean and structured\n",
    "- **RunnableLambda**: For injecting custom transformation steps into the chain\n",
    "- **RunnablePassthrough**: For adding a new items to the input\n",
    "- **RunnableBranch**: To create a branch in the chain based on the output of a previous step\n",
    "\n",
    "### 🗃️ Our Database\n",
    "\n",
    "We’ll define three tables:\n",
    "- `students`: each person’s attributes like height, eye color, and favorite food\n",
    "- `tracks`: areas of specialization\n",
    "- `student_tracks`: a connection table (many-to-many)\n",
    "\n",
    "```sql\n",
    "\n",
    "-- students\n",
    "id | name       | height_cm | eye_color | hair_color | favorite_food\n",
    "1  | Alice      | 155       | blue      | black      | sushi\n",
    "2  | Bob        | 180       | brown     | blonde     | pizza\n",
    "...\n",
    "\n",
    "-- tracks\n",
    "id | name         | focus_area\n",
    "1  | Robotics     | Hardware\n",
    "2  | NLP          | Language\n",
    "\n",
    "-- student_tracks\n",
    "student_id | track_id\n",
    "1          | 1\n",
    "2          | 2\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📦 1. Install and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import mlflow\n",
    "\n",
    "dotenv.load_dotenv(\".env\")\n",
    "\n",
    "mlflow.langchain.autolog()\n",
    "mlflow.set_tracking_uri('http://127.0.0.1:5000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"Guess_Who_SQLAgent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from operator import itemgetter\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnableBranch, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "from langchain.agents import create_sql_agent\n",
    "from langchain_community.agent_toolkits import SQLDatabaseToolkit\n",
    "from langchain_community.utilities.sql_database import SQLDatabase\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🏗️ 2. Create a Local SQLite Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Create database and tables\n",
    "conn = sqlite3.connect(\"students.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "\n",
    "# Create tables\n",
    "cursor.executescript(\"\"\"\n",
    "DROP TABLE IF EXISTS students;\n",
    "DROP TABLE IF EXISTS tracks;\n",
    "DROP TABLE IF EXISTS student_tracks;\n",
    "\n",
    "CREATE TABLE students (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    name TEXT,\n",
    "    height_cm INTEGER,\n",
    "    eye_color TEXT,\n",
    "    hair_color TEXT,\n",
    "    favorite_food TEXT\n",
    ");\n",
    "\n",
    "CREATE TABLE tracks (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    name TEXT,\n",
    "    focus_area TEXT\n",
    ");\n",
    "\n",
    "CREATE TABLE student_tracks (\n",
    "    student_id INTEGER,\n",
    "    track_id INTEGER,\n",
    "    FOREIGN KEY(student_id) REFERENCES students(id),\n",
    "    FOREIGN KEY(track_id) REFERENCES tracks(id)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# Insert data\n",
    "cursor.executescript(\"\"\"\n",
    "INSERT INTO students VALUES (1, 'Alice', 155, 'blue', 'black', 'sushi');\n",
    "INSERT INTO students VALUES (2, 'Bob', 180, 'brown', 'blonde', 'pizza');\n",
    "INSERT INTO students VALUES (3, 'Charlie', 165, 'green', 'brown', 'pasta');\n",
    "\n",
    "INSERT INTO tracks VALUES (1, 'Robotics', 'Hardware');\n",
    "INSERT INTO tracks VALUES (2, 'NLP', 'Language');\n",
    "\n",
    "INSERT INTO student_tracks VALUES (1, 1);\n",
    "INSERT INTO student_tracks VALUES (2, 2);\n",
    "INSERT INTO student_tracks VALUES (3, 2);\n",
    "\"\"\")\n",
    "\n",
    "# Execute\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🤖 3. Set Up the SQL Agent with Gemini \n",
    "#### (with a local deepseek model as fallback option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.0)\n",
    "# llm = ChatOllama(model=\"codellama:latest\", temperature=0.0)\n",
    "db = SQLDatabase.from_uri(\"sqlite:///students.db\")\n",
    "\n",
    "toolkit = SQLDatabaseToolkit(db=db, llm=llm)\n",
    "agent = create_sql_agent(llm=llm, toolkit=toolkit, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🤖 4. A Smarter Agent: Handling History, Decisions, and Exit Conditions\n",
    "\n",
    "Our updated version of the agent is significantly more capable. In addition to asking and answering SQL-based questions, it now includes:\n",
    "\n",
    "#### ✅ Enhanced Prompt Logic and Decision Making\n",
    "\n",
    "The chain starts by building a **conversation history**, which allows the agent to:\n",
    "- Recall what questions the user already asked\n",
    "- Avoid repeating queries\n",
    "- Use prior responses to guide new decisions\n",
    "\n",
    "To achieve this, we use a custom `format_question_with_history()` function that builds a full interaction context using:\n",
    "- A `previous_qa` list, storing each (question, answer) pair\n",
    "- A formatted string combining this history and the current question\n",
    "- A carefully crafted prompt instructing the agent to:\n",
    "  - Ask questions that narrow the possible candidates\n",
    "  - Avoid redundant SQL queries (e.g., if it has just asked a question, it shouldn't try to answer it immediately)\n",
    "  - Guess the identity when there's only one candidate left\n",
    "  - Gracefully exit if no matches are possible or the user confirms the guess\n",
    "\n",
    "#### 🔀 Branching Execution with `RunnableBranch`\n",
    "\n",
    "To decide whether to **continue** playing or **end** the session, we introduce a **reasoning LLM sub-chain**:\n",
    "- A separate `early_exit_chain` evaluates whether the agent should proceed or not\n",
    "- The decision is parsed and attached to the output as a `\"decision\"` key\n",
    "- The final chain uses `RunnableBranch` to:\n",
    "  - Route `\"continue\"` cases into the agent execution path\n",
    "  - Return a thank-you message if the decision is `\"stop\"`\n",
    "\n",
    "This makes the agent more dynamic and user-friendly:\n",
    "- It doesn’t just mindlessly execute SQL queries\n",
    "- It knows when to stop, either because it made a confident guess or because no candidates remain\n",
    "- It waits for user confirmation before finalizing a match\n",
    "\n",
    "#### 🛠️ Modular and Extensible Design\n",
    "\n",
    "Because the chain is broken into:\n",
    "- Formatting\n",
    "- Reasoning\n",
    "- Querying\n",
    "- Output handling\n",
    "\n",
    "…you can easily modify or extend each part. For example:\n",
    "- Replace the LLM used for decision-making\n",
    "- Add memory or external logging\n",
    "- Customize prompts for different personalities or styles\n",
    "\n",
    "This advanced version simulates **stateful interaction**, **tool use**, and **self-reflection**, all critical steps in designing intelligent assistants. It's a strong pattern for production-ready LLM chains that need to maintain coherence across turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_qa = []\n",
    "\n",
    "\n",
    "def format_question_with_history(new_question):\n",
    "    history = (\"\\n\" + \"\\n\".join(f\" - User: {q}\\n - Agent: {a}\" for q, a in previous_qa)) if len(previous_qa) else \"\"\n",
    "    return (\n",
    "        f\"You are playing a game of 'Guess Who?' but this time the candidates are students from a database. \"\n",
    "        f\"\\nYour task is to figure out which student the user is thinking of:\"\n",
    "        f\"\\n- Ask relevant questions to shortlist the possible candidates.\"\n",
    "        f\"\\n- If you come up with a question do not query the database again, use that question as your final answer.\"\n",
    "        f\"\\n- If you know who the student is, make your guess (eg. your filtered query returned a single student).\"\n",
    "        f\"\\n- If you think that there is no possible candidate left, say so.\"\n",
    "        f\"\\n- If you found out who the student was, and the user has confirmed, thank them for the game and stop playing.\"\n",
    "        f\"\\n\"\n",
    "        f\"\\nHere is the conversation so far:\"\n",
    "        f\"{history}\"\n",
    "        f\"\\n - User: {new_question}\"\n",
    "        f\"\\n\"\n",
    "        f\"\\nNow try asking relevant questions. \"\n",
    "        f\"\\n\"\n",
    "        \n",
    "    )\n",
    "\n",
    "\n",
    "reasoning_prompt = PromptTemplate.from_template(\n",
    "    \"Based on the input, decide if you should continue or stop playing.\"\n",
    "    '\\nIf you would continue playing, output the following message: \"continue\".'\n",
    "    '\\nIf you think, that you should stop playing, output the following message: \"stop\".'\n",
    "    '\\nWhen outputting the result, do not include anything but the string. Do not add backticks or markdown formatting around the text.'\n",
    "    \"\\n\"\n",
    "    \"\\nHere is the input:\"\n",
    "    \"\\n{question}\"\n",
    ")\n",
    "\n",
    "\n",
    "agent_chain = (\n",
    "    agent\n",
    "    | RunnableLambda(lambda response: response['output'])\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "early_exit_chain = (\n",
    "    reasoning_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"question\": RunnableLambda(format_question_with_history)\n",
    "    }\n",
    "    | RunnablePassthrough.assign(decision=early_exit_chain)\n",
    "    | RunnableBranch(\n",
    "        (lambda result: result['decision'] == 'continue', itemgetter('question') | agent_chain),\n",
    "        lambda result: \"Thank you for playing!\"\n",
    "    )\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎮 5. Play the Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Let's play a new round! Who is the student I’m thinking of?\"\n",
    "response = chain.invoke(question)\n",
    "previous_qa.append((question, response))\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Yes, the student has blue eyes.\"\n",
    "response = chain.invoke(question)\n",
    "previous_qa.append((question, response))\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Yes, the student has black hair.\"\n",
    "response = chain.invoke(question)\n",
    "previous_qa.append((question, response))\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Yes, I was thinking Alice.\"\n",
    "response = chain.invoke(question)\n",
    "previous_qa.append((question, response))\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exercise: ⚖️ Trolley Dilemma, D&D Edition: Morality by Alignment\n",
    "\n",
    "In this challenge, you’ll build a custom LangChain app that simulates moral judgments on classic trolley problem scenarios — but with a twist.\n",
    "\n",
    "Your judge isn't just any LLM — it’s one role-playing a **Dungeons & Dragons moral alignment**, such as *Lawful Good*, *Chaotic Neutral*, or *Neutral Evil*. Each alignment will process the same moral dilemma differently, applying wildly different ethics, logic, and values.\n",
    "\n",
    "### 🔍 Goal\n",
    "\n",
    "You will:\n",
    "\n",
    "- Collect the entities on the different tracks\n",
    "- Allow user to select a D&D alignment (or pick one randomly)\n",
    "- Prompt the LLM to generate a short, ridiculous reason why **each** track should be spared\n",
    "- If both justifications are equally absurd or equally convincing (based on a second LLM call or logic), flip a coin.\n",
    "- Parse the response to extract the **verdict** and **rationale**\n",
    "- Generate a dramatic verdict monologue like a courtroom closing argument.\n",
    "\n",
    "\n",
    "### 🧠 Chain Design\n",
    "\n",
    "1. LLM Call 1 – **Absurd Justification Generator**:\n",
    "Prompt the LLM to generate a short, ridiculous reason why each track should be spared.\n",
    "\n",
    "2. Custom Logic – **Decide or flip a coin if indecisive**\n",
    "    If both justifications are equally absurd or equally convincing (based on a second LLM call or logic), flip a coin.\n",
    "\n",
    "3. LLM Call 3 – **Final Verdict Generator**:\n",
    "Generate a dramatic verdict monologue like a courtroom closing argument.\n",
    "Output structure:\n",
    "```json\n",
    "{\n",
    "  \"alignment\": \"chaotic_neutral\",\n",
    "  \"track_spared\": \"track_1\",\n",
    "  \"justification\": \"The sentient toaster deserves a chance at fame.\",\n",
    "  \"final_statement\": \"In the great cosmic balance of toaster art and clone anxiety, the answer is clear.\"\n",
    "}\n",
    "```\n",
    "\n",
    "### 🧪 Example Scenario\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"track_1\": [\n",
    "    \"A sentient toaster that just wants to sing\",\n",
    "    \"Your childhood imaginary friend\",\n",
    "    \"A mime committing tax fraud\"\n",
    "  ],\n",
    "  \"track_2\": [\n",
    "    \"A talking dog who thinks he’s a lawyer\",\n",
    "    \"Elon Musk tweeting about Dogecoin\",\n",
    "    \"Three clones of you, but more charismatic\"\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "The Absurd Justification Generator could generate:\n",
    "- Track 1: The toaster has potential for a Grammy.\n",
    "- Track 2: The clones might replace the user and do a better job at work.\n",
    "\n",
    "Than, the final verdict could be:\n",
    "A **Lawful Good** judge might weigh lives saved.\n",
    "A **Chaotic Evil** judge might cheer for the chaos.\n",
    "A **True Neutral** judge might refuse to interfere.\n",
    "\n",
    "And the closing argument could be outputted:\n",
    "```json\n",
    "{\n",
    "  \"alignment\": \"Chaotic Neutral\",\n",
    "  \"track_spared\": \"track_1\",\n",
    "  \"justification\": \"The sentient toaster deserves a chance at fame.\",\n",
    "  \"final_statement\": \"In the great cosmic balance of toaster art and clone anxiety, the answer is clear.\"\n",
    "}\n",
    "```\n",
    "\n",
    "You’ll explore how prompt framing affects output, and how to model personality-driven logic flows.\n",
    "\n",
    "Let’s get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📦 Step 1: Prepare the environment\n",
    "\n",
    "Update the mlflow experiment, set it to `\"DND_Trolley_Dilemma_chain\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔧 Step 2: Define the User Inputs\n",
    "\n",
    "Ask the user for two things:\n",
    "1. A trolley-style moral scenario with two tracks\n",
    "2. A D&D alignment (like \"Lawful Good\", \"Chaotic Neutral\", etc.)\n",
    "\n",
    "Example:\n",
    "```python\n",
    "scenario = {\n",
    "    \"track_1\": [\n",
    "        \"A sentient toaster that just wants to sing\",\n",
    "        \"Your childhood imaginary friend\",\n",
    "        \"A mime committing tax fraud\"\n",
    "    ],\n",
    "    \"track_2\": [\n",
    "        \"A talking dog who thinks he’s a lawyer\",\n",
    "        \"Elon Musk tweeting about Dogecoin\",\n",
    "        \"Three clones of you, but more charismatic\"\n",
    "    ]\n",
    "}\n",
    "alignment = \"Chaotic Neutral\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧠 Step 3: Generate the Absurd Justification Generator chain\n",
    "\n",
    "- Prompt the LLM to generate a short, ridiculous reason why each track should be spared a justification for each track. Use a `PromptTemplate`.\n",
    "- Call the LLM\n",
    "- Parse the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: 🪵 Make a decision, or flip the coin\n",
    "\n",
    "- Create a prompt that judges the justifications for each track. Make sure to make it roleplay according the inputted alignment. Allow it to be indecisive. (eg. three possible judgement: track1, track2, indecisive)\n",
    "- Call the LLM\n",
    "- If indicisive, pick a random track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⚖️ Step 5: Final Verdict Generator \n",
    "\n",
    "- Write a prompt that takes in the output of the previous step and generates a final verdict in the pre-determined format.\n",
    "- Call the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6.\n",
    "\n",
    "Put it all together into a single chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧪 Step 5: Run the Chain\n",
    "\n",
    "Execute the full flow using the user’s scenario and alignment.\n",
    "\n",
    "```python\n",
    "response = chain.invoke({\n",
    "    \"track_1\": scenario['track_1'],\n",
    "    \"track_2\": scenario['track_2'],\n",
    "    \"alignment\": alignment\n",
    "})\n",
    "\n",
    "print(\"🧑‍⚖️ Moral Judge's Decision:\\n\")\n",
    "print(response.content)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "szisz_ds_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
