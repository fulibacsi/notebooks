{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Data Science\n",
    "## Part V. - Text Mining\n",
    "\n",
    "### Table of contents\n",
    "- #### Text Mining\n",
    "    - <a href=\"#What-is-Text-Mining?\">Theory</a>\n",
    "    - <a href=\"#Text-Mining-in-practice\">In practice</a>\n",
    "    - <a href=\"#a)-Bag-of-words-representation\">Vectorizing documents</a>\n",
    "    - <a href=\"#b)-Tf-Idf\">Normalizing document vectors</a>\n",
    "    - <a href=\"#c)-Hashing\">Vectorizing large corpus</a>\n",
    "    - <a href=\"#3.-Latent-Semantic-Indexing\">Topic modelling</a>\n",
    "    - <a href=\"#4.-Document-similarity-metrics\">Document similarity</a>\n",
    "- #### ANN\n",
    "    - <a href=\"#Neural-Networks\">Single layer networks</a>\n",
    "    - <a href=\"#Solving-non-linear-problems\">Multi layer networks</a>\n",
    "    \n",
    "\n",
    "## What is Text Mining?\n",
    "Text mining or text analytics is the process of extracting quantified features from (un)structured (natural language) texts. Processing unstructured data involves using Natural Language Processing (NLP), statistical modeling and machine learning techniques.\n",
    "\n",
    "## Why is it important?\n",
    "80% of the generated data is not available in structured, numerical format (emails, texts, meeting notes, documents, social media feeds). These unstructured data includes images, drawings, videos, voice recordings and unstructured texts. These data can be described with their meta data (length, topic, category, etc.) but transforming them into structured data is important to access the more detailed information stored in such data sources. Voice recordings, videos and drawings can also be transcribed into unstructured texts so they can be processing as textual data. \n",
    "Most common use cases are:\n",
    "\n",
    "- document similarity computation\n",
    "- document deduplication\n",
    "- document clustering\n",
    "- topic extraction\n",
    "- sentiment analysis\n",
    "- automated annotation\n",
    "- text filtering\n",
    "- text classification\n",
    "\n",
    "## Tools\n",
    "- NLP tools\n",
    "    - tokenization\n",
    "    - stemming\n",
    "    - part-of-speech (POS) tagging\n",
    "    - stop word filtering\n",
    "    - bag-of-words representation\n",
    "    - tf-idf transformation\n",
    "- Other tools\n",
    "    - Word2Vec representation\n",
    "    - hashing\n",
    "    - cosine/jaccard/levenshtein/etc similarity computation\n",
    "    - matrix factorization\n",
    "    - etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Mining in practice\n",
    "\n",
    "### 1. Read and examine data\n",
    "\n",
    "Examining unstructured data is the key to proper preprocessing.  \n",
    "The collection of texts is called __`corpus`__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/SMSSpamCollection', 'rb') as spamfile:\n",
    "    corpus = [line.decode('utf-8').strip() for line in spamfile]\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in corpus[:5]:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the data is in TSV format, read it accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_csv('./data/SMSSpamCollection', sep='\\t', names=['label', 'message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus['length'] = corpus.message.str.len()\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus['wordcount'] = corpus.message.str.split().str.len()\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.length.plot(bins=20, kind='hist');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.length.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "910 long sms???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.loc[corpus.length > 900, 'message'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there a difference between spam and ham messages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[['length', 'label']].hist(bins=50, by='label', sharex=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why not try a simple predictor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted = train_test_split(corpus.length.values[:, np.newaxis], # we need a matrix, not a vector\n",
    "                            corpus.label.values,\n",
    "                            test_size=.25,\n",
    "                            random_state=42)\n",
    "X_train, X_test, y_train, y_test = splitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('nb', MultinomialNB())])\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, pipe.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "87% percent is our baseline, let's get into the preprocessing!\n",
    "\n",
    "### 2. Preprocessing\n",
    "#### a) [Bag-of-words representation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer)\n",
    "\n",
    "Bag of words representation represent documents as a vector where each different word is represented in a fixed position. The values in the positions are the occurence counts in the given document. For example:\n",
    "The vector features for the documents:\n",
    "```python\n",
    "docs = [\"I like trains.\", \"Trains are like big cars.\", \"I like big cars\"]\n",
    "```\n",
    "will be \n",
    "```python\n",
    "features = {'I': 0, 'like': 1, 'trains': 2, 'are': 3, 'big': 4, 'cars': 5}\n",
    "```\n",
    "and the vectorial representations will be\n",
    "```python\n",
    "vectors = [[1, 1, 1, 0, 0, 0],\n",
    "           [0, 1, 1, 1, 1, 1],\n",
    "           [1, 1, 0, 0, 1, 1]]\n",
    "```\n",
    "\n",
    "Fortunately `scikit-learn` has a built-in solution for this: the [`CountVectorizer`](http://scikit-learn.org/stable/modules/feature_extraction.html#the-bag-of-words-representation).  \n",
    "Let's try out our little example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cntvec = CountVectorizer()\n",
    "docs = [\"I like trains.\",\n",
    "        \"Trains are like big cars.\",\n",
    "        \"I like big cars\"]\n",
    "\n",
    "cntvec.fit_transform(docs).todense(), cntvec.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-grams\n",
    "N-grams are n long word tuples. They are generated by an n long rolling window and they can provide contextual information which sometimes yields better results. An example 2-gram for the `\"I like trains.\"` sentence would be:\n",
    "```python\n",
    "[(\"I\", \"like\"), (\"like\", \"trains\")]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cntvec = CountVectorizer(ngram_range=(2, 2))\n",
    "cntvec.fit_transform(docs).todense(), cntvec.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimum and maximum document frequency\n",
    "\n",
    "Minimum and maximum document frequency (`min_df` and `max_df`) are set thresholds to limit the feature numbers. If a _term_ (transformed word) appears less than `min_df` or more than `max_df` times (or percent) then it will be left out from the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cntvec = CountVectorizer(max_df=1)\n",
    "cntvec.fit_transform(docs).todense(), cntvec.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cntvec = CountVectorizer(min_df=3)\n",
    "cntvec.fit_transform(docs).todense(), cntvec.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced tokenization\n",
    "\n",
    "In the vocabulary generation process each word are analyzed and transformed in order to reduce vocabulary length. Scikit-learn's default analization function is lowercasing the words and filtering short and stop words but no further transformation is done.\n",
    "\n",
    "NLP has more detailed techniques to better extract the base words. Lemmatization is a powerful tool to reduce a word into it's _root_ form (as it appears in dictionaries): that's how `are` becomes `be` and `trains` becomes `train`, etc.\n",
    "\n",
    "#### Stemming\n",
    "\n",
    "Word stemming means removing affixes from words and return the root word. Stemming do not use contextual information to execute the stripping.\n",
    "\n",
    "##### NLTK, the base NLP library\n",
    "There is an almost standard library called [`Natural Language ToolKit`](https://www.nltk.org/) for basic NLP tasks, like stemming.  \n",
    "To use:\n",
    "```bash\n",
    "conda install nltk\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmer.stem('trains')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "\n",
    "Word lemmatizing is similar to stemming, but the difference is the result of lemmatizing is a real word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('are', pos='v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization has better accuracy but slower than stemming.\n",
    "\n",
    "Using lemmatization we can create custom analyzers to use in CountVectorizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_lemmas(message):\n",
    "    message = ''.join([char for char in message.lower()\n",
    "                       if char.isalnum() or char.isspace()])\n",
    "    return [lemmatizer.lemmatize(word, pos='v') \n",
    "            for word in message.split()]\n",
    "\n",
    "[split_into_lemmas(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Textblob, an advanced NLP library\n",
    "\n",
    "The [__`textblob`__](https://textblob.readthedocs.io/en/dev/) library provides a more user friendly interface for lemmatization.  \n",
    "To use:\n",
    "```bash\n",
    "conda install -c conda-forge textblob\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_lemmas(message):\n",
    "    message = message.lower()\n",
    "    words = TextBlob(message).words\n",
    "    return [word.lemma for word in words]\n",
    "\n",
    "[split_into_lemmas(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cntvec = CountVectorizer(analyzer=split_into_lemmas)\n",
    "cntvec.fit_transform(docs).todense(), cntvec.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's insert our vectorizer to our pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted = train_test_split(corpus.message,\n",
    "                            corpus.label.values,\n",
    "                            test_size=.25,\n",
    "                            random_state=42)\n",
    "X_train, X_test, y_train, y_test = splitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('cntvec', CountVectorizer(analyzer=split_into_lemmas, min_df=10, max_df=.5)),\n",
    "                 ('nb', MultinomialNB())])\n",
    "pipe.fit(X_train, y_train)\n",
    "accuracy_score(y_test, pipe.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pipe['cntvec'].vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spacy, a more advanced NLP library\n",
    "\n",
    "There is an advanced library called [`spacy`](https://spacy.io/) which has more sophisticated tokenization and lemmatization.  \n",
    "To use (__requires admin rights!__):\n",
    "```bash\n",
    "conda install -c conda-forge spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "[token.lemma_ for token in nlp(docs[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame([\n",
    "    {'text': token.text, \n",
    "     'lemma': token.lemma_, \n",
    "     'POS': token.pos_, \n",
    "     'tag': token.tag_, \n",
    "     'dep': token.dep_,\n",
    "     'shape': token.shape_,\n",
    "     'is_alpha': token.is_alpha, \n",
    "     'is_stop': token.is_stop}\n",
    "    for token in nlp(docs[0])\n",
    "]).set_index('text').transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('cntvec', CountVectorizer(analyzer=lambda x: [w.lemma_ for w in nlp(x)], \n",
    "                                            min_df=10, max_df=.5)),\n",
    "                 ('nb', MultinomialNB())])\n",
    "pipe.fit(X_train, y_train)\n",
    "accuracy_score(y_test, pipe.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) [Tf-Idf](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "\n",
    "Tf-Idf is short for Term Frequency - Inverse Document Frequency and is a way of normalizing term counts. It is a product of two separate metrics:\n",
    "\n",
    "- _Term Frequency_ shows how often a word is appearing in a document. ${\\displaystyle \\mathrm {tf} (t,d)={\\frac {1}{2}} + {\\frac {f_{t,d}}{2 \\cdot \\max\\{f_{t',d}:t'\\in d\\}}}}$\n",
    "- _Inverse Document Frequency_ shows if a term is common or rare across all documents. $ \\mathrm {idf}(t, D) =  \\log \\frac{N}{|\\{d \\in D: t \\in d\\}|}$ where $N$ is the total number of documents in the corpus, $t$ is a term, $D$ is the set of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('cntvec', CountVectorizer(analyzer=split_into_lemmas, min_df=5, max_df=.9)),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('nb', MultinomialNB())])\n",
    "pipe.fit(X_train, y_train)\n",
    "accuracy_score(y_test, pipe.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort([5, 3, 7, 9, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in np.argsort(pipe['tfidf'].idf_)[-20:][::-1]:\n",
    "    print(word, pipe['cntvec'].get_feature_names_out()[word], pipe['tfidf'].idf_[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) [Hashing](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer)\n",
    "\n",
    "Really large corpora induce several problems with required memory: the larger the corpus, the larger the vocabulary will grow in memory.\n",
    "To avoid this issue a [_hashing trick_](http://scikit-learn.org/stable/modules/feature_extraction.html#feature-hashing) can be used. Basically instead of storing each different word in a dictionary it applies a hash function to the features to determine their column index in sample matrices directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('hash', HashingVectorizer(analyzer=split_into_lemmas, n_features=1000, alternate_sign=False)),\n",
    "                 ('nb', MultinomialNB())])\n",
    "pipe.fit(X_train, y_train)\n",
    "accuracy_score(y_test, pipe.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Latent Semantic Indexing\n",
    "\n",
    "_\"Latent semantic analysis (LSA) is a technique in natural language processing of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms. LSA assumes that words that are close in meaning will occur in similar pieces of text.\"_ from: [wiki](https://en.wikipedia.org/wiki/Latent_semantic_analysis)  \n",
    "LSA can be created by appling SVD to Tf-Idf vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('cntvec', CountVectorizer(analyzer=split_into_lemmas, stop_words='english')),\n",
    "                 ('tfidf', TfidfTransformer(sublinear_tf=True)),\n",
    "                 ('svd', TruncatedSVD(n_components=300, random_state=42)),\n",
    "                 ('svm', SVC(C=300))])\n",
    "pipe.fit(X_train, y_train)\n",
    "accuracy_score(y_test, pipe.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_names = pipe['cntvec'].get_feature_names_out()\n",
    "topics = pipe['svd'].components_\n",
    "topic_str = pipe['svd'].explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_important(topic, feat_names):\n",
    "    indeces = np.argsort(topic)[::-1]\n",
    "    terms = [feat_names[weightIndex] for weightIndex in indeces[:10]]    \n",
    "    weights = [topic[weightIndex] for weightIndex in indeces[:10]] \n",
    "    return dict(zip(terms, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(i, topic_str[i], get_most_important(topics[i], feat_names))\n",
    "    print('-' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Document similarity metrics\n",
    "\n",
    "The euclidean distance is not feasable to determine the likeness of documents. Instead the cosine similarity is used which is the angle between the document vectors.\n",
    "\n",
    "It is super convenient that the cosine similarity can be computed by the dot product between document tfidf vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_lemmas_and_filter(message):\n",
    "    lemmas = split_into_lemmas(message)\n",
    "    return [lemma for lemma in lemmas \n",
    "            if lemma not in ENGLISH_STOP_WORDS]\n",
    "    \n",
    "tfidf = TfidfVectorizer(analyzer=split_to_lemmas_and_filter,\n",
    "                        min_df=10,\n",
    "                        max_df=.5).fit(corpus.message)\n",
    "vects = tfidf.transform(corpus.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = tfidf.transform([corpus.message[0]])\n",
    "corpus.message[0], vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = vects.dot(vect.T).toarray().flatten()\n",
    "most_similar = np.argsort(sims)[-10:][::-1]\n",
    "\n",
    "for i, index in enumerate(most_similar):\n",
    "    print(i, sims[index])\n",
    "    print(corpus.message[index])\n",
    "    print('-' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model of the week:\n",
    "### Neural Networks\n",
    "\n",
    "<img src =\"pics/artificial_neuron.png\" width=\"300\" align=\"left\"/>\n",
    "\n",
    "Artificial Neural Networks are a supervised machine learning method for classification and regression purpose. It is based on the inner workings of the (human) brain. It consists of basic execution units and the connections between them.  \n",
    "\n",
    "The execution units are called __neurons__, and their job is to compute the weighted summation of their inputs, then appling an output function. Based on their simple nature a [neuron](https://en.wikipedia.org/wiki/Perceptron) is only capable of solving linear problems. Their mechanism is easily expressed by the following equation:\n",
    "$$y_{i} = f(\\sum_{i}w(t)_{i}x_{i})$$\n",
    "\n",
    "The learning process is simply updating the input weights: \n",
    "$${\\displaystyle w_{i}(t+1)=w_{i}(t)+(d_{j}-y_{j}(t))x_{j,i}\\,}$$\n",
    "where $d$ is the expected output for the $j$th input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XOR_X, XOR_y = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]), np.array([0, 1, 1, 0])\n",
    "df = pd.DataFrame(data=XOR_X, columns=['x', 'y'])\n",
    "df['label'] = XOR_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results_with_hyperplane(clf, clf_name, df, ax):\n",
    "    x_min, x_max = df.x.min() - .5, df.x.max() + .5\n",
    "    y_min, y_max = df.y.min() - .5, df.y.max() + .5\n",
    "\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, .02), np.arange(y_min, y_max, .02))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    ax.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired, shading='auto')\n",
    "    ax.scatter(df.x, df.y, c=df.label, edgecolors='k')\n",
    "    ax.set_title(clf_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron = Perceptron(verbose=2, random_state=42).fit(XOR_X, XOR_y)\n",
    "perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_results_with_hyperplane(perceptron, 'perceptron', df, ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "conf_mat = confusion_matrix(XOR_y, perceptron.predict(XOR_X))\n",
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solving non-linear problems\n",
    "\n",
    "As we can see, a single neuron is not able to solve this (not linear) problem. But they are not called __networks__ for nothing! The power of artificial neural networks lies in their topology. If we connect more of the __neurons__ we get a (real) neural network. The neurons are ordered into __layer__s. The first layer is the __input layer__ then there are zero, or more __hidden layer__(s), finally there is the __output layer__. Each layer can cosist any number of neurons. Based on the different topologies there are different ANN subtypes.\n",
    "\n",
    "<img src=\"pics/mlp.png\" width=350 align=\"left\"/>\n",
    "\n",
    "The most simple version of ANNs is the multi layer feed forward perceptron network (MLP). The tipical output (activation) functions are $y(v_i) = \\tanh(v_i) ~~ \\textrm{and} ~~ y(v_i) = (1+e^{-v_i})^{-1}$\n",
    "\n",
    "The weight updating algorithm is called [__Backpropagation__](https://en.wikipedia.org/wiki/Backpropagation). It basically propagates the errors back to their \"root\" neuron. So every neuron which was (even slightly) responsible for an error will get their weights updated accordingly. The main updating equations are easily expressed by appling [gradient descent rules](https://en.wikipedia.org/wiki/Backpropagation#Derivation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(random_state=42).fit(XOR_X, XOR_y)\n",
    "mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_results_with_hyperplane(mlp, 'mlp', df, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(XOR_y, mlp.predict(XOR_X))\n",
    "conf_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A super nice tutorial can be found [here](https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch12/ch12.ipynb), it is worth checking out."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "szisz_ds_23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "d0dc57cccaa2d0d305072673e9fe47ca44c23e744f2f05647d6b373471557b60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
