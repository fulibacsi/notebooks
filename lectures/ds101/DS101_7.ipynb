{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Data Science \n",
    "## Part VII. - Regression and Embedding pipelines\n",
    "\n",
    "### Table of contents\n",
    "\n",
    "- #### Regression\n",
    "    - <a href=\"#What-is-Regression?\">Theory</a>\n",
    "    - <a href=\"#Linear-regression---OLS\">Linear regression</a>\n",
    "    - <a href=\"#Ridge-regression\">Ridge regression</a>\n",
    "    - <a href=\"#LASSO\">LASSO regression</a>\n",
    "    - <a href=\"#Bayesian-Ridge-regression\">Bayesian regression</a>\n",
    "    - <a href=\"#Support-Vector-regression\">Support Vector regression</a>\n",
    "    - <a href=\"#XGBoost\">XGBoost</a>\n",
    "\n",
    "- #### Managing model lifecycle\n",
    "    - <a href=\"#Reusing-trained-pipelines\">Reusing trained pipelines</a>\n",
    "        - <a href=\"#Saving-pipelines\">Exporting pipelines</a>\n",
    "        - <a href=\"#Loading-pipelines\">Loading pipelines</a>\n",
    "    - <a href=\"#Tracking-sklearn-models\">Managing model lifecycle with MLFlow</a>\n",
    "        - <a href=\"#What-is-MLFlow?\">MLFlow Experiments</a>\n",
    "        - <a href=\"#Tracking-Experiments\">Tracking Experiments</a>\n",
    "        - <a href=\"#Loading-saved-models\">Saving and loading models</a>\n",
    "    - <a href=\"#Track-and-save-regression-models\">Track multiple experiments</a>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Regression?\n",
    "Regression is a type of supervised machine learning where the goal is to predict a continuous target variable based on one or more input features. Unlike classification, which assigns discrete labels, regression models estimate numeric values.\n",
    "\n",
    "It is also _\"a statistical process for estimating the relationships among variables. It includes many techniques for modeling and analyzing several variables, when the focus is on the relationship between a __dependent variable__ and one or more __independent variable__s (or 'predictors').\"_ — from: <a href=\"https://en.wikipedia.org/wiki/Regression_analysis\">Wiki</a>.\n",
    "\n",
    "While traditional statistical regression focuses on understanding relationships between variables, in Data Science, regression is primarily used for predictive modeling.\n",
    "\n",
    "## Why is it important?\n",
    "Regression is one of the most fundamental techniques in machine learning, widely used for predicting continuous values based on past data. It serves as a baseline for more complex models and helps uncover relationships between variables.  \n",
    "\n",
    "_\"Regression analysis is widely used for prediction and forecasting, where its use has substantial overlap with the field of machine learning.\"_ — from: <a href=\"https://en.wikipedia.org/wiki/Regression_analysis\">Wiki</a>.  \n",
    "\n",
    "Common applications include:\n",
    "- **Stock market forecasting** – Predicting future stock prices based on historical trends.\n",
    "- **Salary prediction** – Estimating salaries based on education, experience, and industry.\n",
    "- **Network traffic analysis** – Predicting bandwidth usage over time.\n",
    "- **Traffic flow estimation** – Forecasting vehicle congestion based on historical traffic data.\n",
    "\n",
    "## Tools\n",
    "Various regression techniques exist, each suited for different types of data and problem complexity:\n",
    "- **Linear regression** – Fits a straight-line relationship between variables.\n",
    "- **Ridge regression** – Adds regularization to prevent overfitting in high-dimensional data.\n",
    "- **LASSO regression** – Performs feature selection by shrinking coefficients of less important variables to zero.\n",
    "- **Bayesian regression** – Incorporates prior knowledge into the regression model.\n",
    "- **Support Vector Regression (SVR)** – Captures non-linear relationships using kernel functions.\n",
    "- **XGBoost** – A gradient-boosting technique that excels in structured data prediction.\n",
    "\n",
    "Each method has strengths and limitations, and choosing the right one depends on the dataset and problem requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score\n",
    "\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pred(y, predicted):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(y, predicted, edgecolors='k')\n",
    "    ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\n",
    "    ax.set_xlabel('Measured')\n",
    "    ax.set_ylabel('Predicted')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def plot_california(ax):\n",
    "    ax.scatter(medinc_train, y_train, edgecolors='k', s=10)\n",
    "    ax.set_xlabel(\"Median income of the building\")\n",
    "    ax.set_ylabel(\"Median value of owner-occupied homes in $1000's\")\n",
    "    ax.set_xlim([0, medinc_train.max()])\n",
    "    ax.set_ylim([0, y_train.max() * 1.5])\n",
    "    \n",
    "    \n",
    "def plot_curve(estimator, param, values, ax):   \n",
    "    for color, value in zip(colors, values):\n",
    "        estimator = estimator.set_params(**{param: value}).fit(medinc_train, y_train)\n",
    "        ax.plot(curve_x, estimator.predict(curve_x), '-', c=color, lw=2, label=value)\n",
    "    plot_california(ax)\n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "    \n",
    "def show_score(model, X, y, cv=10, metric=None):\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring=metric)\n",
    "    return \"Accuracy: {:.2f} (+/- {:.2f})\".format(scores.mean(), scores.std() * 2)\n",
    "\n",
    "colors = ['g', 'r', 'y', 'c', 'm', 'b']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variations on a Theme\n",
    "\n",
    "The traditional linear regression problem is formulated as follows:\n",
    "$$ y_i = {x}_i {\\beta} $$\n",
    "for each observation $i$, or more compactly:\n",
    "$$ {y} = {X}{\\beta} $$\n",
    "where:\n",
    "- $ {X} $ is the matrix of observed input values (features),\n",
    "- $ {y} $ is the vector of observed outputs (target variable),\n",
    "- $ {\\beta} $ is the weight (coefficient) vector that we aim to estimate.  \n",
    "\n",
    "In **Ordinary Least Squares (OLS)** regression, we estimate ${\\beta}$ by minimizing a loss function, specifically the **sum of squared residuals (SSR)**:\n",
    "$$ \\mathrm{Cost}({\\beta}) = \\mathrm{SSR}({\\beta}) = \\sum _i (\\hat y_i - y_i)^{2}. $$\n",
    "\n",
    "However, standard OLS can suffer from **overfitting**, especially when dealing with high-dimensional data. To address this, various **regularized** regression methods modify the loss function by adding a penalty on the size of the coefficients. These include:\n",
    "\n",
    "- **Ridge regression** (L2 regularization), which adds the sum of the squared coefficients:\n",
    "  $$ \\mathrm{Cost}({\\beta}) = \\sum _i (\\hat y_i - y_i)^{2} + \\alpha \\sum _i \\beta _i^{2}. $$\n",
    "\n",
    "- **LASSO regression** (L1 regularization), which adds the sum of the absolute values of the coefficients:\n",
    "  $$ \\mathrm{Cost}({\\beta}) = \\sum _i (\\hat y_i - y_i)^{2} + \\alpha \\sum _i \\vert \\beta _i \\vert. $$\n",
    "\n",
    "### **Why does this matter?**\n",
    "This technique, known as <a href=\"https://en.wikipedia.org/wiki/Regularization_(mathematics)\">**regularization**</a>, helps prevent **overfitting**, which occurs when a model learns noise in the training data rather than generalizable patterns.  \n",
    "\n",
    "- **Ridge regression** prevents extreme coefficient values but does not force any coefficients to be exactly zero.\n",
    "- **LASSO regression** can shrink some coefficients to zero, effectively performing **feature selection** by removing less important variables.\n",
    "\n",
    "### **Example Dataset**  \n",
    "To illustrate regularization, we will use the <a href=\"https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset\">**California housing dataset**</a>, a modern alternative that provides median house values in California districts.  \n",
    "\n",
    "However, it’s worth discussing the <a href=\"https://scikit-learn.org/1.0/modules/generated/sklearn.datasets.load_boston.html\">**Boston housing dataset**</a>, which was widely used in the past but has been **deprecated in `scikit-learn` due to ethical concerns**. The dataset includes a feature representing the proportion of the population that is Black, which was used as a predictor for housing prices. This highlights a significant issue in data science: **historical biases in datasets can reinforce societal inequalities when used in predictive models**. Addressing bias in data is a critical part of responsible machine learning.  \n",
    "\n",
    "For more details on Ridge and LASSO regression, check out this <a href=\"https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/\">Analytics Vidhya tutorial</a>.  \n",
    "\n",
    "---\n",
    "\n",
    "## Loading the California housing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "X_trimmed, y_trimmed = X[y < 5.0], y[y < 5.0]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_trimmed, y_trimmed, random_state=42)\n",
    "\n",
    "medinc = X_trimmed[:, 0][:, np.newaxis]\n",
    "medinc_train = X_train[:, 0][:, np.newaxis]\n",
    "medinc_test = X_test[:, 0][:, np.newaxis]\n",
    "\n",
    "curve_x = np.linspace(0, 20, num=300)[..., np.newaxis]\n",
    "\n",
    "print(housing.DESCR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression - OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols = Pipeline([('poly', PolynomialFeatures()), \n",
    "                ('ols', LinearRegression())])\n",
    "parameters = {'poly__degree': range(1,16)}\n",
    "ols_grid = GridSearchCV(ols, \n",
    "                        parameters, \n",
    "                        cv=5,\n",
    "                        n_jobs=2, \n",
    "                        scoring='neg_mean_squared_error')\n",
    "ols_grid.fit(medinc_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_score(ols_grid.best_estimator_, medinc_test, y_test, metric='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = ols_grid.best_estimator_.predict(medinc_test)\n",
    "plot_pred(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot some example curve with different degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_curve(ols, 'poly__degree', [1, 2, 3, 5, 13], ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Pipeline([\n",
    "    ('scale', StandardScaler()),\n",
    "    ('poly', PolynomialFeatures(degree=5)), \n",
    "    ('ridge', Ridge())\n",
    "])\n",
    "params = {'ridge__alpha': np.logspace(-1, 13, 29)}\n",
    "ridge_grid = GridSearchCV(ridge, \n",
    "                          params, \n",
    "                          cv=5,\n",
    "                          n_jobs=2, \n",
    "                          scoring='neg_mean_squared_error')\n",
    "ridge_grid.fit(medinc_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Available scorers are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' -', '\\n - '.join(sklearn.metrics._scorer._SCORERS.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_score(ridge_grid.best_estimator_, medinc_test, y_test, metric='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = ridge_grid.best_estimator_.predict(medinc_test)\n",
    "plot_pred(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot some example curves to see how the regularization parameters \"deform\" the 5 degree polynomial we saw in the previous plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_curve(ridge, 'ridge__alpha', [1e-13, 1e-6, 1e-1, 1e0, 1e2], ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LASSO\n",
    "\n",
    "Least absolute shrinkage and selection operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lasso = Pipeline([\n",
    "    ('scale', StandardScaler()),\n",
    "    ('poly', PolynomialFeatures(degree=5)), \n",
    "    ('lasso', Lasso(max_iter=100_000))\n",
    "])\n",
    "params = {'lasso__alpha': np.logspace(-5, 13, 19)}\n",
    "lasso_grid = GridSearchCV(lasso, \n",
    "                          params, \n",
    "                          cv=5,\n",
    "                          scoring='neg_mean_squared_error')\n",
    "lasso_grid.fit(medinc_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_score(lasso_grid.best_estimator_, medinc_test, y_test, metric='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = lasso_grid.best_estimator_.predict(medinc_test)\n",
    "plot_pred(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LASSO also serves as a **feature selection** tool. By setting the alpha parameter high enough, it drives some coefficients to **exactly zero**, effectively removing less important features from the model. However, if alpha is set too high, too many features are eliminated, leading to **underfitting**, where the model becomes too simple to capture the underlying patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = pd.DataFrame()\n",
    "pipe = Pipeline([('poly', PolynomialFeatures(degree=5)),\n",
    "                 ('lasso', Lasso(max_iter=100_000))])\n",
    "\n",
    "for alpha in np.logspace(-5, 13, 19):\n",
    "    pipe = pipe.set_params(lasso__alpha=alpha).fit(medinc_train, y_train)\n",
    "    coefs[alpha] = pipe.named_steps['lasso'].coef_[1:]\n",
    "\n",
    "coefs.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_curve(lasso, 'lasso__alpha', [1e-5, 1e-2, 1e-1, 1e1, 1e8], ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Ridge Regression\n",
    "\n",
    "Bayesian Ridge Regression is similar to standard Ridge Regression, but instead of setting a fixed $\\lambda$ parameter for $\\ell_{2}$ regularization, it treats $\\lambda$ as a random variable and estimates it from the data using a Bayesian framework. This allows the model to incorporate **uncertainty** in the regularization strength, making it more adaptable to different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import BayesianRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes = Pipeline([('poly', PolynomialFeatures(degree=5)), \n",
    "                  ('bayes', BayesianRidge())])\n",
    "params = {'bayes__alpha_1': np.logspace(-5, 5, 5),\n",
    "          'bayes__alpha_2': np.logspace(-5, 13, 5),\n",
    "          'bayes__lambda_1': np.logspace(-5, 13, 5),\n",
    "          'bayes__lambda_2': np.logspace(-5, 13, 5)}\n",
    "bayes_grid = GridSearchCV(bayes, \n",
    "                          params,\n",
    "                          cv=5,\n",
    "                          scoring='neg_mean_squared_error')\n",
    "bayes_grid.fit(medinc_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_score(bayes_grid.best_estimator_, medinc_test, y_test, metric='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = bayes_grid.best_estimator_.predict(medinc_test)\n",
    "plot_pred(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_curve(bayes, 'bayes__alpha_1', [1e-5, 1e-2, 1e-1, 1e1, 1e2], ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Regression\n",
    "\n",
    "Support Vector Machines (SVMs) are commonly used for classification, but they can also be adapted for regression tasks. This approach, called **Support Vector Regression (SVR)**, maintains the core idea of SVMs: instead of minimizing the error directly, it attempts to fit a model that ignores deviations within a certain margin while still penalizing larger errors.\n",
    "\n",
    "The key steps in SVR are:  \n",
    "a) Identify a subset of **support vectors**, which are the most influential training points that define the regression function.  \n",
    "b) Fit a linear model that best captures the relationship between input and output while keeping deviations within a specified margin.  \n",
    "c) (For nonlinear relationships) Transform data points into a **higher-dimensional space** where a linear model can be fitted more effectively.  \n",
    "d) Instead of explicitly transforming the data (which can be computationally expensive), apply **kernel functions** to implicitly operate in the higher-dimensional space.  \n",
    "\n",
    "This technique allows SVR to model both **linear and nonlinear relationships** while being robust to outliers and generalizing well to unseen data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr = SVR(kernel='rbf', C=1e3, gamma=5e-5, degree=5)\n",
    "svr.fit(medinc_train, y_train)\n",
    "show_score(svr, medinc, y_trimmed, metric='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = svr.predict(medinc_test)\n",
    "plot_pred(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_curve(svr, 'kernel', ['linear', 'poly', 'rbf'], ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_curve(svr, 'degree', [2, 3, 4, 5], ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [XGBoost](https://xgboost.readthedocs.io/en/latest/model.html)\n",
    "\n",
    "XGBoost is short for **Extreme Gradient Boosting** which is a Gradient Boosted Tree method. Boosted tree is an **ensemble method**, basically training multiple trees on the same training set results a more robust solution. It is important that boosted trees incorporates a **regularization term** in its objective function. In this sense, boosted trees are the same as random forests. The difference comes from the training process. \n",
    "\n",
    "XGBoost use additive training: in each step it adds individual trees by selecting the best tree each time. The best tree is the **simplest tree** (tree structure score is minimal) **with the most information gain**.\n",
    "\n",
    "For more detailed explanation please consult with these [slides](https://web.njit.edu/~usman/courses/cs675_spring20/BoostedTree.pdf) and this [tutorial](https://xgboost.readthedocs.io/en/latest/tutorials/model.html) or with this [wiki page](https://en.wikipedia.org/wiki/Gradient_boosting) on gradient boosting. Install it using the `conda install py-xgboost` command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [XGBoost](https://xgboost.readthedocs.io/en/latest/model.html)\n",
    "\n",
    "XGBoost (**Extreme Gradient Boosting**) is an optimized gradient boosting algorithm designed for efficiency and scalability. It builds an ensemble of decision trees iteratively, correcting errors from previous trees while incorporating regularization to prevent overfitting.  \n",
    "\n",
    "### How XGBoost Works  \n",
    "\n",
    "1. **Initialize a weak model**  \n",
    "   The algorithm starts with an initial prediction (e.g., the mean of the target variable).  \n",
    "\n",
    "2. **Compute Residuals (Pseudo-Residuals)**  \n",
    "   Instead of directly fitting the target variable, XGBoost fits the residual errors (gradients) from the previous step:  \n",
    "   $$ r_i = y_i - \\hat{y}_i $$  \n",
    "   These residuals indicate how much improvement is needed in the next iteration.  \n",
    "\n",
    "3. **Train a New Decision Tree**  \n",
    "   A new tree is trained to predict these residuals. This tree learns to minimize the loss function by capturing patterns in the errors.  \n",
    "\n",
    "4. **Compute Leaf Weights**  \n",
    "   Each leaf of the tree gets a weight that determines how much it contributes to the final prediction. XGBoost optimizes these weights using a second-order Taylor approximation, making it faster and more stable.  \n",
    "\n",
    "5. **Update the Predictions**  \n",
    "   The new tree's output is scaled by a learning rate **(shrinkage factor, $\\eta$)** and added to the previous prediction:  \n",
    "   $$ \\hat{y}_i^{(t+1)} = \\hat{y}_i^{(t)} + \\eta f_t(x_i) $$  \n",
    "   This gradual update process helps prevent overfitting.  \n",
    "\n",
    "6. **Apply Regularization**  \n",
    "   XGBoost incorporates L1 (LASSO) and L2 (Ridge) regularization to penalize complex models and avoid overfitting:  \n",
    "   $$ \\text{Loss} = \\sum (y_i - \\hat{y}_i)^2 + \\lambda ||\\beta||^2 + \\alpha ||\\beta||_1 $$  \n",
    "\n",
    "7. **Repeat Until Convergence**  \n",
    "   Steps 2-6 are repeated, adding new trees iteratively until the stopping criteria (e.g., number of trees, early stopping) is met.  \n",
    "\n",
    "### Key Optimizations in XGBoost  \n",
    "\n",
    "- **Gradient and Hessian-based Optimization**: Uses both first and second derivatives of the loss function for more efficient updates.  \n",
    "- **Feature Importance**: Evaluates which features contribute most to predictions.  \n",
    "- **Handling Missing Values**: Automatically learns optimal splits for missing data.  \n",
    "- **Parallel Processing**: Splits computations across multiple cores for faster training.  \n",
    "- **Early Stopping**: Stops training when validation performance stops improving.  \n",
    "\n",
    "For further details, check the [XGBoost documentation](https://xgboost.readthedocs.io/en/latest/model.html).  \n",
    "\n",
    "### Key Advantages of XGBoost  \n",
    "\n",
    "- **Speed & Efficiency**: Optimized for parallel computation, handling large datasets efficiently.  \n",
    "- **Built-in Regularization**: Helps in avoiding overfitting, making it more generalizable.  \n",
    "- **Feature Importance**: Provides built-in methods to understand which features are the most influential.  \n",
    "- **Handles Missing Values**: Unlike traditional models, XGBoost can infer missing values intelligently.  \n",
    "- **Scalability**: Works well with large-scale datasets and supports distributed computing.  \n",
    "\n",
    "### Installation  \n",
    "To install XGBoost, you can use:  \n",
    "```bash\n",
    "conda activate szisz_ds_2025\n",
    "pip install xgboost\n",
    "```\n",
    "\n",
    "For a more detailed explanation, refer to these [slides](https://web.njit.edu/~usman/courses/cs675_spring20/BoostedTree.pdf), this [tutorial](https://xgboost.readthedocs.io/en/latest/tutorials/model.html), or this [wiki page](https://en.wikipedia.org/wiki/Gradient_boosting) on gradient boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost.sklearn import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBRegressor()\n",
    "xgb.fit(medinc_train, y_train)\n",
    "y_hat = xgb.predict(medinc_test)\n",
    "show_score(xgb, medinc, y_trimmed, metric='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pred(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_curve(xgb, 'n_estimators', [1, 5, 10, 25, 100], ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection & Engineering for Regression\n",
    "\n",
    "Effective feature selection and engineering are crucial for improving the performance and interpretability of regression models. Poorly chosen features can lead to **multicollinearity**, overfitting, or poor generalization, while well-engineered features can improve model accuracy and efficiency.\n",
    "\n",
    "### Handling Multicollinearity\n",
    "\n",
    "Multicollinearity occurs when two or more predictor variables are highly correlated, leading to unstable coefficient estimates in regression models. This can reduce the interpretability of the model and increase variance in coefficient estimates. To address multicollinearity:\n",
    "- **Variance Inflation Factor (VIF)**: Compute VIF for each feature; high values (typically >10) indicate severe multicollinearity.\n",
    "- **Principal Component Analysis (PCA)**: Transform correlated variables into uncorrelated principal components.\n",
    "- **Feature Selection**: Remove one of the correlated features if both convey similar information.\n",
    "\n",
    "### Interaction Terms\n",
    "\n",
    "Interaction terms capture relationships between two or more features that may have a combined effect on the target variable. Instead of treating each feature independently, we create new features by multiplying or combining them:\n",
    "- **Example**: In a real estate model, `House Size` and `Number of Bedrooms` may interact, affecting price differently than when considered separately.\n",
    "- **Polynomial Features**: Higher-order terms like $ x_1 x_2 $ or $ x_1^2 $ can capture nonlinear effects.\n",
    "\n",
    "### Feature Scaling\n",
    "\n",
    "Some regression models, particularly regularized methods like Ridge and LASSO, require proper scaling to ensure fair treatment of all features:\n",
    "- **Standardization (Z-score normalization)**: Centers data to mean 0 and scales to unit variance.\n",
    "- **Min-Max Scaling**: Rescales features to a fixed range, typically [0,1].\n",
    "- **Robust Scaling**: Uses median and interquartile range, reducing sensitivity to outliers.\n",
    "\n",
    "### Encoding Categorical Variables\n",
    "\n",
    "Regression models typically require numerical input, so categorical features must be converted into a numerical representation:\n",
    "- **One-Hot Encoding**: Converts categorical variables into binary (0/1) columns.\n",
    "- **Ordinal Encoding**: Assigns integer values to ordered categories.\n",
    "- **Target Encoding**: Replaces categories with their mean target value (useful but prone to leakage).\n",
    "\n",
    "Properly handling feature selection and engineering can significantly improve model performance and interpretability, making regression models more robust and generalizable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling Multicollinearity - Variance Inflation Factor (VIF)\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "def calculate_vif(X):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Feature\"] = housing.feature_names\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X_trimmed, i) for i in range(X.shape[1])]\n",
    "    return vif_data\n",
    "\n",
    "vif_df = calculate_vif(X_train)\n",
    "print(vif_df.sort_values(by=\"VIF\", ascending=False))  # High VIF indicates multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Interaction Terms\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_train_interaction = poly.fit_transform(X_train)\n",
    "X_test_interaction = poly.transform(X_test)\n",
    "\n",
    "print(\"Original feature count:\", X_train.shape[1])\n",
    "print(\"Feature count after adding interactions:\", X_train_interaction.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "## Managing Model Lifecycle\n",
    "\n",
    "### Reusing Trained Pipelines\n",
    "\n",
    "Trained pipelines can be used outside of the training environment, making it possible to deploy models, share them across teams, or resume training later.\n",
    "\n",
    "#### Saving Pipelines\n",
    "\n",
    "First, we need to **serialize** the model. This process saves the entire pipeline object into a file, allowing us to move and reload it elsewhere.  \n",
    "\n",
    "**Important:** The libraries and their versions used during saving and loading must be identical to ensure compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('xgboost_model.pickle', 'wb') as picklefile:\n",
    "    pickle.dump(xgb, picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Pipelines\n",
    "\n",
    "Loading and using a saved pipeline is straightforward, provided that the same libraries (with identical versions) are installed. This ensures compatibility and prevents potential errors due to differences in model serialization formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('xgboost_model.pickle', 'rb') as picklefile:\n",
    "    model = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_score(model, medinc, y_trimmed, metric='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking sklearn Models\n",
    "\n",
    "A common mistake, even among experienced professionals, is training models without properly tracking experiments. When multiple pipeline configurations, hyperparameters, and models are tested, it becomes difficult to remember which combination performed best. To avoid this, a tracking solution like MLflow can be used.\n",
    "\n",
    "#### What is <a href=\"https://mlflow.org/\">MLflow</a>?\n",
    "\n",
    "From its <a href=\"https://mlflow.org/docs/latest/index.html\">documentation</a>:  \n",
    "\n",
    "_\"MLflow is an open-source platform for managing the end-to-end machine learning lifecycle. It covers four key areas:\"_\n",
    "\n",
    "- _Tracking experiments to log and compare parameters and results (MLflow Tracking)._\n",
    "- _Packaging ML code in a reproducible format for collaboration and deployment (MLflow Projects)._\n",
    "- _Managing and serving models across different ML frameworks and environments (MLflow Models)._\n",
    "- _Providing a central repository for model versioning, stage transitions, and collaboration (MLflow Model Registry)._\n",
    "\n",
    "#### Tracking Experiments with MLflow\n",
    "\n",
    "To track your experiments with MLflow:\n",
    "\n",
    "1. Install MLflow:\n",
    "    ```bash\n",
    "    conda activate szisz_ds_2025\n",
    "    pip install mlflow\n",
    "    ```\n",
    "2. Start the MLflow tracking server:\n",
    "    ```bash\n",
    "    mlflow ui\n",
    "    ```\n",
    "3. Use the MLflow library to log and manage experiments.\n",
    "4. Open the tracking UI in your browser:\n",
    "    ```\n",
    "    http://localhost:5000\n",
    "    ```\n",
    "\n",
    "Once the tracking server is running, you can monitor and compare your experiments through the MLflow UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"xgboost-default\"):\n",
    "    xgb = XGBRegressor()\n",
    "    xgb.fit(medinc_train, y_train)\n",
    "    \n",
    "    # Log parameter values\n",
    "    for param, val in xgb.get_params().items():\n",
    "        mlflow.log_param(param, val)\n",
    "    \n",
    "    # Log metrics of the run\n",
    "    predictions = xgb.predict(medinc_test)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    rmse = mean_squared_error(y_test, predictions)\n",
    "    ev = explained_variance_score(y_test, predictions)\n",
    "    \n",
    "    mlflow.log_metric(\"r2\", r2)\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    mlflow.log_metric(\"ev\", ev)\n",
    "    \n",
    "    # Log pictures\n",
    "    fig, ax = plt.subplots()\n",
    "    plot_curve(xgb, 'n_estimators', [1, 5, 10, 25, 100], ax)\n",
    "    fig.savefig('xgboost_default_model_curve.png', transparent=True)\n",
    "    mlflow.log_artifact('xgboost_default_model_curve.png')\n",
    "    \n",
    "    # Log the model itself\n",
    "    mlflow.sklearn.log_model(xgb, \"xgboost_default_model\", input_example=medinc_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading saved models\n",
    "\n",
    "Exported models can be loaded later. You have to check the logged model details on the UI in order to get the model path:\n",
    "<img src=\"pics/mlflow_ui_model_details.png\" width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_loaded = mlflow.sklearn.load_model(\"path/from/the/mlflow/ui\")\n",
    "show_score(xgb_loaded, medinc, y_trimmed, metric='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track and save regression models\n",
    "\n",
    "Use the pipelines we built previously to:\n",
    "- track them using mlflow (kudos for using functions and/or loops)\n",
    "- compare the results on the mlflow UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "szisz_ds_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
