{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Data Science \n",
    "## Part VII. - Regression and Embedding pipelines\n",
    "\n",
    "### Table of contents\n",
    "\n",
    "- #### Regression\n",
    "    - <a href=\"#What-is-Regression?\">Theory</a>\n",
    "    - <a href=\"#Linear-regression---OLS\">Linear regression</a>\n",
    "    - <a href=\"#Ridge-regression\">Ridge regression</a>\n",
    "    - <a href=\"#LASSO\">LASSO regression</a>\n",
    "    - <a href=\"#Bayesian-Ridge-regression\">Bayesian regression</a>\n",
    "    - <a href=\"#Support-Vector-regression\">Support Vector regression</a>\n",
    "    - <a href=\"#XGBoost\">XGBoost</a>\n",
    "\n",
    "- #### Managing model lifecycle\n",
    "    - <a href=\"#Reusing-trained-pipelines\">Reusing trained pipelines</a>\n",
    "        - <a href=\"#Saving-pipelines\">Exporting pipelines</a>\n",
    "        - <a href=\"#Loading-pipelines\">Loading pipelines</a>\n",
    "    - <a href=\"#Tracking-sklearn-models\">Managing model lifecycle with MLFlow</a>\n",
    "        - <a href=\"#What-is-MLFlow?\">MLFlow Experiments</a>\n",
    "        - <a href=\"#Tracking-Experiments\">Tracking Experiments</a>\n",
    "        - <a href=\"#Loading-saved-models\">Saving and loading models</a>\n",
    "    - <a href=\"#Track-and-save-regression-models\">Track multiple experiments</a>\n",
    "\n",
    "---\n",
    "\n",
    "## What is Regression?\n",
    "Regression - just as classification - is a supervised machine learning problem however in case of regression the target variable is continuous. It is also _\"a statistical process for estimating the relationships among variables. It includes many techniques for modeling and analyzing several variables, when the focus is on the relationship between a __dependent variable__ and one or more __independent variable__s (or 'predictors').\"_ from: <a href=\"https://en.wikipedia.org/wiki/Regression_analysis\">Wiki</a>\n",
    "\n",
    "It is important to note that instead of the descriptive nature of statistical regression analysis Data Science focuses on the predictive side of this method.\n",
    "\n",
    "## Why is it important?\n",
    "_\"Regression analysis is widely used for prediction and forecasting, where its use has substantial overlap with the field of machine learning.\"_ from: <a href=\"https://en.wikipedia.org/wiki/Regression_analysis\">Wiki</a>\n",
    "\n",
    "It is used to forecast any continuous variable:\n",
    "- stock market\n",
    "- salary prediction\n",
    "- network traffic\n",
    "- traffic\n",
    "- etc.\n",
    "\n",
    "## Tools\n",
    "- Linear regression\n",
    "- Ridge regression\n",
    "- LASSO\n",
    "- Bayesian regression\n",
    "- Support Vector regression\n",
    "- etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score\n",
    "\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pred(y, predicted):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(y, predicted, edgecolors='k')\n",
    "    ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\n",
    "    ax.set_xlabel('Measured')\n",
    "    ax.set_ylabel('Predicted')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def plot_boston(ax):\n",
    "    ax.scatter(lsop_train, y_train, edgecolors='k', s=10)\n",
    "    ax.set_xlabel(\"% lower status of the population\")\n",
    "    ax.set_ylabel(\"Median value of owner-occupied homes in $1000's\")\n",
    "    ax.set_xlim([-10,50])\n",
    "    ax.set_ylim([0,60])\n",
    "    \n",
    "    \n",
    "def plot_curve(estimator, param, values, ax):   \n",
    "    for color, value in zip(colors, values):\n",
    "        estimator = estimator.set_params(**{param: value}).fit(lsop_train, y_train)\n",
    "        ax.plot(curve_x, estimator.predict(curve_x), '-', c=color, lw=2, label=value)\n",
    "    plot_boston(ax)\n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "    \n",
    "def show_score(model, X, y, cv=10, metric=None):\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring=metric)\n",
    "    return \"Accuracy: {:.2f} (+/- {:.2f})\".format(scores.mean(), scores.std() * 2)\n",
    "\n",
    "colors = ['g', 'r', 'y', 'c', 'm', 'b']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variations on a Theme\n",
    "\n",
    "The traditional linear problem is stated like this:\n",
    "$$ y_i = \\bs{x}_i \\bs{\\beta} $$\n",
    "for every observation $i$, or more compactly\n",
    "$$ \\bs{y} = \\bs{X}\\bs{\\beta} $$\n",
    "where $ \\bs{X} $ is the matrix observed values, $\\bs{y}$ is the vector of observed output variables, and $\\bs{\\beta}$ is the weight vector which we want to find. \n",
    "\n",
    "In OLS, we try to find the $\\bs{\\beta}$ while minimizing a *loss function*, which is simply the sum of squares of the differences between the predicted and observed values (also called sum of squared residuals or SSR), \n",
    "\n",
    "$ \\mathrm{Cost}(\\bs{\\beta}) = \\mathrm{SSR}(\\bs{\\beta}) = \\sum _i (\\hat y_i - y_i)^{2} $.  \n",
    "\n",
    "<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\">Ridge</a>, <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html\">LASSO</a> and <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.BayesianRidge.html\">Bayesian</a> regressions (and a couple more) are basically simple <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\">linear</a> regressions, but with the loss function being modified.  \n",
    "Ridge regression adds the sum of the squares of the weights with a constant multiplier to the loss, i.e.\n",
    "\n",
    "$ \\mathrm{Cost}(\\bs{\\beta}) = \\sum _i (\\hat y_i - y_i)^{2} + \\alpha \\sum _i \\beta _i^{2}. $\n",
    "\n",
    "LASSO adds the sum of the absolute values of the coefficients, i.e.\n",
    "\n",
    "$ \\mathrm{Cost}(\\bs{\\beta}) = \\sum _i (\\hat y_i - y_i)^{2} + \\alpha \\sum _i \\vert \\beta _i. \\vert $\n",
    "\n",
    "### Ok, but what is the point of this?\n",
    "\n",
    "This technique is called <a href=\"https://en.wikipedia.org/wiki/Regularization_(mathematics)\">**regularization**</a>, and the use of this in our case is to prevent the model from **overfitting** to the data (which is our greatest enemy, right before **the curse of dimensionality**). Basically it prevents the coefficients from growing too large. To illustrate this, we use the <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html\">*boston dataset*</a> - it has ethical issues built-in, but it is a good opportunity to discover how datasets includes prejudice. (You should also check out <a href=\"https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/\">this</a> for a more detailed discussion on Ridge and LASSO)\n",
    "\n",
    "---\n",
    "\n",
    "## Loading the boston dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "target = raw_df.values[1::2, 2]\n",
    "\n",
    "X, y = data, target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "lsop = X[:, 12][:, np.newaxis]\n",
    "lsop_train = X_train[:, 12][:, np.newaxis]\n",
    "lsop_test = X_test[:, 12][:, np.newaxis]\n",
    "\n",
    "curve_x = np.linspace(-10, 50, num=300)[..., np.newaxis]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression - OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols = Pipeline([('poly', PolynomialFeatures()), \n",
    "                ('ols', LinearRegression())])\n",
    "parameters = {'poly__degree': range(1,16)}\n",
    "ols_grid = GridSearchCV(ols, \n",
    "                        parameters, \n",
    "                        cv=5,\n",
    "                        n_jobs=2, \n",
    "                        scoring='neg_mean_squared_error')\n",
    "ols_grid.fit(lsop_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_score(ols_grid.best_estimator_, lsop_test, y_test, metric='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = ols_grid.best_estimator_.predict(lsop_test)\n",
    "plot_pred(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot some example curve with different degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_curve(ols, 'poly__degree', [1, 2, 3, 5, 13], ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Pipeline([\n",
    "    ('scale', StandardScaler()),\n",
    "    ('poly', PolynomialFeatures(degree=5)), \n",
    "    ('ridge', Ridge())\n",
    "])\n",
    "params = {'ridge__alpha': np.logspace(-15, 13, 29)}\n",
    "ridge_grid = GridSearchCV(ridge, \n",
    "                          params, \n",
    "                          cv=5,\n",
    "                          n_jobs=2, \n",
    "                          scoring='neg_mean_squared_error')\n",
    "ridge_grid.fit(lsop_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Available scorers are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' -', '\\n - '.join(key for key in sklearn.metrics.SCORERS.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_score(ridge_grid.best_estimator_, lsop_test, y_test, metric='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = ridge_grid.best_estimator_.predict(lsop_test)\n",
    "plot_pred(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot some example curves to see how the regularization parameters \"deform\" the 5 degree polynomial we saw in the previous plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_curve(ridge, 'ridge__alpha', [1e-13, 1e-6, 1e-1, 1e0, 1e2], ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LASSO\n",
    "\n",
    "Least absolute shrinkage and selection operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lasso = Pipeline([\n",
    "    ('scale', StandardScaler()),\n",
    "    ('poly', PolynomialFeatures(degree=5)), \n",
    "    ('lasso', Lasso(max_iter=100_000))\n",
    "])\n",
    "params = {'lasso__alpha': np.logspace(-5, 13, 19)}\n",
    "lasso_grid = GridSearchCV(lasso, \n",
    "                          params, \n",
    "                          cv=5,\n",
    "                          scoring='neg_mean_squared_error')\n",
    "lasso_grid.fit(lsop_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_score(lasso_grid.best_estimator_, lsop_test, y_test, metric='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = lasso_grid.best_estimator_.predict(lsop_test)\n",
    "plot_pred(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LASSO also works as a feature selection tool, we can see that by setting the alpha high enough, it sets some coefficients to zero. Also, we can see that if we go overboard with this, it can lead to **underfitting**, which is also bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = pd.DataFrame()\n",
    "pipe = Pipeline([('poly', PolynomialFeatures(degree=5)),\n",
    "                 ('lasso', Lasso(max_iter=100_000))])\n",
    "\n",
    "for alpha in np.logspace(-5, 13, 19):\n",
    "    pipe = pipe.set_params(lasso__alpha=alpha).fit(lsop_train, y_train)\n",
    "    coefs[alpha] = pipe.named_steps['lasso'].coef_[1:]\n",
    "\n",
    "coefs.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_curve(lasso, 'lasso__alpha', [1e-5, 1e-2, 1e-1, 1e1, 1e8], ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Ridge regression\n",
    "\n",
    "Bayesian Ridge Regression is really similar to the regular Ridge regression with a major difference: instead of setting an arbitrary $\\lambda$ parameter for the $\\ell_{2}$ regularization, the parameter is considered a variable and estimated from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import BayesianRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes = Pipeline([('poly', PolynomialFeatures(degree=5)), \n",
    "                  ('bayes', BayesianRidge())])\n",
    "params = {'bayes__alpha_1': np.logspace(-5, 5, 5),\n",
    "          'bayes__alpha_2': np.logspace(-5, 13, 5),\n",
    "          'bayes__lambda_1': np.logspace(-5, 13, 5),\n",
    "          'bayes__lambda_2': np.logspace(-5, 13, 5)}\n",
    "bayes_grid = GridSearchCV(bayes, \n",
    "                          params,\n",
    "                          cv=5,\n",
    "                          scoring='neg_mean_squared_error')\n",
    "bayes_grid.fit(lsop_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_score(bayes_grid.best_estimator_, lsop_test, y_test, metric='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = bayes_grid.best_estimator_.predict(lsop_test)\n",
    "plot_pred(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_curve(bayes, 'bayes__alpha_1', [1e-5, 1e-2, 1e-1, 1e1, 1e2], ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Regression\n",
    "\n",
    "Support vector machines can be used for regression purposes too. The main idea is to:\n",
    "a) reduce the number of required training points to the support vectors\n",
    "b) fit a linear model\n",
    "c) transform data points into higher dimensions and fit the linear model in that higher space then transform the fitted curve to the original, lower dimension\n",
    "d) instead of actually transforming the data, use kernel functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr = SVR(kernel='rbf', C=1e3, gamma=5e-5, degree=5)\n",
    "svr.fit(lsop_train, y_train)\n",
    "show_score(svr, lsop, y, metric='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = svr.predict(lsop_test)\n",
    "plot_pred(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_curve(svr, 'kernel', ['linear', 'poly', 'rbf'], ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_curve(svr, 'degree', [2, 3, 4, 5], ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [XGBoost](https://xgboost.readthedocs.io/en/latest/model.html)\n",
    "\n",
    "XGBoost is short for **Extreme Gradient Boosting** which is a Gradient Boosted Tree method. Boosted tree is an **ensemble method**, basically training multiple trees on the same training set results a more robust solution. It is important that boosted trees incorporates a **regularization term** in its objective function. In this sense, boosted trees are the same as random forests. The difference comes from the training process. \n",
    "\n",
    "XGBoost use additive training: in each step it adds individual trees by selecting the best tree each time. The best tree is the **simplest tree** (tree structure score is minimal) **with the most information gain**.\n",
    "\n",
    "For more detailed explanation please consult with these [slides](https://web.njit.edu/~usman/courses/cs675_spring20/BoostedTree.pdf) and this [tutorial](https://xgboost.readthedocs.io/en/latest/tutorials/model.html) or with this [wiki page](https://en.wikipedia.org/wiki/Gradient_boosting) on gradient boosting. Install it using the `conda install py-xgboost` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost.sklearn import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBRegressor()\n",
    "xgb.fit(lsop_train, y_train)\n",
    "y_hat = xgb.predict(lsop_test)\n",
    "show_score(xgb, lsop, y, metric='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pred(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_curve(xgb, 'n_estimators', [1, 5, 10, 25, 100], ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "## Managing model lifecycle\n",
    "\n",
    "### Reusing trained pipelines\n",
    "\n",
    "Trained pipelines can be used outside of the training program as well.\n",
    "\n",
    "#### Saving pipelines\n",
    "\n",
    "First, we have to `serialize` the models. This process will save the whole pipeline object into a file. After saving, we can freely move the file and read it in elsewhere.  \n",
    "**Important** to know that the used libraries must be the same versions in the saving and the loading end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('xgboost_model.pickle', 'wb') as picklefile:\n",
    "    pickle.dump(xgb, picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading pipelines\n",
    "\n",
    "Loading and using the models is pretty easy - as long as we have the same libraries installed (and the same versions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('xgboost_model.pickle', 'rb') as picklefile:\n",
    "    model = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_score(model, lsop, y, metric='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking sklearn models\n",
    "\n",
    "One of the typical errors even experienced professionals are exposed to is training models without tracking all of their experiments. Once several combination of pipeline items, parameters, models are tried it is hard to remember which gave the best performance. To avoid these mistakes, a tracking solution can be used.\n",
    "\n",
    "#### What is <a href=\"https://mlflow.org/\">MLFlow</a>?\n",
    "\n",
    "From it's <a href=\"https://mlflow.org/docs/latest/index.html\">documentation</a>:  \n",
    "\n",
    "_\"MLflow is an open source platform for managing the end-to-end machine learning lifecycle. It tackles four primary functions:_\n",
    "- _Tracking experiments to record and compare parameters and results (MLflow Tracking)._\n",
    "- _Packaging ML code in a reusable, reproducible form in order to share with other data scientists or transfer to production (MLflow Projects)._\n",
    "- _Managing and deploying models from a variety of ML libraries to a variety of model serving and inference platforms (MLflow Models)._\n",
    "- _Providing a central model store to collaboratively manage the full lifecycle of an MLflow Model, including model versioning, stage transitions, and annotations (MLflow Model Registry).\"_\n",
    "\n",
    "\n",
    "#### Tracking Experiments\n",
    "\n",
    "In order to track your experiment, you have to:\n",
    "- install the library with:\n",
    "    ```bash\n",
    "    pip install mlflow\n",
    "    ```\n",
    "- then start mlflow's tracking server with:\n",
    "    ```bash\n",
    "    mlflow ui\n",
    "    ```\n",
    "- and use the library to create and log your experiments\n",
    "- once the tracking server is running you can follow your experiments at:\n",
    "    ```\n",
    "    localhost:5000\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"xgboost-default\"):\n",
    "    xgb = XGBRegressor()\n",
    "    xgb.fit(lsop_train, y_train)\n",
    "    \n",
    "    # Log parameter values\n",
    "    for param, val in xgb.get_params().items():\n",
    "        mlflow.log_param(param, val)\n",
    "    \n",
    "    # Log metrics of the run\n",
    "    predictions = xgb.predict(lsop_test)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    rmse = mean_squared_error(y_test, predictions, squared=False)\n",
    "    ev = explained_variance_score(y_test, predictions)\n",
    "    \n",
    "    mlflow.log_metric(\"r2\", r2)\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    mlflow.log_metric(\"ev\", ev)\n",
    "    \n",
    "    # Log pictures\n",
    "    fig, ax = plt.subplots()\n",
    "    plot_curve(xgb, 'n_estimators', [1, 5, 10, 25, 100], ax)\n",
    "    fig.savefig('xgboost_default_model_curve.png', transparent=True)\n",
    "    mlflow.log_artifact('xgboost_default_model_curve.png')\n",
    "    \n",
    "    # Log the model itself\n",
    "    mlflow.sklearn.log_model(xgb, \"xgboost_default_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading saved models\n",
    "\n",
    "Exported models can be loaded later. You have to check the logged model details on the UI in order to get the model path:\n",
    "<img src=\"pics/mlflow_ui_model_details.png\" width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_loaded = mlflow.sklearn.load_model(\"path/to/model/from/the/ui\")\n",
    "show_score(xgb_loaded, lsop, y, metric='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track and save regression models\n",
    "\n",
    "Use the pipelines we built previously to:\n",
    "- track them using mlflow (kudos for using functions and/or loops)\n",
    "- compare the results on the mlflow UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
