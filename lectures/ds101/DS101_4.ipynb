{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Data Science\n",
    "## Part IV. - Dimensionality Reduction\n",
    "\n",
    "### Table of contents\n",
    "\n",
    "- ##### Dimensionality reduction:\n",
    "    - <a href=\"#What-is-Dimensionality-Reduction?\">Theory</a>\n",
    "    - <a href=\"#1.-Feature-Selection\">Feature Selection</a>\n",
    "    - <a href=\"#2.-Matrix-Decomposition\">Matrix Decomposition</a>\n",
    "    - <a href=\"#3.-Nonlinear-Dimensionality-Reduction\">Nonlinear Dimensionality Reduction</a>\n",
    "    - <a href=\"#2.-Trade-offs-&-When-to-Use-What\">Trade-offs & When to Use What</a>\n",
    "\n",
    "- ##### SVM\n",
    "    - <a href=\"#SVM-=-Support-Vector-Machines\">Theory</a>\n",
    "    - <a href=\"#Example\">Example</a>\n",
    "\n",
    "- ##### Feature Union\n",
    "    - <a href=\"#Feature-Unions\">Feature Union</a>\n",
    "    - <a href=\"#Create-custom-transformers\">Custom transformers</a>\n",
    "    - <a href=\"#Exercise:-Prediction-on-last-week's-dataset\">Exercise</a>\n",
    "    \n",
    "---\n",
    "\n",
    "## What is Dimensionality Reduction?\n",
    "\n",
    "Dimensionality reduction _\"is the process of reducing the number of random variables under consideration and can be divided into feature selection and feature extraction.\"_\n",
    "\n",
    "_\"__Feature selection__ approaches try to find a subset of the original variables. ... In some cases, data analysis such as regression or classification can be done in the reduced space more accurately than in the original space.\"_\n",
    "\n",
    "_\"__Feature extraction__ transforms the data in the high-dimensional space to a space of fewer dimensions. The data transformation may be linear, as in principal component analysis (PCA), but many nonlinear dimensionality reduction techniques also exist.\"_ from: <a href=\"https://en.wikipedia.org/wiki/Dimensionality_reduction\">Wiki</a>\n",
    "\n",
    "\n",
    "## Why it is important?\n",
    "\n",
    "With datasets containing hundreds (or even thousands) of features, some will inevitably be redundant, overlapping, or simply irrelevant to the prediction task. These unnecessary features can:\n",
    "- Slow down training and prediction\n",
    "- Increase the risk of overfitting\n",
    "- Make models harder to interpret\n",
    "\n",
    "Reducing the number of features to a manageable amount improves both model performance and efficiency.\n",
    "\n",
    "### <a href=\"https://en.wikipedia.org/wiki/Curse_of_dimensionality\">The curse of dimensionality</a>\n",
    "\n",
    "<img src=\"pics/curse-dimensions.png\" align=\"left\"> \n",
    "<br style=\"clear:left;\"/>\n",
    "(<a href=\"http://tm.durusau.net/wp-content/uploads/2016/06/curse-dimensions-460.png\">source</a>)\n",
    "\n",
    "This is one of our greatest enemies—right up there with overfitting and underfitting. As the number of dimensions increases, the number of possible states or input vectors grows **exponentially**. Even in the simplest case of binary variables, a dataset with 50 dimensions already has $2^{50} > 10^{15}$ number of possible inputs. That's **over a quadrillion** possible input combinations! 😱 To achieve the same level of effectiveness, **we need exponentially more training data** — which is often impractical.\n",
    "\n",
    "## Tools\n",
    "- <a href=\"http://scikit-learn.org/stable/modules/feature_selection.html\">Feature Selection</a>\n",
    "- <a href=\"http://scikit-learn.org/stable/modules/decomposition.html#decompositions\">Matrix decomposition</a>\n",
    "- <a href=\"http://scikit-learn.org/stable/modules/feature_extraction.html#feature-hashing\">Feature Hashing</a>\n",
    "- etc. (e.g., Autoencoders, t-SNE, UMAP, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d, Axes3D\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotme(X, y):\n",
    "    with sns.color_palette('muted', n_colors=3) as mycolors:\n",
    "        plt.scatter(*X.T, c=y, cmap=ListedColormap(mycolors), edgecolors='k')\n",
    "\n",
    "def plot_results_with_hyperplane(clf, clf_name, df, ax):\n",
    "    x_min, x_max = df.x.min() - .5, df.x.max() + .5\n",
    "    y_min, y_max = df.y.min() - .5, df.y.max() + .5\n",
    "\n",
    "    # step between points. i.e. [0, 0.02, 0.04, ...]\n",
    "    step = .02\n",
    "    # to plot the boundary, we're going to create a matrix of every possible point\n",
    "    # then label each point using our classifier\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, step), np.arange(y_min, y_max, step))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    # this gets our predictions back into a matrix\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # plot the boundaries\n",
    "    ax.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired, shading='auto')\n",
    "    ax.scatter(xs, ys, c=['r' if c else 'b' for c in cs], edgecolors='k')\n",
    "    ax.set_title(clf_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature Selection\n",
    "\n",
    "### Simple (Variance Threshold) Based Selection:\n",
    "\n",
    "\"[_`VarianceThreshold`_](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html) _is a simple baseline approach to feature selection. It removes all features whose variance doesn’t meet some threshold. By default, it removes all zero-variance features, i.e. features that have the same value in all samples.\"_ from: <a href=\"http://scikit-learn.org/stable/modules/feature_selection.html#removing-features-with-low-variance\">sklearn docs</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "thres = VarianceThreshold(.6)\n",
    "X_t = thres.fit_transform(X)\n",
    "X_t.shape, list(zip(iris.feature_names, thres.variances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotme(X_t, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination (<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn-feature-selection-rfe\">`RFE`</a>):\n",
    "\n",
    "_\"Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), __recursive feature elimination (RFE)__ is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and weights are assigned to each one of them. Then, features whose absolute weights are the smallest are pruned from the current set features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.\"_ from: <a href=\"http://scikit-learn.org/stable/modules/feature_selection.html#recursive-feature-elimination\">sklearn docs</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "rfe = RFE(LinearRegression(), n_features_to_select=2)\n",
    "X_t = rfe.fit_transform(X, y)\n",
    "X_t.shape, list(zip(iris.feature_names, rfe.ranking_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotme(X_t, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thought experiment: Consider the __<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html\">digits</a>__ dataset and try to describe the results found __<a href=\"http://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_digits.html#recursive-feature-elimination\">here</a>__.\n",
    "\n",
    "**Tip**: RFE is particularly useful when working with high-dimensional datasets, but it can be computationally expensive. Consider using [`RFECV`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html) (Recursive Feature Elimination with Cross-Validation) to automatically determine the optimal number of features! \n",
    "\n",
    "\n",
    "### Select based on models:\n",
    "\n",
    "This method is highly versatile since it relies on an external model to determine feature importance. Features are selected based on the model's coefficients or importance scores. If a feature's importance is below a predefined threshold, it is considered unimportant and removed.  \n",
    "In sklearn, the <a href=\"http://scikit-learn.org/stable/modules/feature_selection.html#feature-selection-using-selectfrommodel\">`SelectFromModel`</a>transformer requires an estimator that has either a `coef_` (for linear models) or `feature_importances_` (for tree-based models) attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "sel = SelectFromModel(LogisticRegression(C=.1, solver='lbfgs', multi_class='auto'))\n",
    "X_t = sel.fit_transform(X, y)\n",
    "X_t.shape, list(zip(iris.feature_names, sel.get_support()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotme(X_t, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**: This method works well with tree-based models (like Random Forests and Gradient Boosting) and linear models with L1 regularization (like Lasso). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Matrix Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis ([`PCA`](http://scikit-learn.org/stable/modules/decomposition.html#principal-component-analysis-pca))\n",
    "\n",
    "<div style=\"display: flex; align-items: center; gap: 20px;\">\n",
    "    <img src=\"pics/transforming_axes.gif\" width=\"500\">\n",
    "    <span style=\"display: block; width: 100%;\">\n",
    "        <h4>What is PCA?</h4>\n",
    "        Principal Component Analysis (PCA) is a <b>linear dimensionality reduction technique</b> that transforms data into a lower-dimensional space while <b>maximizing variance</b>. Instead of using the original features, PCA finds new axes (principal components) that capture the most variation in the data.  \n",
    "        </br></br>\n",
    "        Mathematically, PCA:\n",
    "        <ul>\n",
    "            <li>Computes the <b>covariance matrix</b> of the data.</li>\n",
    "            <li>Finds the <b>eigenvectors</b> and <b>eigenvalues</b> of this matrix.</li>\n",
    "            <li>Uses the eigenvectors corresponding to the largest eigenvalues as new feature axes.</li>\n",
    "        </ul>\n",
    "        Simply put, PCA <b>rotates</b> the data to find the best angles that preserve its structure with the least number of dimensions.\n",
    "    </span>\n",
    "</div>\n",
    "\n",
    "</br>\n",
    "\n",
    "<div style=\"display: flex; align-items: center; gap: 20px;\">\n",
    "    <img src=\"pics/finding_pca.gif\" width=\"500\">\n",
    "    <span style=\"display: block; width: 100%;\">\n",
    "        <h4>Why is PCA useful?</h4>\n",
    "        <ul>\n",
    "            <li><b>Reduces dimensionality</b> while retaining essential information.</li>\n",
    "            <li><b>Removes redundancy</b> by capturing correlated features in fewer dimensions.</li>\n",
    "            <li><b>Speeds up models</b> by working with fewer input variables.</li>\n",
    "            <li><b>Helps visualization</b> of high-dimensional data in 2D or 3D.</li>\n",
    "        </ul>\n",
    "        In practice, we don't always compute exact eigenvectors manually. Instead, PCA is implemented using <b>eigendecomposition** or iterative methods like <b>Singular Value Decomposition (SVD)</b>.\n",
    "        </br></br>\n",
    "        💡 <b>Fun Fact:</b> The first principal component is the direction of maximum variance, the second is orthogonal to it, capturing the next highest variance, and so on.\n",
    "    </span>\n",
    "</div>\n",
    "\n",
    "**Animations above are from [this great StackExchange answer](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_t = pca.fit_transform(X)\n",
    "X_t.shape, pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotme(X_t, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A notebook with the results shown in 3d space can be downloaded from <a href=\"http://scikit-learn.org/stable/_downloads/plot_pca_iris.ipynb\">here</a>.\n",
    "\n",
    "\n",
    "### Singular Value Decomposition (<a href=\"http://scikit-learn.org/stable/modules/decomposition.html#truncated-singular-value-decomposition-and-latent-semantic-analysis\">`SVD`</a>):\n",
    "\n",
    "<img src=\"pics/vector_decomposition.gif\" height=400 align=\"left\" style=\"margin-right: 20px\">\n",
    "\n",
    "Singular Value Decomposition (SVD) is a well-known matrix factorization method widely used in various fields, including statistics and signal processing. Essentially, it is a technique to decompose a matrix into its fundamental components, revealing its underlying structure.\n",
    "\n",
    "It is analogous to decomposing a 2D vector into its $x$ and $y$ components. Each component describes the original vector by storing information about:  \n",
    "\n",
    "**a)** The **direction of the components**, given an orthogonal basis (e.g., $u_x$ points to $(0, 1)$ and $u_y$ points to $(1, 0)$).  \n",
    "**b)** The **length of the projections**, indicating how much each basis contributes (e.g., $s_x=0.5$ and $s_y=1.2$).  \n",
    "**c)** The **description of the orthogonal basis** the projections are based on (e.g., $v_x$ points to $(0, 1)$, and $v_y$ to $(1, 0)$, so the original axes remain unchanged).  \n",
    "\n",
    "<br style=\"clear:left;\"/>\n",
    "\n",
    "The animation is sourced from <a href=\"https://towardsdatascience.com/svd-8c2f72e264f\">this detailed article on SVD</a>.\n",
    "\n",
    "Instead of decomposing just a single vector (of dimension $n$), we can extend this to a set of $m$ vectors by forming an $m \\times n$ matrix $M$. The decomposition of $M$ results in three matrices:\n",
    "\n",
    "$$\n",
    "M = U \\Sigma V^T\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $U$ is an $m \\times m$ orthogonal matrix storing the left singular vectors (directions of new basis vectors in the original space).  \n",
    "- $\\Sigma$ is an $m \\times n$ diagonal matrix storing singular values, which indicate the importance of each component.  \n",
    "- $V$ is an $n \\times n$ orthogonal matrix describing the projection axes.  \n",
    "\n",
    "The diagonal elements of $\\Sigma$ contain the singular values of $M$, ordered in descending magnitude. In feature extraction, we can approximate $M$ by keeping only the top $k$ singular values and their corresponding vectors:\n",
    "\n",
    "$$\n",
    "X \\approx X_k = U_k \\Sigma_k V_k^\\top\n",
    "$$\n",
    "\n",
    "This allows us to reduce dimensionality while retaining the most significant information in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "X_t = svd.fit_transform(X)\n",
    "X_t.shape, svd.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotme(X_t, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Nonlinear Dimensionality Reduction\n",
    "\n",
    "When working with high-dimensional data, linear methods like PCA or SVD often fall short in capturing complex nonlinear relationships. This is where **t-SNE** (t-Distributed Stochastic Neighbor Embedding) and **UMAP** (Uniform Manifold Approximation and Projection) come into play. These methods are particularly useful for **visualizing high-dimensional datasets in 2D or 3D** while preserving important structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE (t-Distributed Stochastic Neighbor Embedding)\n",
    "\n",
    "<img src=\"./pics/tsne.gif\" width=500 align=\"center\" style=\"margin-bottom: 20px\">\n",
    "\n",
    "[Source: Google Research](https://research.google/blog/realtime-tsne-visualizations-with-tensorflowjs/)\n",
    "\n",
    "### t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "\n",
    "t-SNE is a **dimensionality reduction technique** designed to help visualize **high-dimensional data** in 2D or 3D. Unlike PCA, which looks for the directions of maximum variance, t-SNE focuses on **preserving relationships between nearby points**, making it useful for **revealing clusters** in complex datasets.\n",
    "\n",
    "The key idea behind t-SNE is to measure how similar two points are in the high-dimensional space and then try to keep those similarities when mapping the data to a lower dimension. To do this, t-SNE builds two probability distributions:  \n",
    "\n",
    "1. **In the original high-dimensional space:** The similarity between two points $\\vec{x}_i$ and $\\vec{x}_j$ is measured using a Gaussian function (bell curve). The probability that $\\vec{x}_j$ is a \"neighbor\" of $\\vec{x}_i$ depends on how close they are:\n",
    "\n",
    "   $$ p_{j|i} = \\frac{\\exp\\left(-\\frac{||\\vec{x}_i - \\vec{x}_j||^2}{2\\sigma_i^2}\\right)}{\\sum_{k \\neq i} \\exp\\left(-\\frac{||\\vec{x}_i - \\vec{x}_k||^2}{2\\sigma_i^2}\\right)} $$\n",
    "\n",
    "   Here, $\\sigma_i$ is chosen automatically to balance local vs. global structure (controlled by a parameter called **perplexity**).\n",
    "\n",
    "2. **In the lower-dimensional space:** t-SNE tries to place the points $\\vec{y}_i$ and $\\vec{y}_j$ so that their similarities match the high-dimensional ones. However, instead of a Gaussian, it uses a **Student’s t-distribution**, which has \"fatter tails\" and allows distant points to stay far apart:\n",
    "\n",
    "   $$ q_{ij} = \\frac{(1 + ||\\vec{y}_i - \\vec{y}_j||^2)^{-1}}{\\sum_{k \\neq l} (1 + ||\\vec{y}_k - \\vec{y}_l||^2)^{-1}} $$\n",
    "\n",
    "   This prevents the **\"crowding problem,\"** where too many points would get squeezed together.\n",
    "\n",
    "t-SNE then **adjusts the low-dimensional positions** by minimizing the **difference (KL divergence)** between the high- and low-dimensional probabilities:\n",
    "\n",
    "   $$ C = \\sum_{i \\neq j} p_{ij} \\log \\frac{p_{ij}}{q_{ij}} $$\n",
    "\n",
    "This optimization process is done through **gradient descent**, moving points around iteratively until their relative similarities match as closely as possible.\n",
    "\n",
    "One thing to note is that **t-SNE focuses on preserving local structure**, meaning nearby points will stay close, but the global shape of the data might not be preserved. This makes it great for spotting clusters, but distances between clusters may not always be meaningful.\n",
    "\n",
    "Here’s an example of how t-SNE transforms data into a lower dimension:\n",
    "\n",
    "#### Pros\n",
    "✅ Great for **clustering visualization**  \n",
    "✅ Captures **local** structure well  \n",
    "✅ Works well for datasets with **clear groups**  \n",
    "\n",
    "#### Cons\n",
    "❌ Computationally expensive (doesn't scale well to large datasets)  \n",
    "❌ Different runs can produce **different visualizations**  \n",
    "❌ Distorts **global** relationships  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP (Uniform Manifold Approximation and Projection)\n",
    "\n",
    "<img src=\"./pics/umap.gif\" width=500 align=\"center\" style=\"margin-bottom: 20px\">\n",
    "\n",
    "[Source: UMAP Documentation](https://umap-learn.readthedocs.io/en/latest/aligned_umap_basic_usage.html)\n",
    "\n",
    "UMAP is a **dimensionality reduction technique** that is often used as an alternative to **t-SNE**, but it is generally **faster, better at preserving global structure, and scales to larger datasets**. Like t-SNE, it aims to keep similar points close together when mapping high-dimensional data into 2D or 3D, making it great for **visualizing clusters**.\n",
    "\n",
    "#### Step 1: Constructing the High-Dimensional Graph  \n",
    "UMAP starts by creating a **graph-based representation** of the data. Instead of just measuring distances, UMAP **assumes that the data lies on a manifold (a curved space) embedded in a high-dimensional space** and tries to learn its structure.  \n",
    "\n",
    "1. **Computing local neighborhoods:**  \n",
    "   - Each point $\\vec{x}_i$ gets a local neighborhood defined by a distance-based function.  \n",
    "   - The number of neighbors is controlled by a parameter called **n_neighbors**, which affects how local/global the embedding is.\n",
    "\n",
    "2. **Building a weighted graph:**  \n",
    "   - For each point, UMAP assigns probabilities to nearby points based on an exponential function:\n",
    "\n",
    "     $$ p_{j|i} = \\exp\\left(\\frac{-d(\\vec{x}_i, \\vec{x}_j) - \\rho_i}{\\sigma_i}\\right) $$  \n",
    "\n",
    "     where:\n",
    "     - $d(\\vec{x}_i, \\vec{x}_j)$ is the distance between two points.\n",
    "     - $\\rho_i$ is a local adjustment term to ensure a minimum number of connections.\n",
    "     - $\\sigma_i$ controls how fast probability decreases with distance.\n",
    "\n",
    "3. **Symmetric Graph Construction:**  \n",
    "   - The final edge weight between two points is computed as:  \n",
    "\n",
    "     $$ p_{ij} = p_{j|i} + p_{i|j} - p_{j|i} p_{i|j} $$  \n",
    "\n",
    "   - This graph represents how \"connected\" each point is to others in the high-dimensional space.\n",
    "\n",
    "#### Step 2: Mapping to a Lower-Dimensional Space  \n",
    "Now, UMAP **tries to construct a similar graph in lower dimensions (2D or 3D)** while preserving the relationships from the high-dimensional space.\n",
    "\n",
    "1. **Defining the Low-Dimensional Probabilities:**  \n",
    "   - Instead of using a Gaussian (like t-SNE), UMAP uses a **fuzzy topological structure** where edges in the new space have a similar probability function.  \n",
    "   - The optimization goal is to **find a low-dimensional layout where the graph structure is best preserved**.\n",
    "\n",
    "2. **Minimizing the Difference Between Graphs (Optimization Step):**  \n",
    "   - UMAP minimizes the **cross-entropy** between the high- and low-dimensional graphs:\n",
    "\n",
    "     $$ C = \\sum_{(i,j) \\in \\text{edges}} p_{ij} \\log q_{ij} + (1 - p_{ij}) \\log (1 - q_{ij}) $$  \n",
    "\n",
    "   - This ensures that nearby points stay together and far points stay apart.\n",
    "\n",
    "#### How is UMAP Different from t-SNE?  \n",
    "- **Faster:** UMAP uses an **approximate nearest neighbor search** (t-SNE does not), making it much faster for large datasets.  \n",
    "- **Better global structure:** Unlike t-SNE, which mainly preserves local relationships, UMAP can maintain **both local and some global structures**.  \n",
    "- **More interpretable distances:** The space UMAP creates is often more meaningful in terms of distances between clusters.\n",
    "\n",
    "UMAP is widely used for **data exploration, clustering, and visualization**, especially in **high-dimensional datasets like images, text, and biological data (e.g., single-cell RNA sequencing).** 🚀\n",
    "\n",
    "#### Pros\n",
    "✅ Faster and **scales better** than t-SNE  \n",
    "✅ Preserves **more of the global structure**  \n",
    "✅ Works well for datasets with complex manifolds  \n",
    "✅ Can be used for **general-purpose dimensionality reduction** (not just visualization)  \n",
    "\n",
    "#### Cons\n",
    "❌ May not always capture clusters as well as t-SNE  \n",
    "❌ Some parameters (like `n_neighbors`) can significantly impact the output  \n",
    "\n",
    "\n",
    "### When to Use t-SNE vs. UMAP?\n",
    "| Feature | t-SNE | UMAP |\n",
    "|---------|------|------|\n",
    "| Focus  | Local Structure | Local + Global Structure |\n",
    "| Speed  | Slower | Faster |\n",
    "| Interpretability | Harder to control | More interpretable |\n",
    "| Large Datasets | Struggles | Handles well |\n",
    "| Reproducibility | Less consistent | More stable |\n",
    "\n",
    "**TL;DR**: Use **t-SNE** for **cluster visualization**, and **UMAP** for **general-purpose dimensionality reduction**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "plotme(X_tsne, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UMAP requires an installation with:\n",
    "```bash\n",
    "pip install umap-learn\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "umap_model = umap.UMAP(n_components=2, random_state=42)\n",
    "X_umap = umap_model.fit_transform(X)\n",
    "\n",
    "plotme(X_umap, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Trade-offs & When to Use What\n",
    "\n",
    "Dimensionality reduction techniques come with different strengths and trade-offs. Choosing the right method depends on the dataset and the specific goal of the analysis. Below is a quick guide to help decide when to use each technique.\n",
    "\n",
    "### **Quick Summary Table**\n",
    "| **Method**        | **Best When...** | **Key Trade-offs** |\n",
    "|------------------|-----------------|--------------------|\n",
    "| **PCA**  | You need a **linear reduction** that captures the most variance. | Assumes linear relationships, may not work well for highly nonlinear data. |\n",
    "| **SVD**  | You're working with **sparse or high-dimensional data**, such as text (NLP). | Similar to PCA but better suited for matrix factorization tasks. |\n",
    "| **t-SNE / UMAP**  | You need a **visualization or clustering** rather than strict feature reduction. | t-SNE is computationally expensive, UMAP is faster but assumes a manifold structure. Neither method is great for actual feature reduction. |\n",
    "| **Feature Selection**  | Interpretability is important (e.g., **medical or business applications**). | May remove informative but redundant features, does not create new combined features. |\n",
    "\n",
    "### **How to Choose the Right Method**\n",
    "- If your goal is to **reduce dimensions while preserving variance** → **Use PCA**.\n",
    "- If you’re working with **sparse, high-dimensional data (e.g., text, NLP)** → **Use SVD**.\n",
    "- If you want to **visualize high-dimensional data** and discover clusters → **Use t-SNE or UMAP**.\n",
    "- If **interpretability** is critical and you need to keep meaningful features → **Use Feature Selection**.\n",
    "\n",
    "There is no single \"best\" dimensionality reduction method—each has strengths and weaknesses. The best approach depends on the dataset and the specific task at hand. 🚀  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model of the Week:\n",
    "## Support Vector Machines (<a href=\"http://scikit-learn.org/stable/modules/svm.html#support-vector-machines\">`SVM`</a>)\n",
    "\n",
    "Behold, the first truly **black-box** classifier! (Fun fact: SVMs can also be used for regression.)  \n",
    "Don't worry about the strange name—everything will make sense soon.  \n",
    "\n",
    "### Why are SVMs awesome?  \n",
    "Because they are **both linear and nonlinear classifiers** at the same time! Sounds confusing? Here’s the trick: **SVMs perform linear classification, but only after transforming the data into a higher-dimensional space.**  \n",
    "\n",
    "Let's first consider a simple **linear** case in 2D. There can be many ways to separate two classes with a straight line (a.k.a. a **hyperplane** in higher dimensions), as shown in the figure below:\n",
    "\n",
    "<img src=\"pics/svm_separating_hyperplanes.png\">  \n",
    "<small>Image source: <a href=\"https://en.wikipedia.org/wiki/Support-vector_machine\">Wikipedia</a></small>\n",
    "\n",
    "Clearly, not all separating hyperplanes are equally good.  \n",
    "- $H_3$ is useless—it doesn’t even separate the classes.  \n",
    "- Both $H_1$ and $H_2$ do separate them, but… **$H_2$ feels like a better separator**. Why?  \n",
    "\n",
    "Look at the training points closest to $H_1$—they are dangerously close to flipping sides. The best decision boundary is the one that **maximizes the margin**, meaning it keeps the classes as far apart as possible while still separating them.  \n",
    "\n",
    "### Support Vectors  \n",
    "The **support vectors** are the data points closest to the decision boundary. These are the most critical points in the dataset—if we remove or move them, the boundary will shift.  \n",
    "**SVMs aim to maximize the margin around the decision boundary**, ensuring the best possible separation.  \n",
    "\n",
    "### Example  \n",
    "Let's see it in action! (Plotting function is adapted from <a href=\"http://blog.yhat.com/posts/why-support-vector-machine.html\">this blog post</a>.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import tree\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate 500 points, and classify them according to an imaginary circle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.random.rand(500) * 5\n",
    "ys = np.random.rand(500) * 5\n",
    "cs = np.int0((xs - 3) ** 2 + (ys - 2) ** 2 > 3)\n",
    "\n",
    "df = pd.DataFrame(data={'x': xs, 'y': ys, 'c': cs})\n",
    "train_cols = ['x', 'y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = {\n",
    "    \"SVM\": svm.SVC(gamma='auto'),\n",
    "    \"Logistic\" : linear_model.LogisticRegression(solver='lbfgs', multi_class='auto'),\n",
    "    \"Tree\": tree.DecisionTreeClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15,5))\n",
    "\n",
    "for i, (clf_name, clf) in enumerate(clfs.items()):\n",
    "    clf.fit(df[train_cols], df.c)\n",
    "    plot_results_with_hyperplane(clf, clf_name, df, ax[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How the heck is this linear?\n",
    "\n",
    "It is linear in the *transformed space*. If we introduce a third dimension, which we get like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs = (xs - 3) ** 2 + (ys - 2) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then our data points will look like this in the 3D space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter3D(xs, ys, zs, c=cs, cmap=plt.cm.Paired)\n",
    "ax.view_init(10, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that the data points can be separated by a plane in this 3D space. Then projecting the intersection of the plane and the function $(x_1-3)^2 + (x_2-2)^2$ back to 2D, we get the classification boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.gca()\n",
    "ax.scatter(xs, ys, c=cs, cmap=plt.cm.Paired, edgecolors='k')\n",
    "ax.add_patch(plt.Circle((3,2), radius=np.sqrt(3), fill=False, linewidth=.7))\n",
    "fig.set_figwidth(4)\n",
    "fig.set_figheight(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are videos that make this waaay more clear: <a href=\"https://www.youtube.com/watch?v=9NrALgHFwTo\">vid1</a>, <a href=\"https://www.youtube.com/watch?v=3liCbRZPrZA\">vid2</a>\n",
    "\n",
    "$\\renewcommand{\\vec}[1]{\\mathbf{#1}}$\n",
    "### The Kernel Trick  \n",
    "\n",
    "When the the data isn’t linearly separable, we might need to **transform** the data into a higher-dimensional space where it *is* separable.  \n",
    "\n",
    "Sounds expensive, right? Computing these extra dimensions can get **very** computationally heavy. Luckily, we don’t actually need to compute the transformed vectors—**we only need their dot products!**  \n",
    "\n",
    "That’s where the **kernel trick** comes in. Instead of explicitly transforming the data, we use a **kernel function** that tells us what the dot product of two transformed vectors *would have been*:\n",
    "\n",
    "$$ K(\\vec{x},\\vec{y}) = \\phi(\\vec{x}) \\cdot \\phi(\\vec{y}) $$\n",
    "\n",
    "This lets us work in higher-dimensional spaces **without ever computing the transformation explicitly**!  \n",
    "\n",
    "#### Example: A Polynomial Kernel  \n",
    "\n",
    "Let’s say we use a simple **polynomial kernel function** in two dimensions:\n",
    "\n",
    "$$ K(\\vec{x},\\vec{y}) = (1 + \\vec{x} \\cdot \\vec{y})^2 $$\n",
    "\n",
    "where $\\vec{x} = (x_1, x_2)$ and $\\vec{y} = (y_1, y_2)$.  \n",
    "\n",
    "At first glance, it’s not obvious what transformation $\\phi$ this corresponds to. Let’s expand the equation:\n",
    "\n",
    "$$K(\\vec{x},\\vec{y}) = (1+\\vec{x} \\cdot \\vec{y})^2 = (1 + x_1y_1 + x_2y_2)^2$$  \n",
    "$$ = 1 + x_1^2y_1^2 + x_2^2y_2^2 + 2x_1y_1 + 2x_2y_2 + 2x_1x_2y_1y_2$$  \n",
    "\n",
    "If we look at this carefully, we can rewrite it as a dot product in a **6-dimensional space**:\n",
    "\n",
    "$$\\vec{x'} = \\phi(\\vec{x}) = (1, x_1^2, x_2^2, \\sqrt{2}x_1, \\sqrt{2}x_2, \\sqrt{2}x_1x_2)$$  \n",
    "\n",
    "and  \n",
    "\n",
    "$$\\vec{y'} = \\phi(\\vec{y}) = (1, y_1^2, y_2^2, \\sqrt{2}y_1, \\sqrt{2}y_2, \\sqrt{2}y_1y_2)$$  \n",
    "\n",
    "So, the **implicit transformation** that our kernel function is applying is:\n",
    "\n",
    "$$\\phi(\\vec{x}) = (1, x_1^2, x_2^2, \\sqrt{2}x_1, \\sqrt{2}x_2, \\sqrt{2}x_1x_2)$$  \n",
    "\n",
    "#### Why Does This Matter?  \n",
    "\n",
    "The **key insight** is that we never actually compute $\\phi(\\vec{x})$! Instead, we just compute $K(\\vec{x},\\vec{y})$, which gives us the same result as if we had transformed the data. This **saves enormous amounts of computation** while still allowing us to work in higher-dimensional spaces.  \n",
    "\n",
    "Kernel functions might sound mysterious at first, but this example shows that they aren’t just black magic! They’re simply a clever way to work in higher dimensions **without ever explicitly going there**.  \n",
    "\n",
    "Check out some <a href=\"http://scikit-learn.org/stable/modules/svm.html#svm-kernels\">common kernel functions</a> that people use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Feature Unions\n",
    "\n",
    "<a href=\"http://scikit-learn.org/stable/modules/pipeline.html#featureunion-composite-feature-spaces\">`FeatureUnions`</a> are \"parallel pipes\". Every transformator in the union is applied to the input data, and the results are concatenated. It is very useful if we want to create new features from appling different transformers on the same data.\n",
    "\n",
    "Not only the transformators steps can be set, but also weight can be associated with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = FeatureUnion(transformer_list=[\n",
    "    ('thres', VarianceThreshold(.7)),\n",
    "    ('svd', TruncatedSVD(n_components=2))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FeatureUnion can be a step in a pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('norm', StandardScaler()),\n",
    "    ('feat', feat),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train, y_train)\n",
    "accuracy_score(y_test, pipe.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipes can be part of unions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union = FeatureUnion([\n",
    "    ('normsvd', Pipeline([('norm', StandardScaler()),\n",
    "                          ('svd', TruncatedSVD(n_components=2))])),\n",
    "    ('pca', PCA('mle'))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And put this into a pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('feat', union),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train, y_train)\n",
    "accuracy_score(y_test, pipe.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIPECEPTION!\n",
    "\n",
    "---\n",
    " \n",
    "### <a href=\"http://scikit-learn.org/stable/modules/preprocessing.html#custom-transformers\">Create custom transformers</a>\n",
    "\n",
    "Sometimes we just couldn't find what we are looking for in sklearn's massive library. In this case we can write our own transformers.  \n",
    "It's pretty easy:\n",
    "\n",
    "- Import the baseclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Subclass our transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multiplier(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, multitude):\n",
    "        self.multitude = multitude\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X * self.multitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We are good to go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(1, 21).reshape(4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi = Multiplier(5)\n",
    "multi.transform(np.arange(1, 21).reshape(4, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi.transform(X_test)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise: Prediction on last week's dataset\n",
    "\n",
    "- Use last week's dataset\n",
    "- Transform the nominal features\n",
    "- Transform the numerical features\n",
    "- Use the custom transformer from the cheat sheet\n",
    "- Create a feature union from the nominal and the numerical feature pipes\n",
    "- Create a pipe with the feature union and a model of your liking\n",
    "- Predict!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: [Car Wash](https://www.youtube.com/watch?v=eB0aROCl530)\n",
    "\n",
    "Build a pipeline to predict the car type using the 2004 cars dataset (`./data/04cars.csv`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "szisz_ds_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
