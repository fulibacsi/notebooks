{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Data Science\n",
    "## Part IV. - Dimensionality Reduction\n",
    "\n",
    "### Table of contents\n",
    "\n",
    "- ##### Dimensionality reduction:\n",
    "    - <a href=\"#What-is-Dimensionality-Reduction?\">Theory</a>\n",
    "    - <a href=\"#1.-Feature-Selection\">Feature Selection</a>\n",
    "    - <a href=\"#2.-Matrix-Decomposition\">Matrix Decomposition</a>\n",
    "\n",
    "- ##### SVM\n",
    "    - <a href=\"#SVM-=-Support-Vector-Machines\">Theory</a>\n",
    "    - <a href=\"#Example\">Example</a>\n",
    "\n",
    "- ##### Feature Union\n",
    "    - <a href=\"#Feature-Unions\">Feature Union</a>\n",
    "    - <a href=\"#Create-custom-transformers\">Custom transformers</a>\n",
    "    - <a href=\"#Exercise:-Prediction-on-last-week's-dataset\">Exercise</a>\n",
    "    \n",
    "---\n",
    "\n",
    "## What is Dimensionality Reduction?\n",
    "Dimensionality reduction _\"is the process of reducing the number of random variables under consideration, and can be divided into feature selection and feature extraction.\"_\n",
    "\n",
    "_\"__Feature selection__ approaches try to find a subset of the original variables. ... In some cases, data analysis such as regression or classification can be done in the reduced space more accurately than in the original space.\"_\n",
    "\n",
    "_\"__Feature extraction__ transforms the data in the high-dimensional space to a space of fewer dimensions. The data transformation may be linear, as in principal component analysis (PCA), but many nonlinear dimensionality reduction techniques also exist.\"_ from: <a href=\"https://en.wikipedia.org/wiki/Dimensionality_reduction\">Wiki</a>\n",
    "\n",
    "\n",
    "## Why it is important?\n",
    "With hundreds of features in the datasets, there will always be some which does not contribute to the overall precision of the predictive model. These features could be redundant, overlapping or linear combination of each other or simply irrelevant to the prediction. To improve training and transformation/prediction time, it is crucial to reduce the number of features to a moderate amount.\n",
    "\n",
    "### <a href=\"https://en.wikipedia.org/wiki/Curse_of_dimensionality\">The curse of dimensionality</a>\n",
    "\n",
    "<img src=\"pics/curse-dimensions.png\" align=\"left\"> \n",
    "<br style=\"clear:left;\"/>\n",
    "(<a href=\"http://tm.durusau.net/wp-content/uploads/2016/06/curse-dimensions-460.png\">source</a>)\n",
    "\n",
    "This is our greatest enemy, after over/underfitting. With the increase of dimensions, the number of possible states or input vectors grow **exponentially**. Even in the most basic case of binary variables, in a moderate amount of say, 50 dimensions, we'll have $2^{50} > 10^{15}$ number of possible inputs. This means that for the same effectiveness, **we need exponentially more training points**!\n",
    "\n",
    "## Tools\n",
    "- <a href=\"http://scikit-learn.org/stable/modules/feature_selection.html\">Feature Selection</a>\n",
    "- <a href=\"http://scikit-learn.org/stable/modules/decomposition.html#decompositions\">Matrix decomposition</a>\n",
    "- <a href=\"http://scikit-learn.org/stable/modules/feature_extraction.html#feature-hashing\">Hashing</a>\n",
    "- etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d, Axes3D\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotme(X, y):\n",
    "    with sns.color_palette('muted', n_colors=3) as mycolors:\n",
    "        plt.scatter(*X.T, c=y, cmap=ListedColormap(mycolors), edgecolors='k')\n",
    "\n",
    "def plot_results_with_hyperplane(clf, clf_name, df, ax):\n",
    "    x_min, x_max = df.x.min() - .5, df.x.max() + .5\n",
    "    y_min, y_max = df.y.min() - .5, df.y.max() + .5\n",
    "\n",
    "    # step between points. i.e. [0, 0.02, 0.04, ...]\n",
    "    step = .02\n",
    "    # to plot the boundary, we're going to create a matrix of every possible point\n",
    "    # then label each point using our classifier\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, step), np.arange(y_min, y_max, step))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    # this gets our predictions back into a matrix\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # plot the boundaries\n",
    "    ax.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired, shading='auto')\n",
    "    ax.scatter(xs, ys, c=['r' if c else 'b' for c in cs], edgecolors='k')\n",
    "    ax.set_title(clf_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature Selection\n",
    "\n",
    "### Simple (variance) threshold based:\n",
    "\n",
    "\"[_`VarianceThreshold`_](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html) _is a simple baseline approach to feature selection. It removes all features whose variance doesnâ€™t meet some threshold. By default, it removes all zero-variance features, i.e. features that have the same value in all samples.\"_ from: <a href=\"http://scikit-learn.org/stable/modules/feature_selection.html#removing-features-with-low-variance\">sklearn docs</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "thres = VarianceThreshold(.6)\n",
    "X_t = thres.fit_transform(X)\n",
    "X_t.shape, list(zip(iris.feature_names, thres.variances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotme(X_t, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination (<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn-feature-selection-rfe\">`RFE`</a>):\n",
    "\n",
    "_\"Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), __recursive feature elimination (RFE)__ is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and weights are assigned to each one of them. Then, features whose absolute weights are the smallest are pruned from the current set features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.\"_ from: <a href=\"http://scikit-learn.org/stable/modules/feature_selection.html#recursive-feature-elimination\">sklearn docs</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "rfe = RFE(LinearRegression(), n_features_to_select=2)\n",
    "X_t = rfe.fit_transform(X, y)\n",
    "X_t.shape, list(zip(iris.feature_names, rfe.ranking_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotme(X_t, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thought experiment: Consider the __<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html\">digits</a>__ dataset and try to describe the results found __<a href=\"http://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_digits.html#recursive-feature-elimination\">here</a>__.\n",
    "\n",
    "\n",
    "### Select based on models:\n",
    "\n",
    "This method is very versatile since it is based on an external model. The features are selected based on the external model's coefficients. If a feature is under a set threshold, it is considered unimportant and removed.  \n",
    "In sklearn, the <a href=\"http://scikit-learn.org/stable/modules/feature_selection.html#feature-selection-using-selectfrommodel\">`SelectFromModel`</a> transformator requires an estimator that has a `coef_` or `feature_importances_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "sel = SelectFromModel(LogisticRegression(C=.1, solver='lbfgs', multi_class='auto'))\n",
    "X_t = sel.fit_transform(X, y)\n",
    "X_t.shape, list(zip(iris.feature_names, sel.get_support()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotme(X_t, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Matrix Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Principal Component Analysis (<a href=\"http://scikit-learn.org/stable/modules/decomposition.html#principal-component-analysis-pca\">`PCA`</a>):\n",
    "\n",
    "<img src=\"pics/transforming_axes.gif\" width=500 align=\"left\">\n",
    "\n",
    "_\"The main linear technique for dimensionality reduction, principal component analysis, performs a **linear mapping** of the data to a lower-dimensional space in such a way that the **variance of the data** in the low-dimensional representation is **maximized**. In practice, the covariance (and sometimes the correlation) matrix of the data is constructed and the eigen vectors on this matrix are computed.\"_ - <a href=\"https://en.wikipedia.org/wiki/Dimensionality_reduction#Principal_component_analysis_.28PCA.29\">wiki</a>  \n",
    "Basically, the goal is to generate new axes based on the original ones by a linear transformation ($w_1x + w_2y$) and select the ones that maximizes the \"spread\" of the points.\n",
    "<br style=\"clear:left;\"/>\n",
    "\n",
    "<img src=\"pics/finding_pca.gif\" width=500 align=\"left\">\n",
    "\n",
    "_\"The eigen vectors that correspond to the largest eigenvalues (the principal components) can now be used to reconstruct a large fraction of the variance of the original data. Moreover, the first few eigen vectors can often be interpreted in terms of the large-scale physical behavior of the system. The original space (with dimension of the number of points) has been reduced (with data loss, but hopefully retaining the most important variance) to the space spanned by a few eigenvectors.\"_ - <a href=\"https://en.wikipedia.org/wiki/Dimensionality_reduction#Principal_component_analysis_.28PCA.29\">wiki</a>  \n",
    "Exact ideal eigenvectors can be found by <a href=\"https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix\">decomposiong the matrix</a> or by using iterative algorithms (eg. <a href=\"https://en.wikipedia.org/wiki/Ordinary_least_squares\">OLS</a>).\n",
    "<br style=\"clear:left;\"/>\n",
    "\n",
    "The animations are from <a href=\"https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues\">this excellent stackechange answer</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_t = pca.fit_transform(X)\n",
    "X_t.shape, pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotme(X_t, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A notebook with the results shown in 3d space can be downloaded from <a href=\"http://scikit-learn.org/stable/_downloads/plot_pca_iris.ipynb\">here</a>.\n",
    "\n",
    "\n",
    "### Singular Value Decomposition (<a href=\"http://scikit-learn.org/stable/modules/decomposition.html#truncated-singular-value-decomposition-and-latent-semantic-analysis\">`SVD`</a>):\n",
    "\n",
    "<img src=\"pics/vector_decomposition.gif\" height=400 align=\"left\" style=\"margin-right: 20px\">\n",
    "\n",
    "A well known matrix factorization method which has been widely used in many field of statistics, signal processing, etc. It is basicly a matrix decomposition method. \n",
    "\n",
    "It is the same as if we want the decompose a 2D vector into its $x$ and $y$ components, which components will describe the original vector by storing information about:  \n",
    "**a)** the **direction of the components** given an orthogonal axes ($u_x$ points to $(0, 1)$, and $u_y$ points to $(1, 0)$),   \n",
    "**b)** the **length of the projections** (eg. $s_x=0.5$ while $s_y=1.2$), and   \n",
    "**c)** the description of the **orthogonal axes the projection are based** ($v_x$ is pointing to $(0, 1)$ and $v_y$ is to $(1, 0)$ so the original axes without any transformation).  \n",
    "\n",
    "<br style=\"clear:left;\"/>\n",
    "Animation is from <a href=\"https://towardsdatascience.com/svd-8c2f72e264f\">this detailed article on SVD</a>.\n",
    "\n",
    "Instead of decomposing a single vector (of dimension $n$) we can decompose a set of vectors ($m$) at the same time by forming an $m{\\times}n$ matrix $M$ from which the decomposition will create 3 matrices:\n",
    "\n",
    "$$M=U{\\Sigma}V^T$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $U$ - $m{\\times}m$ (orthogonal matrix), storing the directions\n",
    "- $\\Sigma$ - $m{\\times}n$ (diagonal matrix), storing the lengths\n",
    "- $V$ - $n{\\times}n$ (orthogonal matrix), describing projection axes\n",
    "\n",
    "The ${\\Sigma}$ matrix's diagonal contains $M$ matrix's singular values (square roots of eigenvalues) in descending order. In the case of feature extraction, only the first _k_ dimensions are left.\n",
    "$$X \\approx X_k = U_k \\Sigma_k V_k^\\top$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "X_t = svd.fit_transform(X)\n",
    "X_t.shape, svd.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotme(X_t, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model of the week:\n",
    "## Support Vector Machines (<a href=\"http://scikit-learn.org/stable/modules/svm.html#support-vector-machines\">`SVM`</a>)\n",
    "\n",
    "\n",
    "Behold, the first truly **black box** classifier (note: SVM-s can also be used for regression). Don't bother with the weird name, it will make sense later.  \n",
    "**Why are SVMs awesome?** Because it is a nonlinear classifier *while it is a linear classifier*. What? Yes! SVM-s do linear classification on the data **after it is transformed**.  \n",
    "  \n",
    "Let's look at the linear 2D case. There can be many solutions to the problem \"Find the separating hyperplane\", as seen in the picture below:\n",
    "<img src=\"pics/svm_separating_hyperplanes.png\">  \n",
    "from: <a href=\"https://en.wikipedia.org/wiki/Support-vector_machine\">wiki</a>\n",
    "However, they are obviously not equally good. $H_3$ doesn't separate the points according to the class, while $H_1$ and $H_2$ do. But if we look at $H_1$ and $H_2$, **we \"feel\" that $H_2$ is a better separator**, because there is less ambiguity in classifying the train points (look at the points closest to $H_1$ - they aren't far from being in the other class!).  \n",
    "The points closest to the possible boundaries are called **support vectors**. **SVM-s try to maximize the margins around the separator**, only points close to the decision boundary affect optimality. The boundary would change if we remove one of the support vectors.  \n",
    "  \n",
    "\n",
    "### Example\n",
    "Let's take a look at an example (plotting function is from <a href=\"http://blog.yhat.com/posts/why-support-vector-machine.html\">here</a>):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import tree\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate 500 points, and classify them according to an imaginary circle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.random.rand(500) * 5\n",
    "ys = np.random.rand(500) * 5\n",
    "cs = np.int0((xs - 3) ** 2 + (ys - 2) ** 2 > 3)\n",
    "\n",
    "df = pd.DataFrame(data={'x': xs, 'y': ys, 'c': cs})\n",
    "train_cols = ['x', 'y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = {\n",
    "    \"SVM\": svm.SVC(gamma='auto'),\n",
    "    \"Logistic\" : linear_model.LogisticRegression(solver='lbfgs', multi_class='auto'),\n",
    "    \"Tree\": tree.DecisionTreeClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15,5))\n",
    "\n",
    "for i, (clf_name, clf) in enumerate(clfs.items()):\n",
    "    clf.fit(df[train_cols], df.c)\n",
    "    plot_results_with_hyperplane(clf, clf_name, df, ax[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How the heck is this linear?\n",
    "\n",
    "It is linear in the *transformed space*. If we introduce a third dimension, which we get like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs = (xs - 3) ** 2 + (ys - 2) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then our data points will look like this in the 3D space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.scatter3D(xs, ys, zs, c=cs, cmap=plt.cm.Paired)\n",
    "ax.view_init(10, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that the data points can be separated by a plane in this 3D space. Then projecting the intersection of the plane and the function $(x_1-3)^2 + (x_2-2)^2$ back to 2D, we get the classification boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.gca()\n",
    "ax.scatter(xs, ys, c=cs, cmap=plt.cm.Paired, edgecolors='k')\n",
    "ax.add_patch(plt.Circle((3,2), radius=np.sqrt(3), fill=False, linewidth=.7))\n",
    "fig.set_figwidth(4)\n",
    "fig.set_figheight(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are videos that make this waaay more clear: <a href=\"https://www.youtube.com/watch?v=9NrALgHFwTo\">vid1</a>, <a href=\"https://www.youtube.com/watch?v=3liCbRZPrZA\">vid2</a>\n",
    "\n",
    "$\\renewcommand{\\vec}[1]{\\mathbf{#1}}$\n",
    "### Kernel trick\n",
    "  \n",
    "But calculating these additional dimensions can be computationally heavy. Fortunately the algorithms only need the dot product of the transformed vectors, not the transformed vectors themselves! This way we only need a **kernel function** that tells us what the product of two transformed vectors are: $ K(\\vec{x},\\vec{y}) = \\phi(\\vec{x}) \\phi(\\vec{y})$.  \n",
    "  \n",
    "For example if we specify a simple polynomial kernel function in two dimensions $\\phi(\\vec{x})\\phi(\\vec{y}) = K(\\vec{x},\\vec{y}) = (1+\\vec{xy})^2$, where $\\vec{x} = (x_1, x_2)$ and $\\vec{y} = (y_1, y_2)$, we don't see immediately what $\\phi$ transformation corresponds to this. Let's see! \n",
    "$$K(\\vec{x},\\vec{y}) = (1+\\vec{xy})^2 = (1+x_1y_1+x_2y_2)^2 = (1 + x_1^2y_1^2 + x_2^2y_2^2 + 2x_1y_1 + 2x_2y_2 + 2x_1x_2y_1y_2)$$\n",
    "With a bit of thinking, we can see that this is a dot product of the 6 dimensional vectors \n",
    "$$\\vec{x'} = \\phi(\\vec{x}) = (1,x_1^2,x_2^2,\\sqrt{2}x_1,\\sqrt{2}x_2,\\sqrt{2}x_1x_2)$$ and \n",
    "$$\\vec{y'} = \\phi(\\vec{y}) = (1,y_1^2,y_2^2,\\sqrt{2}y_1,\\sqrt{2}y_2,\\sqrt{2}y_1y_2)$$! So the transformation function is $\\phi(\\vec{x}) = \\phi(x_1, x_2) = \\vec{x'} = (1,x_1^2,x_2^2,\\sqrt{2}x_1,\\sqrt{2}x_2,\\sqrt{2}x_1x_2)$.  \n",
    "  \n",
    "The point of kernel functions is that we don't *need* the transformations themselves, only the dot products! This example just shows that kernel functions aren't some form of black magic.\n",
    "  \n",
    "Take a look at some <a href=\"http://scikit-learn.org/stable/modules/svm.html#svm-kernels\">common</a> kernels that people use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Feature Unions\n",
    "\n",
    "<a href=\"http://scikit-learn.org/stable/modules/pipeline.html#featureunion-composite-feature-spaces\">`FeatureUnions`</a> are \"parallel pipes\". Every transformator in the union is applied to the input data, and the results are concatenated. It is very useful if we want to create new features from appling different transformers on the same data.\n",
    "\n",
    "Not only the transformators steps can be set, but also weight can be associated with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = FeatureUnion(transformer_list=[\n",
    "    ('thres', VarianceThreshold(.7)),\n",
    "    ('svd', TruncatedSVD(n_components=2))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FeatureUnion can be a step in a pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('norm', StandardScaler()),\n",
    "    ('feat', feat),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train, y_train)\n",
    "accuracy_score(y_test, pipe.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipes can be part of unions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union = FeatureUnion([\n",
    "    ('normsvd', Pipeline([('norm', StandardScaler()),\n",
    "                          ('svd', TruncatedSVD(n_components=2))])),\n",
    "    ('pca', PCA('mle'))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And put this into a pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('feat', union),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train, y_train)\n",
    "accuracy_score(y_test, pipe.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIPECEPTION!\n",
    "\n",
    "---\n",
    " \n",
    "### <a href=\"http://scikit-learn.org/stable/modules/preprocessing.html#custom-transformers\">Create custom transformers</a>\n",
    "\n",
    "Sometimes we just couldn't find what we are looking for in sklearn's massive library. In this case we can write our own transformers.  \n",
    "It's pretty easy:\n",
    "\n",
    "- Import the baseclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Subclass our transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multiplier(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, multitude):\n",
    "        self.multitude = multitude\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X * self.multitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We are good to go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(1, 21).reshape(4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi = Multiplier(5)\n",
    "multi.transform(np.arange(1, 21).reshape(4, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi.transform(X_test)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise: Prediction on last week's dataset\n",
    "\n",
    "- Use last week's dataset\n",
    "- Transform the nominal features\n",
    "- Transform the numerical features\n",
    "- Use the custom transformer from the cheat sheet\n",
    "- Create a feature union from the nominal and the numerical feature pipes\n",
    "- Create a pipe with the feature union and a model of your liking\n",
    "- Predict!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
