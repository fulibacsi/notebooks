{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Data Science\n",
    "## Part III. - Data Transformation\n",
    "\n",
    "### Table of contents\n",
    "\n",
    "- ##### Data Transformation\n",
    "    - <a href=\"#What-is-Data-Transformation?\">Theory</a>\n",
    "    - <a href=\"#1.-Numerical-features\">Numerical features</a>\n",
    "    - <a href=\"#2.-Nominal-features\">Nominal features</a>\n",
    "\n",
    "- ##### Models:\n",
    "    - <a href=\"#Intermission---instance-based-classifiers\">kNN</a>\n",
    "    - <a href=\"Intermission-II.---Model-of-the-week\">Decision tree</a>\n",
    "\n",
    "---\n",
    "\n",
    "## What is Data Transformation?\n",
    "During data transformation the goal is to prepare the data to be usable in the modelling steps. These transformations include normalization, standardization, text processing, generating complex features from basic ones, or any kind of data mapping.\n",
    "\n",
    "_\"...a data transformation converts a set of data values from the data format of a source data system into the data format of a destination data system._\n",
    "\n",
    "_Data transformation can be divided into two steps:_\n",
    "1. _data mapping maps data elements from the source data system to the destination data system and captures any transformation that must occur_\n",
    "2. _code generation that creates the actual transformation program\"_\n",
    "from: <a href=\"https://en.wikipedia.org/wiki/Data_transformation\">Wikipedia</a>\n",
    "\n",
    "### Why it is important?\n",
    "\n",
    "Most of the models are sensitive to data, so you must transform it into a more desired format. Unfortunately the data you start with is usually in terrible shape:\n",
    "\n",
    "- It has missing values\n",
    "- It is full of outliers\n",
    "- The data is distorted by noise\n",
    "- The features are in different scales\n",
    "- The features are correlated/redundant/uninformative\n",
    "\n",
    "\n",
    "### Tools\n",
    "\n",
    "- scaling/binarizing\n",
    "- normalizing/standardizing\n",
    "- outlier detecting\n",
    "- filtering\n",
    "- mathematical transformations\n",
    "- representational changes\n",
    "- etc.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing(df, cols, inf=False, percent=.2):\n",
    "    nrows, _ = df.shape\n",
    "    missing = np.nan if not inf else np.inf\n",
    "    df['tmp'] = np.random.rand(nrows)\n",
    "    df.loc[df.tmp < percent, cols] = missing\n",
    "    df = df.drop(columns=['tmp'])\n",
    "    return df\n",
    "\n",
    "def apply_scaler(df, cols, scaler):\n",
    "    df = df.copy()\n",
    "    df[cols] = scaler.transform(df[cols])\n",
    "    return df\n",
    "        \n",
    "def gridplot(X, y=None, cols=None):\n",
    "    if y is not None:\n",
    "        data = pd.concat((X, y), axis=1)\n",
    "        fig = sns.PairGrid(data, vars=cols, hue='Label')\n",
    "    else:\n",
    "        fig = sns.PairGrid(X, vars=cols)\n",
    "    fig = fig.map_diag(plt.hist)\n",
    "    fig = fig.map_offdiag(plt.scatter)\n",
    "    fig = fig.add_legend()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermission - instance-based classifiers\n",
    "\n",
    "### K-Nearest Neighbour classification\n",
    "\n",
    "#### Philosophy\n",
    "<img src=\"./pics/kacsa.png\" width=400 align=\"left\">\n",
    "\n",
    "<br style=\"clear:left;\"/>\n",
    "\n",
    "\n",
    "If it looks like a duck and quacks like a duck, it is probably a duck. \n",
    "\n",
    "#### Taxonomy & Definition\n",
    ">_\"In machine learning, instance-based learning (sometimes called memory-based learning) is a family of learning algorithms that, instead of performing explicit generalization, compares new problem instances with instances seen in training, which have been stored in memory.\"_ - <a href=\"https://en.wikipedia.org/wiki/Instance-based_learning\">Wiki</a>\n",
    "\n",
    "\n",
    "#### Algorithm\n",
    "The basic <a href=\"http://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification\">`kNN`</a> algorithm stores training points with their labels without any coefficient fitting done. During classification a simple majority voting is done to determine the class label.\n",
    "\n",
    "It is called `k` nearest neighbour for a reason: `k` is the number of closest data point considered in the majority voting. When a new data point arrives, the algorithm calculates the `k` nearest data point from the training set. Then these `k` label is used to determine the new entry's label. It is possible to provide a `weights` parameter as well. In this case the labels will be weighted according to the weighting function. Most common weighting method is distance based; the closer the point with a label the more weight it gets.\n",
    "\n",
    "There are different strategies to set the <a href=\"https://www.analyticsvidhya.com/blog/2014/10/introduction-k-neighbours-algorithm-clustering/\">ideal `k` values</a>. `k = 1` is a special case where the new data entry gets the closest train point's label.\n",
    "\n",
    "#### Shortcomings\n",
    "\n",
    "If the data is high dimensinal, the <a href=\"https://en.wikipedia.org/wiki/Curse_of_dimensionality\">curse of dimensionality</a> affects the algorithm's performance.  \n",
    "There is no clear method to determine the best distance metrics, it always depends on the data.  \n",
    "For best performance, the data should be preprocessed and transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "pipe = Pipeline(steps=[('knn', KNeighborsClassifier())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the loan dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/loan.csv', index_col=0)\n",
    "# Transform target values early for plotting reasons\n",
    "df['Label'] = LabelEncoder().fit_transform(df['Target'].values)\n",
    "\n",
    "# Intentionally left 'Loan_ID' out\n",
    "nominal_cols = ['Gender', 'Married', 'Education', 'Dependents', \n",
    "                'Self_Employed', 'Property_Area', 'Credit_History']\n",
    "numerical_cols = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount',\n",
    "                  'Loan_Amount_Term']\n",
    "target_col = 'Label'\n",
    "df[numerical_cols] = df[numerical_cols].astype('float64')\n",
    "X = df[numerical_cols + nominal_cols].copy()\n",
    "y = df[target_col].copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=1/3,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Numerical features\n",
    "\n",
    "\n",
    "### <a href=\"http://pandas.pydata.org/pandas-docs/stable/missing_data.html\">missing values</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = add_missing(df, numerical_cols)\n",
    "missing.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dropping NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped = missing.dropna(axis=0)\n",
    "dropped.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- fill NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled = missing.fillna(value=0)\n",
    "filled.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- intepolate NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated = missing.interpolate(method='nearest')\n",
    "interpolated.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"http://pandas.pydata.org/pandas-docs/stable/missing_data.html#values-considered-missing\">infinite values</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = add_missing(df, numerical_cols, inf=True)\n",
    "missing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('mode.use_inf_as_null', True):\n",
    "    print(missing.dropna(axis=0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('mode.use_inf_as_null', True):\n",
    "    print(missing.fillna(value=0).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('mode.use_inf_as_null', True):\n",
    "    print(missing.interpolate().describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"http://scikit-learn.org/stable/modules/preprocessing.html#standardization-or-mean-removal-and-variance-scaling\">different scales</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train[numerical_cols], y_train)\n",
    "accuracy_score(y_test.values, pipe.predict(X_test[numerical_cols]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax = MinMaxScaler()\n",
    "scaled_pipe = Pipeline(steps=[\n",
    "    ('minmax', minmax),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "scaled_pipe.fit(X_train[numerical_cols], y_train)\n",
    "accuracy_score(y_test.values, scaled_pipe.predict(X_test[numerical_cols]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridplot(apply_scaler(X_train, numerical_cols, minmax), y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Try the same experiment with logistic regression\n",
    "- Create a pipe containing a logistic regressor and measure its accuracy\n",
    "- Create an another pipe with minmaxscaler and logistic regressor. Compare the results and try to explain the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"http://scikit-learn.org/stable/modules/preprocessing.html#normalization\">unnormalized data</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard = StandardScaler()\n",
    "normalized_pipe = Pipeline(steps=[\n",
    "    ('standard', standard),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "normalized_pipe.fit(X_train[numerical_cols], y_train)\n",
    "accuracy_score(y_test.values, normalized_pipe.predict(X_test[numerical_cols]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridplot(apply_scaler(X_train, numerical_cols, standard), y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Try the same experiment with logistic regression\n",
    "- Create a pipe containing a logistic regressor and measure its accuracy\n",
    "- Create an another pipe with standardscaler and logistic regressor. Compare the results and try to explain the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr(), robust=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not now. More about this topic in the next issue of DS101. Cough-cough-<a href=\"http://scikit-learn.org/stable/modules/preprocessing.html#scaling-data-with-outliers\" style=\"color: black; text-decoration: none; cursor: default;\">PCA</a>-cough.\n",
    "\n",
    "### <a href=\"http://scikit-learn.org/stable/modules/preprocessing.html#scaling-data-with-outliers\">outliers</a>\n",
    "\n",
    "<img src=\"https://i0.wp.com/flowingdata.com/wp-content/uploads/2014/09/outlier.gif\" align=\"left\" width=\"400\">\n",
    "\n",
    "<br style=\"clear:left;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robust = RobustScaler()\n",
    "robust_pipe = Pipeline(steps=[\n",
    "    ('robust', robust),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "robust_pipe.fit(X_train[numerical_cols], y_train)\n",
    "accuracy_score(y_test.values, robust_pipe.predict(X_test[numerical_cols]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Try the same experiment with logistic regression\n",
    "- Create a pipe containing a logistic regressor and measure its accuracy\n",
    "- Create an another pipe with robustscaler and logistic regressor. Compare the results and try to explain the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"http://scikit-learn.org/stable/modules/preprocessing.html#feature-binarization\">binarization</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binarizer = Binarizer(threshold=101.0)\n",
    "X_train['BinLoanAmount'] = binarizer.fit_transform(X_train[['LoanAmount']])\n",
    "X_test['BinLoanAmount'] = binarizer.transform(X_test[['LoanAmount']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numwithbincols = [col for col in numerical_cols \n",
    "                  if not col == 'LoanAmount'] + ['BinLoanAmount']\n",
    "pipe.fit(X_train[numwithbincols], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test.values, pipe.predict(X_test[numwithbincols]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Intermission II. - Model of the week\n",
    "\n",
    "### Decision Trees\n",
    "\n",
    "Decision trees are a type of supervised machine learning algorithms that can be used to predict both categorical and continuous values (in this case they are called *regression trees*). **Basically the algorithm divides the training population along the values of their attributes, and assigns a prediction value to each of these categories.** In the case of **categorical** target variable, the assigned prediction will be **the mode of the target variable values** in the particular subpopulation. In the other case it will be **the mean of the values**.  \n",
    "An example which will be familiar, from the <a href=\"https://en.wikipedia.org/wiki/Decision_tree_learning\">wikipedia page of decision trees</a>:  \n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f3/CART_tree_titanic_survivors.png\">  \n",
    "A tree showing survival of passengers on the Titanic (\"sibsp\" is the number of spouses or siblings aboard). The figures under the leaves show the probability of outcome and the percentage of observations in the leaf.\n",
    "\n",
    "#### So, where's the trick?  \n",
    "At deciding *where to split the population*. **The goal is to make these subpopulations as homogenous in the target variable as we can.** (So naturally if the target variable and other variables are completely uncorrelated with an attribute, the decision tree won't split the population according to that variable.) There are multiple *metrics* used to decide what splitting is good at a particular point in the tree, but most of them calculate an **\"impurity\"** of the parent node, and choose a split that will furthest reduce this impurity, most of the time only considering the local node - meaning **it doesn't ensure that the final output will be the best globally**.  \n",
    "\n",
    "##### How big/complex tree do we want to build? Or: When should a node be declared a leaf?\n",
    "As one of the biggest weakness of decision/regression trees is **overfitting**, these are very important questions. There are multiple ways to limit a tree: \n",
    "- Set a minimum sample number at which a split can be made\n",
    "- Set a maximum sample number at which a node can be a leaf\n",
    "- Set a threshold for the impurity decrease, below which no splits are made\n",
    "- Set a maximum depth of the tree\n",
    "- Set the maximum number of variables used for splitting\n",
    "- Set a maximum number of leaves\n",
    "\n",
    "Another answer to overfitting is **tree pruning**. This basically means to let the tree grow to the point where every leaf has only a few number of observations, and then starting from the top or bottom, get rid of the splits that don't contribute too much to the accuraccy of the model.\n",
    "\n",
    "#### And why are trees better than say, logistic regression?\n",
    "The most important case is when **there is a non-linear relation between the attributes and the target variable**. Another good side of decision trees is that despite this they are very easy to interpret (just look at the picture above).\n",
    "\n",
    "#### What is better than a tree? Multiple trees!\n",
    "To tackle overfitting, an early technique was to randomly sample the training set and using different samples, build multiple trees. Then the prediction is made using the \"votes\" of the trees (either the mean or the mode of them). This is called **bagging**. **Random forests** enhance this procedure by excluding random attributes from the potential splitting at each node. The reasoning behind this is that if there are a few attributes which have very good explaining powers, they would be used at most of the splits - and so the different trees would be strongly correlated, reducing the effect of bagging.  \n",
    "An important thing to remember:  \n",
    "> When in doubt, use <s>brute force</s> random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Nominal Features\n",
    "\n",
    "### Replacing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "replace_map = {'Dependents': {'0': 0, '1': 1, '2': 2, '3+': 4}}\n",
    "X_train.replace(replace_map).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"\">Label encoding</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodedX_train = X_train[nominal_cols].copy()\n",
    "encodedX_test = X_test[nominal_cols].copy()\n",
    "for col in nominal_cols:\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(X[col].values)\n",
    "    encodedX_train[col] = encoder.transform(encodedX_train[col].values)\n",
    "    encodedX_test[col] = encoder.transform(encodedX_test[col].values)\n",
    "\n",
    "pipe.fit(encodedX_train, y_train.values)\n",
    "accuracy_score(y_test.values, pipe.predict(encodedX_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Try the same experiment with decision tree\n",
    "- Create a pipe containing a decision tree and measure its accuracy with the encoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"http://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features\">One-hot encoding</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot = OneHotEncoder(categories='auto', sparse=False)\n",
    "onehotpipe = Pipeline(steps=[\n",
    "    ('hot', onehot),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "onehotpipe.fit(encodedX_train, y_train)\n",
    "accuracy_score(y_test.values, onehotpipe.predict(encodedX_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Try the same experiment with decision tree\n",
    "- Create a pipe containing a decision tree and measure its accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label binarizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Try out LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Putting all together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing numerical cols\n",
    "standard = StandardScaler().fit(X[numerical_cols])\n",
    "X_train[numerical_cols] = standard.transform(X_train[numerical_cols])\n",
    "\n",
    "# Label encoding nominal cols\n",
    "for col in nominal_cols:\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(X[col].values)\n",
    "    X_train[col] = encoder.transform(X_train[col].values)\n",
    "    X_test[col] = encoder.transform(X_test[col].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train, y_train)\n",
    "y_hat = pipe.predict(X_test)\n",
    "accuracy_score(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Try out the different classifiers with the preprocessed data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
